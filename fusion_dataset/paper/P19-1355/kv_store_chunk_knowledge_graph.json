{
    "0": {
        "chunk_key": "chunk-c474bd136004e0c87233db96a8e12b49",
        "entities": [
            {
                "entity_name": "\"EMMA STRUBELL\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Emma Strubell is one of the authors of the paper discussing energy and policy considerations for deep learning in NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"ANANYA GANESH\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ananya Ganesh is one of the authors of the paper discussing energy and policy considerations for deep learning in NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"ANDREW MCCALLUM\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Andrew McCallum is one of the authors of the paper discussing energy and policy considerations for deep learning in NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"COLLEGE OF INFORMATION AND COMPUTER SCIENCES\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"College of Information and Computer Sciences is the academic institution where the authors are affiliated.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"UNIVERSITY OF MASSACHUSETTS AMHERST\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"University of Massachusetts Amherst is the university where the authors are affiliated.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"NLP\"",
                "entity_type": "\"EVENT\"",
                "description": "\"NLP refers to the field of Natural Language Processing, which is the focus of the paper discussing energy and policy considerations for deep learning.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"LISA\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"LISA is a state-of-the-art NLP model from EMNLP 2018, which is used as a case study in the paper to analyze the computational and environmental costs.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"BAHDANAU ET AL., 2015\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Bahdanau et al., 2015 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"LUONG ET AL., 2015\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Luong et al., 2015 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"DOZAT AND MANNING, 2017\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Dozat and Manning, 2017 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"VASWANI ET AL., 2017\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Vaswani et al., 2017 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"PETERS ET AL., 2018\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Peters et al., 2018 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"DEVLIN ET AL., 2019\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Devlin et al., 2019 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"RADFORD ET AL., 2019\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Radford et al., 2019 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"SO ET AL., 2019\"",
                "entity_type": "\"EVENT\"",
                "description": "\"So et al., 2019 refers to a specific event or publication in the field of NLP that contributed to accuracy improvements in deep neural networks.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"STRUBELL ET AL., 2018\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Strubell et al., 2018 refers to a specific event or publication in the field of NLP, detailing the development and tuning of a state-of-the-art NLP pipeline.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"GPU\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"GPU refers to Graphics Processing Units, which are specialized hardware required for training the most computationally-hungry deep neural network models in NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"TPU\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"TPU refers to Tensor Processing Units, which are specialized hardware required for training the most computationally-hungry deep neural network models in NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"NVIDIA TITAN X\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"NVIDIA Titan X is a specific model of GPU used for training deep neural network models in NLP, as mentioned in the paper.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"NVIDIA GTX 1080 TI\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"NVIDIA GTX 1080 Ti is a specific model of GPU used for training the ELMo model, as mentioned in the paper.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"NVIDIA SYSTEM MANAGEMENT INTERFACE\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"NVIDIA System Management Interface is a tool used to sample GPU power consumption during the training of deep neural network models.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"INTEL’S RUNNING AVERAGE POWER LIMIT INTERFACE\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"Intel’s Running Average Power Limit interface is a tool used to sample CPU power consumption during the training of deep neural network models.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"CO2\"",
                "entity_type": "\"GEO\"",
                "description": "\"CO2 refers to carbon dioxide emissions, which are a key environmental concern associated with the energy consumption of training deep neural network models.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "entity_name": "\"RENEWABLE\"",
                "entity_type": "\"GEO\"",
                "description": "\"Renewable refers to renewable energy sources like hydro, solar, and wind, which are compared to non-renewable sources in terms of their contribution to energy consumption for cloud compute providers.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            }
        ],
        "relationships": [
            {
                "src_id": "\"EMMA STRUBELL\"",
                "tgt_id": "\"COLLEGE OF INFORMATION AND COMPUTER SCIENCES\"",
                "weight": 7.0,
                "description": "\"Emma Strubell is affiliated with the College of Information and Computer Sciences.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"ANANYA GANESH\"",
                "tgt_id": "\"COLLEGE OF INFORMATION AND COMPUTER SCIENCES\"",
                "weight": 7.0,
                "description": "\"Ananya Ganesh is affiliated with the College of Information and Computer Sciences.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"ANDREW MCCALLUM\"",
                "tgt_id": "\"COLLEGE OF INFORMATION AND COMPUTER SCIENCES\"",
                "weight": 7.0,
                "description": "\"Andrew McCallum is affiliated with the College of Information and Computer Sciences.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"COLLEGE OF INFORMATION AND COMPUTER SCIENCES\"",
                "tgt_id": "\"UNIVERSITY OF MASSACHUSETTS AMHERST\"",
                "weight": 9.0,
                "description": "\"The College of Information and Computer Sciences is a part of the University of Massachusetts Amherst.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"EMMA STRUBELL\"",
                "tgt_id": "\"UNIVERSITY OF MASSACHUSETTS AMHERST\"",
                "weight": 7.0,
                "description": "\"Emma Strubell is affiliated with the University of Massachusetts Amherst.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"ANANYA GANESH\"",
                "tgt_id": "\"UNIVERSITY OF MASSACHUSETTS AMHERST\"",
                "weight": 7.0,
                "description": "\"Ananya Ganesh is affiliated with the University of Massachusetts Amherst.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"ANDREW MCCALLUM\"",
                "tgt_id": "\"UNIVERSITY OF MASSACHUSETTS AMHERST\"",
                "weight": 7.0,
                "description": "\"Andrew McCallum is affiliated with the University of Massachusetts Amherst.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"EMMA STRUBELL\"",
                "tgt_id": "\"NLP\"",
                "weight": 7.0,
                "description": "\"Emma Strubell is an author of a paper discussing energy and policy considerations for deep learning in the field of NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"ANANYA GANESH\"",
                "tgt_id": "\"NLP\"",
                "weight": 7.0,
                "description": "\"Ananya Ganesh is an author of a paper discussing energy and policy considerations for deep learning in the field of NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"ANDREW MCCALLUM\"",
                "tgt_id": "\"NLP\"",
                "weight": 7.0,
                "description": "\"Andrew McCallum is an author of a paper discussing energy and policy considerations for deep learning in the field of NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            },
            {
                "src_id": "\"LISA\"",
                "tgt_id": "\"NLP\"",
                "weight": 8.0,
                "description": "\"LISA is a state-of-the-art NLP model used in the paper to analyze the computational and environmental costs of training deep neural network models for NLP.\"",
                "source_id": "chunk-c474bd136004e0c87233db96a8e12b49"
            }
        ]
    },
    "3": {
        "chunk_key": "chunk-1826475fb0a73465b6f130778848bc2f",
        "entities": [
            {
                "entity_name": "\"CENTERS FOR DATA SCIENCE AND INTELLIGENT INFORMATION RETRIEVAL\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The Centers for Data Science and Intelligent Information Retrieval is an organization that supported the work in part.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"CHAN ZUCKERBERG INITIATIVE\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The Chan Zuckerberg Initiative is an organization that supported the work under the Scientific Knowledge Base Construction project.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"IBM COGNITIVE HORIZONS NETWORK\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The IBM Cognitive Horizons Network is an organization that supported the work through an agreement numbered W1668553.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "The National Science Foundation is an organization that provided support through grant number IIS-1514053 and is suggested to fund shared compute centers for academic researchers.",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"SHERIEF FAROUK\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Sherief Farouk is a person who provided helpful feedback on earlier drafts of the work.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"AWS\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"AWS is a cloud compute service provider that offers valuable and flexible compute resources.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"GOOGLE CLOUD\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Google Cloud is a cloud compute service provider that offers valuable and flexible compute resources.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"MICROSOFT AZURE\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Microsoft Azure is a cloud compute service provider that offers valuable and flexible compute resources.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"RHONDA ASCIERTO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Rhonda Ascierto is the author of the Uptime Institute Global Data report referenced in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"UPTIME INSTITUTE GLOBAL DATA\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Uptime Institute Global Data is a report authored by Rhonda Ascierto and cited in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"MATTHEW E. PETERS\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Matthew E. Peters is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"MARK NEUMANN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Mark Neumann is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"MOHIT IYYER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Mohit Iyyer is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"MATT GARDNER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Matt Gardner is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"CHRISTOPHER CLARK\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Christopher Clark is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"KENTON LEE\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Kenton Lee is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"LUKE ZETTLEMOYER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Luke Zettlemoyer is one of the authors of the paper cited in the text about deep contextualized word representations.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"DEEP CONTEXTUALIZED WORD REPRESENTATIONS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Deep contextualized word representations is a paper presented at NAACL and cited in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"ALEC RADFORD\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Alec Radford is one of the authors of the paper cited in the text about language models being unsupervised multitask learners.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"JEFFREY WU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jeffrey Wu is one of the authors of the paper cited in the text about language models being unsupervised multitask learners.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"REWON CHILD\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Rewon Child is one of the authors of the paper cited in the text about language models being unsupervised multitask learners.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"DAVID LUAN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"David Luan is one of the authors of the paper cited in the text about language models being unsupervised multitask learners.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"DARIO AMODEI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Dario Amodei is one of the authors of the paper cited in the text about language models being unsupervised multitask learners.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"ILYA SUTSKEVER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ilya Sutskever is one of the authors of the paper cited in the text about language models being unsupervised multitask learners.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"LANGUAGE MODELS ARE UNSUPERVISED MULTITASK LEARNERS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Language models are unsupervised multitask learners is a paper cited in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"JASPER SNOEK\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jasper Snoek is one of the authors of the paper cited in the text about practical bayesian optimization of machine learning algorithms.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"HUGO LAROCHELLE\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Hugo Larochelle is one of the authors of the paper cited in the text about practical bayesian optimization of machine learning algorithms.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "entity_name": "\"RYAN P ADAMS\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ryan P Adams is one of the authors of the paper cited in the text about practical bayesian optimization of machine learning algorithms.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            }
        ],
        "relationships": [
            {
                "src_id": "\"CENTERS FOR DATA SCIENCE AND INTELLIGENT INFORMATION RETRIEVAL\"",
                "tgt_id": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "weight": 5.0,
                "description": "\"Both organizations are sponsors of the work mentioned in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "src_id": "\"CHAN ZUCKERBERG INITIATIVE\"",
                "tgt_id": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "weight": 5.0,
                "description": "\"Both organizations are sponsors of the work mentioned in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "src_id": "\"IBM COGNITIVE HORIZONS NETWORK\"",
                "tgt_id": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "weight": 5.0,
                "description": "\"Both organizations are sponsors of the work mentioned in the text.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "src_id": "\"AWS\"",
                "tgt_id": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "weight": 6.0,
                "description": "\"AWS is a cloud service provider that could be an alternative to the shared compute centers suggested to be funded by the U.S. National Science Foundation.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "src_id": "\"GOOGLE CLOUD\"",
                "tgt_id": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "weight": 6.0,
                "description": "\"Google Cloud is a cloud service provider that could be an alternative to the shared compute centers suggested to be funded by the U.S. National Science Foundation.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            },
            {
                "src_id": "\"MICROSOFT AZURE\"",
                "tgt_id": "\"NATIONAL SCIENCE FOUNDATION (U.S. NATIONAL SCIENCE FOUNDATION)\"",
                "weight": 6.0,
                "description": "\"Microsoft Azure is a cloud service provider that could be an alternative to the shared compute centers suggested to be funded by the U.S. National Science Foundation.\"",
                "source_id": "chunk-1826475fb0a73465b6f130778848bc2f"
            }
        ]
    },
    "4": {
        "chunk_key": "chunk-d05a506bbb35c0c92c5637fe84470d23",
        "entities": [
            {
                "entity_name": "\"DAVID WEISS\"",
                "entity_type": "\"PERSON\"",
                "description": "\"David Weiss is an author of the paper 'Linguistically-Informed Self-Attention for Semantic Role Labeling' presented at EMNLP 2018.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ANDREW MCCALLUM\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Andrew McCallum is an author of the paper 'Linguistically-Informed Self-Attention for Semantic Role Labeling' presented at EMNLP 2018.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"EMNLP\"",
                "entity_type": "\"EVENT\"",
                "description": "\"EMNLP is the Conference on Empirical Methods in Natural Language Processing where the paper by David Weiss and Andrew McCallum was presented.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"BRUSSELS, BELGIUM\"",
                "entity_type": "\"GEO\"",
                "description": "\"Brussels, Belgium is the location where the EMNLP 2018 conference took place.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ASHISH VASWANI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ashish Vaswani is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"NOAM SHAZEER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Noam Shazeer is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"NIKI PARMAR\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Niki Parmar is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"JAKOB USZKOREIT\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jakob Uszkoreit is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"LLION JONES\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Llion Jones is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"AIDAN N GOMEZ\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Aidan N Gomez is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"LUKASZ KAISER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Lukasz Kaiser is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ILLIA POLOSUKHIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Illia Polosukhin is an author of the paper 'Attention is all you need' presented at NIPS 2017.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"NIPS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"NIPS is the 31st Conference on Neural Information Processing Systems where the paper by Ashish Vaswani et al. was presented.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"DZMITRY BAHDANAU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Dzmitry Bahdanau is an author of the paper 'Neural Machine Translation by Jointly Learning to Align and Translate' presented at ICLR 2015.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"KYUNGHYUN CHO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Kyunghyun Cho is an author of the paper 'Neural Machine Translation by Jointly Learning to Align and Translate' presented at ICLR 2015.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"YOSHUA BENGIO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yoshua Bengio is an author of multiple papers, including 'Neural Machine Translation by Jointly Learning to Align and Translate' presented at ICLR 2015.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ICLR\"",
                "entity_type": "\"EVENT\"",
                "description": "\"ICLR is the 3rd International Conference for Learning Representations where the paper by Dzmitry Bahdanau et al. was presented.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"SAN DIEGO, CALIFORNIA, USA\"",
                "entity_type": "\"GEO\"",
                "description": "\"San Diego, California, USA is the location where the ICLR 2015 conference took place.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"JAMES BERGSTRA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"James Bergstra is an author of multiple papers, including 'Random search for hyper-parameter optimization' and 'Algorithms for hyper-parameter optimization'.(\"entity\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"BALA´ZS KE´GL\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Bala´zs Ke´gl is an author of the paper 'Algorithms for hyper-parameter optimization' presented at Advances in neural information processing systems.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Advances in neural information processing systems is the venue where the paper by James S Bergstra et al. was presented.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"BRUNO BURGER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Bruno Burger is the author of the technical report 'Net Public Electricity Generation in Germany in 2018' from Fraunhofer Institute for Solar Energy Systems ISE.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"FRAUNHOFER INSTITUTE FOR SOLAR ENERGY SYSTEMS ISE\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Fraunhofer Institute for Solar Energy Systems ISE is the organization that published the technical report by Bruno Burger.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ALFREDO CANZIANI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Alfredo Canziani is an author of the paper 'An analysis of deep neural network models for practical applications'.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ADAM PASZKE\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Adam Paszke is an author of the paper 'An analysis of deep neural network models for practical applications'.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"EUGENIO CULURCIELLO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Eugenio Culurciello is an author of the paper 'An analysis of deep neural network models for practical applications'.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"GARY COOK\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Gary Cook is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"JUDE LEE\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jude Lee is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"TAMINA TSAI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Tamina Tsai is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ADA KONGN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ada Kongn is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"JOHN DEANS\"",
                "entity_type": "\"PERSON\"",
                "description": "\"John Deans is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"ELIZABETH JARDIM\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Elizabeth Jardim is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"BRIAN JOHNSON\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Brian Johnson is an author of the technical report 'Clicking Clean: Who is winning the race to build a green internet?' from Greenpeace.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"GREENPEACE\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Greenpeace is the organization that published the technical report 'Clicking Clean: Who is winning the race to build a green internet?'.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"JACOB DEVLIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jacob Devlin is an author of the paper 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' presented at NAACL.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            },
            {
                "entity_name": "\"MING-WEI CHANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ming-Wei Chang is an author of the paper 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding' presented at NAACL.\"",
                "source_id": "chunk-d05a506bbb35c0c92c5637fe84470d23"
            }
        ],
        "relationships": []
    },
    "2": {
        "chunk_key": "chunk-3eec394eafb261cc01db2d32f0050eb0",
        "entities": [
            {
                "entity_name": "\"OPENAI\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"OpenAI is an organization responsible for developing the GPT-2 model, a general-purpose token encoder based on Transformer-style self-attention.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"RADFORD ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Radford et al. are researchers who have published findings on the GPT-2 model, demonstrating high zero-shot performance on question answering and language modeling benchmarks.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"LI ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Li et al. are researchers who have conducted a study on the energy use required for training and inference in popular convolutional models for image classification in computer vision.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"CANZANI ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Canzani et al. are researchers who have assessed image classification model accuracy as a function of model size and gigaflops required during inference, and measured average power draw required during inference on GPUs.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"BERGSTRA ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Bergstra et al. are researchers who have performed analysis of hyperparameter tuning in the context of improved algorithms for hyperparameter search.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"SNOEK ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Snoek et al. are researchers involved in the analysis of hyperparameter tuning.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"STRUBELL ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Strubell et al. are researchers who developed the LinguisticallyInformed Self-Attention model, a multi-task model for part-of-speech tagging, labeled dependency parsing, predicate detection, and semantic role labeling.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"BERT\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"BERT is a model developed for natural language processing tasks, noted for its cost-efficiency on TPUs and substantial carbon emissions when trained on GPUs.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"EMNLP\"",
                "entity_type": "\"EVENT\"",
                "description": "\"EMNLP is an event where the LinguisticallyInformed Self-Attention model by Strubell et al. was recognized as a Best Long Paper.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"GPT-2\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"GPT-2 is a model developed by OpenAI, based on Transformer-style self-attention and trained with a language modeling objective.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"TESLA V100\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Tesla V100 refers to a type of GPU used in the training of large models, with a total of 64 such GPUs mentioned in the context.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"FORSTER ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Forster et al. are researchers cited in the text, associated with the computational details of training large models using DGX-2H servers and Tesla V100 GPUs.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"TRANSFORMER-STYLE SELF-ATTENTION\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Transformer-style self-attention is a concept related to the architecture of models like GPT-2, which is based on this attention mechanism.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"LANGUAGE MODELING OBJECTIVE\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Language modeling objective refers to the training approach used for models like GPT-2, which is aimed at predicting the next item in a sequence.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"ZERO-SHOT PERFORMANCE\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Zero-shot performance is a concept indicating how well a model performs on a task without any specific training on that task, highlighted as a strength of GPT-2.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"QUESTION ANSWERING\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Question answering is an event or task where models like GPT-2 are evaluated for their zero-shot performance.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"LANGUAGE MODELING BENCHMARKS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Language modeling benchmarks are events or tasks used to evaluate the performance of models like GPT-2.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"TPUV3\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"TPUv3 refers to a type of chip used for training large models, specifically mentioned in the context of the training requirements for the GPT-2 model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"NVIDIA TITAN X\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"NVIDIA Titan X refers to a type of GPU used in the training of models, specifically mentioned as being used in 72% of the training for the LinguisticallyInformed Self-Attention model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"NVIDIA M40\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"NVIDIA M40 refers to a type of GPU used in the training of models, specifically mentioned as being used in 28% of the training for the LinguisticallyInformed Self-Attention model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"GOOGLE CLOUD\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Google Cloud is an organization providing cloud compute services, mentioned in the context of estimating the cost of training models.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"CO2E\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"CO2e refers to the concept of carbon dioxide equivalent, used to measure carbon emissions, specifically in the context of training model emissions.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"NAS\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"NAS refers to Neural Architecture Search, a method that achieves a new state-of-the-art BLEU score for machine translation at a significant computational cost.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"BLEU SCORE\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"BLEU score is a metric used to evaluate the performance of machine translation models, with NAS achieving a new state-of-the-art score.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "entity_name": "\"ENGLISH TO GERMAN MACHINE TRANSLATION\"",
                "entity_type": "\"EVENT\"",
                "description": "\"English to German machine translation is an event or task where NAS achieved a new state-of-the-art BLEU score.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            }
        ],
        "relationships": [
            {
                "src_id": "\"OPENAI\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 10.0,
                "description": "\"OpenAI is the developer of the GPT-2 model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"RADFORD ET AL.\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 9.0,
                "description": "\"Radford et al. have published findings on the performance of the GPT-2 model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"LI ET AL.\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 6.0,
                "description": "\"Li et al.'s work provides a precedent for characterizing the computational requirements of training neural network architectures, which is relevant to the GPT-2 model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"CANZANI ET AL.\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 6.0,
                "description": "\"Canzani et al.'s work on model accuracy and power draw during inference is related to the computational requirements of models like GPT-2.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"BERGSTRA ET AL.\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 6.0,
                "description": "\"Bergstra et al.'s analysis of hyperparameter tuning is relevant to the development and optimization of models like GPT-2.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"SNOEK ET AL.\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 6.0,
                "description": "\"Snoek et al.'s work on hyperparameter tuning is relevant to the development and optimization of models like GPT-2.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"STRUBELL ET AL.\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 7.0,
                "description": "\"Strubell et al.'s development of a multi-task model provides a case study in computational requirements for R&D in NLP, which is relevant to the development of models like GPT-2.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            },
            {
                "src_id": "\"TESLA V100\"",
                "tgt_id": "\"GPT-2\"",
                "weight": 7.0,
                "description": "\"Tesla V100 GPUs were used in the training of the GPT-2 model.\"",
                "source_id": "chunk-3eec394eafb261cc01db2d32f0050eb0"
            }
        ]
    },
    "1": {
        "chunk_key": "chunk-f96a801f228339f43372f3cdb35b0f77",
        "entities": [
            {
                "entity_name": "\"COOK ET AL., 2017\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Cook et al., 2017 are researchers who provided data on energy sources for cloud compute providers and countries.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"BURGER, 2019\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Burger, 2019 is a researcher who provided data on energy sources for Germany and the United States.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"ASCIERTO, 2018\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ascierto, 2018 is a researcher who provided the Power Usage Effectiveness (PUE) coefficient for data centers.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"U.S. ENVIRONMENTAL PROTECTION AGENCY (EPA)\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The U.S. Environmental Protection Agency (EPA) provides data on CO2 emissions produced per kilowatt-hour for power consumed in the U.S.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"UNITED STATES\"",
                "entity_type": "\"GEO\"",
                "description": "\"The United States is a country compared in terms of energy sources and CO2 emissions in the context of cloud computing.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"CHINA\"",
                "entity_type": "\"GEO\"",
                "description": "\"China is a country compared in terms of energy sources in the context of cloud computing.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"GERMANY\"",
                "entity_type": "\"GEO\"",
                "description": "\"Germany is a country compared in terms of energy sources in the context of cloud computing.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"TRANSFORMER (T2T) MODEL\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The Transformer (T2T) model is an encoder-decoder architecture recognized for machine translation, with code available online.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"VASWANI ET AL., 2017\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Vaswani et al., 2017 are researchers who reported on the training of the Transformer base and big models.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"ELMO MODEL\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The ELMo model is based on stacked LSTMs and provides word representations in context, with code available online.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"PETERS ET AL., 2018\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Peters et al., 2018 are researchers who reported on the training of the ELMo model.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"BERT MODEL\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The BERT model provides a Transformer-based architecture for building contextual representations, with code available online.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"DEVLIN ET AL., 2019\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Devlin et al., 2019 are researchers who reported on the training of the BERT base model.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"GPT-2 MODEL\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The GPT-2 model is OpenAI’s latest edition of GPT general-purpose token encoder, trained with a language modeling objective.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"RADFORD ET AL., 2019\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Radford et al., 2019 are researchers who reported on the training of the GPT-2 model.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"FORSTER ET AL., 2019\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Forster et al., 2019 are researchers who reported on training a BERT model using NVIDIA's DGX-2H servers.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"TPUV2 CORE\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"TPUv2 core is a type of hardware used for training machine learning models, specifically mentioned in the context of the architecture search.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"SO ET AL., 2019\"",
                "entity_type": "\"PERSON\"",
                "description": "\"So et al., 2019 are researchers who reported on their full architecture search and its training duration.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"STRUBELL ET AL., 2018\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Strubell et al., 2018 are researchers who studied the NLP pipeline in more detail.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"AMAZON WEB SERVICES\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Amazon Web Services is a cloud compute service provider, and its energy breakdown is compared to that of the United States.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"DGX-2H SERVERS\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"DGX-2H servers are high-performance servers used for training machine learning models, mentioned in the context of BERT model training.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"TESLA V100 GPUS\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"Tesla V100 GPUs are a type of GPU used in training machine learning models, specifically in the context of BERT model training.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "entity_name": "\"TRANSFORMER-STYLE SELF-ATTENTION\"",
                "entity_type": "\"TECHNOLOGY\"",
                "description": "\"Transformer-style self-attention is an architectural feature used in models like GPT-2 for language modeling tasks.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            }
        ],
        "relationships": [
            {
                "src_id": "\"COOK ET AL., 2017\"",
                "tgt_id": "\"UNITED STATES\"",
                "weight": 5.0,
                "description": "\"Cook et al., 2017 provided data on energy sources for the United States.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"COOK ET AL., 2017\"",
                "tgt_id": "\"CHINA\"",
                "weight": 5.0,
                "description": "\"Cook et al., 2017 provided data on energy sources for China.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"BURGER, 2019\"",
                "tgt_id": "\"GERMANY\"",
                "weight": 5.0,
                "description": "\"Burger, 2019 provided data on energy sources for Germany.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"BURGER, 2019\"",
                "tgt_id": "\"UNITED STATES\"",
                "weight": 5.0,
                "description": "\"Burger, 2019 provided data on energy sources for the United States.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"ASCIERTO, 2018\"",
                "tgt_id": "\"TRANSFORMER (T2T) MODEL\"",
                "weight": 1.0,
                "description": "\"Ascierto, 2018 provided the PUE coefficient used in the power consumption calculations for the Transformer model.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"FORSTER ET AL., 2019\"",
                "tgt_id": "\"BERT MODEL\"",
                "weight": 7.0,
                "description": "\"Forster et al., 2019 reported on the training of the BERT model using NVIDIA's DGX-2H servers.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"TPUV2 CORE\"",
                "tgt_id": "\"SO ET AL., 2019\"",
                "weight": 7.0,
                "description": "\"So et al., 2019 reported that their base model requires training on TPUv2 cores.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"STRUBELL ET AL., 2018\"",
                "tgt_id": "\"TRANSFORMER (T2T) MODEL\"",
                "weight": 7.0,
                "description": "\"Strubell et al., 2018 studied the NLP pipeline that includes the Transformer (T2T) model.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            },
            {
                "src_id": "\"AMAZON WEB SERVICES\"",
                "tgt_id": "\"UNITED STATES\"",
                "weight": 7.0,
                "description": "\"The energy breakdown of Amazon Web Services is comparable to that of the United States.\"",
                "source_id": "chunk-f96a801f228339f43372f3cdb35b0f77"
            }
        ]
    }
}