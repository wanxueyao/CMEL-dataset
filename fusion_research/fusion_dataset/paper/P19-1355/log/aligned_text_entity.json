{
    "image_1": [
        {
            "merged_entity_name": "HUMAN LIFE, AVG, 1 YEAR",
            "entity_type": "UNKNOWN",
            "description": "Human life, avg, 1 year是从image_1中提取的实体。",
            "source_image_entities": [
                "HUMAN LIFE, AVG, 1 YEAR"
            ],
            "source_text_entities": []
        },
        {
            "merged_entity_name": "AMERICAN LIFE, AVG, 1 YEAR",
            "entity_type": "UNKNOWN",
            "description": "American life, avg, 1 year是从image_1中提取的实体。",
            "source_image_entities": [
                "AMERICAN LIFE, AVG, 1 YEAR"
            ],
            "source_text_entities": []
        },
        {
            "merged_entity_name": "CAR, AVG INCL. FUEL, 1 LIFETIME",
            "entity_type": "UNKNOWN",
            "description": "Car, avg incl. fuel, 1 lifetime是从image_1中提取的实体。",
            "source_image_entities": [
                "CAR, AVG INCL. FUEL, 1 LIFETIME"
            ],
            "source_text_entities": []
        },
        {
            "merged_entity_name": "AIR TRAVEL, 1 PERSON, NY↔SF",
            "entity_type": "UNKNOWN",
            "description": "Air travel, 1 person, NY↔SF是从image_1中提取的实体。",
            "source_image_entities": [
                "AIR TRAVEL, 1 PERSON, NY↔SF"
            ],
            "source_text_entities": []
        }
    ],
    "image_2": [
        {
            "entity_name": "NLP PIPELINE (PARSING, SRL)",
            "entity_type": "ORGANIZATION",
            "description": "A computational framework used for natural language processing tasks including parsing and semantic role labeling. It is noted to have a complexity score of 39. This framework is central to the paper's discussion on energy and policy considerations for deep learning in NLP.",
            "source_image_entities": [
                "NLP PIPELINE (PARSING, SRL)"
            ],
            "source_text_entities": [
                "NLP"
            ]
        },
        {
            "entity_name": "W/ TUNING & EXPERIMENTS",
            "entity_type": "EVENT",
            "description": "The process of adjusting parameters and conducting various experiments on the NLP pipeline to optimize its performance. This event has a complexity score of 78,468. It is highlighted in the paper as part of the resource-intensive development and tuning of a recent state-of-the-art NLP pipeline.",
            "source_image_entities": [
                "W/ TUNING & EXPERIMENTS"
            ],
            "source_text_entities": [
                "STRUBELL ET AL., 2018"
            ]
        },
        {
            "entity_name": "TRANSFORMER (BIG)",
            "entity_type": "ORGANIZATION",
            "description": "A large-scale implementation of the Transformer model, which is a type of neural network architecture used in machine learning for natural language processing tasks. It has a complexity score of 192. The paper discusses the environmental and financial costs associated with training such models.",
            "source_image_entities": [
                "TRANSFORMER (BIG)"
            ],
            "source_text_entities": [
                "TRANSFORMER (T2T) MODEL"
            ]
        },
        {
            "entity_name": "W/ NEURAL ARCH. SEARCH",
            "entity_type": "EVENT",
            "description": "The process of searching for an optimal neural architecture for the Transformer model to enhance its efficiency and effectiveness. This event has a complexity score of 626,155. The paper includes a case study analyzing the full computational resources required for such a search as part of developing LISA, a state-of-the-art NLP model.",
            "source_image_entities": [
                "W/ NEURAL ARCH. SEARCH"
            ],
            "source_text_entities": [
                "STRUBELL ET AL., 2018"
            ]
        }
    ],
    "image_3": [
        {
            "entity_name": "CHINA",
            "entity_type": "GEO",
            "description": "China is a country compared in terms of energy sources in the context of cloud computing.",
            "source_image_entities": [
                "CHINA"
            ],
            "source_text_entities": [
                "CHINA"
            ]
        },
        {
            "entity_name": "GERMANY",
            "entity_type": "GEO",
            "description": "Germany is a country compared in terms of energy sources in the context of cloud computing.",
            "source_image_entities": [
                "GERMANY"
            ],
            "source_text_entities": [
                "GERMANY"
            ]
        },
        {
            "entity_name": "UNITED STATES",
            "entity_type": "GEO",
            "description": "The United States is a country compared in terms of energy sources and CO2 emissions in the context of cloud computing.",
            "source_image_entities": [
                "UNITED STATES"
            ],
            "source_text_entities": [
                "UNITED STATES"
            ]
        },
        {
            "entity_name": "AMAZON-AWS",
            "entity_type": "ORGANIZATION",
            "description": "Amazon Web Services is a cloud compute service provider, and its energy breakdown is compared to that of the United States.",
            "source_image_entities": [
                "AMAZON-AWS"
            ],
            "source_text_entities": [
                "AMAZON WEB SERVICES"
            ]
        },
        {
            "entity_name": "GOOGLE",
            "entity_type": "ORGANIZATION",
            "description": "Google is a technology company known for its cloud computing services and contributions to deep learning and machine learning models.",
            "source_image_entities": [
                "GOOGLE"
            ],
            "source_text_entities": []
        },
        {
            "entity_name": "MICROSOFT",
            "entity_type": "ORGANIZATION",
            "description": "Microsoft is a technology company known for its cloud computing services and contributions to deep learning and machine learning models.",
            "source_image_entities": [
                "MICROSOFT"
            ],
            "source_text_entities": []
        }
    ],
    "image_4": [
        {
            "merged_entity_name": "GPT-2 MODEL",
            "entity_type": "MODEL",
            "description": "GPT-2 is OpenAI’s latest edition of GPT general-purpose token encoder, trained with a language modeling objective and based on Transformer-style self-attention. It requires 1 week (168 hours) of training on 32 TPUv3 chips and incurs a cloud compute cost ranging from $12,902 to $43,008.",
            "source_image_entities": [
                "GPT-2"
            ],
            "source_text_entities": [
                "GPT-2 MODEL"
            ]
        },
        {
            "merged_entity_name": "BERT BASE MODEL",
            "entity_type": "MODEL",
            "description": "The BERT BASE model provides a Transformer-based architecture for building contextual representations. It uses TPUv2x16 hardware, runs for 96 hours, and incurs a cloud compute cost ranging from $2074 to $6912. Additionally, it uses V100x64 hardware, consumes 12,041.51 watts of power, runs for 79 hours, generates 1507 kWh of energy, emits 1438 kg of CO2, and incurs a cloud compute cost ranging from $3751 to $12,571.",
            "source_image_entities": [
                "BERT_BASE"
            ],
            "source_text_entities": [
                "BERT MODEL"
            ]
        },
        {
            "merged_entity_name": "ELMO MODEL",
            "entity_type": "MODEL",
            "description": "The ELMo model is based on stacked LSTMs and provides word representations in context. It uses P100x3 hardware, consumes 517.66 watts of power, runs for 336 hours, generates 275 kWh of energy, emits 262 kg of CO2, and incurs a cloud compute cost ranging from $433 to $1472.",
            "source_image_entities": [
                "ELMO"
            ],
            "source_text_entities": [
                "ELMO MODEL"
            ]
        },
        {
            "merged_entity_name": "TRANSFORMER (T2T) MODEL",
            "entity_type": "MODEL",
            "description": "The Transformer (T2T) model is an encoder-decoder architecture recognized for machine translation. T2T_BASE uses P100x8 hardware, consumes 1415.78 watts of power, runs for 12 hours, generates 27 kWh of energy, emits 26 kg of CO2, and incurs a cloud compute cost ranging from $41 to $140. T2T_BIG uses P100x8 hardware, consumes 1515.43 watts of power, runs for 84 hours, generates 201 kWh of energy, emits 192 kg of CO2, and incurs a cloud compute cost ranging from $289 to $981.",
            "source_image_entities": [
                "T2T_BASE",
                "T2T_BIG"
            ],
            "source_text_entities": [
                "TRANSFORMER (T2T) MODEL"
            ]
        },
        {
            "merged_entity_name": "NAS MODEL",
            "entity_type": "MODEL",
            "description": "NAS (Neural Architecture Search) achieves a new state-of-the-art BLEU score of 29.7 for English to German machine translation. It uses P100x8 hardware, consumes 1515.43 watts of power, runs for 274,120 hours, generates 656,347 kWh of energy, emits 626,155 kg of CO2, and incurs a cloud compute cost ranging from $942,973 to $3,201,722. Additionally, it uses TPUv2x1 hardware, runs for 32,623 hours, and incurs a cloud compute cost ranging from $44,055 to $146,848.",
            "source_image_entities": [
                "NAS"
            ],
            "source_text_entities": [
                "NAS"
            ]
        }
    ],
    "image_5": [
        {
            "entity_name": "MODELS",
            "entity_type": "ORGANIZATION",
            "description": "A list of models with corresponding hours and estimated costs for both cloud and electric options, including models like BERT and GPT-2 which have specific training durations and carbon emissions associated with their training processes.",
            "source_image_entities": [
                "MODELS"
            ],
            "source_text_entities": [
                "BERT",
                "GPT-2"
            ]
        },
        {
            "entity_name": "HOURS",
            "entity_type": "EVENT",
            "description": "The time duration associated with each model, measured in hours, such as the 168 hours required for training the GPT-2 model and the 79.2 hours mentioned in the context of using DGX-2H servers.",
            "source_image_entities": [
                "HOURS"
            ],
            "source_text_entities": [
                "GPT-2",
                "DGX-2H SERVERS"
            ]
        },
        {
            "entity_name": "CLOUD COMPUTE COST",
            "entity_type": "GEO",
            "description": "The cost range for using cloud services, given in USD, which includes the estimated cost of training models like BERT and GPT-2, as well as the cost of R&D for models such as the LinguisticallyInformed Self-Attention model.",
            "source_image_entities": [
                "CLOUD"
            ],
            "source_text_entities": [
                "BERT",
                "GPT-2",
                "LinguisticallyInformed Self-Attention model"
            ]
        },
        {
            "entity_name": "ELECTRIC COMPUTE COST",
            "entity_type": "GEO",
            "description": "The cost for using electric services, given in USD, which corresponds to the estimated electricity costs associated with training models like BERT and GPT-2, as well as the carbon emissions from such training processes.",
            "source_image_entities": [
                "ELECTRIC"
            ],
            "source_text_entities": [
                "BERT",
                "GPT-2"
            ]
        },
        {
            "entity_name": "TESLA V100 GPUs",
            "entity_type": "TECHNOLOGY",
            "description": "Tesla V100 GPUs are a type of GPU used in training machine learning models, specifically mentioned in the context of training BERT model and requiring a total of 64 such GPUs.",
            "source_image_entities": [],
            "source_text_entities": [
                "TESLA V100 GPUS"
            ]
        },
        {
            "entity_name": "DGX-2H SERVERS",
            "entity_type": "TECHNOLOGY",
            "description": "DGX-2H servers are high-performance servers used for training machine learning models, mentioned in the context of BERT model training and the use of 4 such servers totaling 64 Tesla V100 GPUs.",
            "source_image_entities": [],
            "source_text_entities": [
                "DGX-2H SERVERS",
                "TESLA V100 GPUS"
            ]
        }
    ]
}