{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "One major result of our findings is that existing work has ignored the importance of conversational flow, leading to state-of-the-art performance. Specifi- cally, we identify and study eight aspects of conversation that can be measured by human judgments, while varying four types of low-level attributes that can be algorithmically controlled in neural models; see Figure 1. To control the lowlevel model attributes, we consider two simple but general algorithms: conditional training, in which the neural model is conditioned on additional control features, and weighted decoding, in which control features are added to the decoding scoring function at test time only.  remain without a de facto solution. Strikingly, the factors that determine human judgments of overall conversation quality are almost entirely unexplored. Most works have been limited to the next utterance prediction problem, whereas a multi-turn evaluation is necessary to evaluate the quality of a full conversation. In this work we both (i) conduct a large-scale study to identify the fine-grained factors governing human judgments of full conversations, and (ii) develop models that apply our findings in practice, Figure 1: We manipulate four low-level attributes and measure their effect on human judgments of individual conversational aspects, as well as overall quality. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-9705c1be981fe537fcfa695340dc636a",
        "description": "The image is a flowchart that illustrates the relationship between low-level controllable attributes and human judgments of conversational aspects, ultimately leading to human judgments of overall quality. On the left side, there are four green boxes labeled 'Low-level controllable attributes,' which include: Repetition (n-gram overlap), Specificity (normalized inverse document frequency), Response-relatedness (cosine similarity of sentence embeddings), and Question-asking ('?' used in utterance). These attributes are connected by an arrow to six blue boxes labeled 'Human judgment of conversational aspects,' which include: Avoiding Repetition, Interestingness, Making sense, Fluency, Listening, and Inquisitiveness. Another arrow leads from these conversational aspects to two purple boxes labeled 'Human judgment of overall quality,' which include Humanness and Engagingness. The flowchart uses arrows to indicate the progression from low-level attributes to conversational aspects and finally to overall quality judgments.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_2.jpg",
        "caption": [],
        "footnote": [],
        "context": "In conversation, it’s generally desirable to produce a response that is related to the partner’s last utterance; for example if the partner says My grandfather died last month, it 6.3 Response-relatedness To control specificity with conditional training, we define the specificity of an utterance $y$ to be the mean NIDF of the words in $y$ . Thus our control variable $z$ is mean NIDF (discretized into 10 equal-sized buckets). As shown in Table 1, this method gives outputs with a narrower NIDF range, but overall produces less nonsensical outputs. nonsensical. The boundary for nonsensical output differs from example to example.  max idf are the minimum and maximum IDFs, taken over all words in the vocabulary. To control specificity with weighted decoding, we use NIDF as a decoding feature. As shown in Table 1, this method produces reasonable outputs when the feature weight is within a certain range, but at the extremes the outputs are Table 1: Middle: Example of controlling specificity (NIDF) via weighted decoding. At the extremes, the model produces only the most rare or the most common tokens. Bottom: Example of controlling specificity via conditional training. This gives a narrower NIDF range, but all the responses are appropriate. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-b6fc9ca9b139955a3cf8698c53a63d18",
        "description": "The image is a table labeled 'Table 1' that provides examples of controlling specificity (NIDF) in response generation. The table is divided into two main sections: 'Weighted Decoding Response' and 'Conditional Training Response'. Each section contains rows with different values of NIDF and corresponding responses. In the 'Weighted Decoding Response' section, the first row has a weight (Wt) of -5.0 and an NIDF of 0.6%, with the response 'Oh.................................'. The second row has a Wt of 0.0 and an NIDF of 17.1%, with the response 'That sounds like a lot of fun!'. The third row has a Wt of 3.0 and an NIDF of 18.3%, with the response 'That sounds like a lot of fun. How long have you been studying?'. The fourth row has a Wt of 7.0 and an NIDF of 38.5%, with the response 'I majored in practising my spiritual full time philosophy test'. The fifth row has a Wt of 10.0 and an NIDF of 71.9%, with the response 'Oh wow! Merna jean isa paino yi hao hui bu acara sya gila [...]' which appears nonsensical. In the 'Conditional Training Response' section, the first row has an NIDF of 16.8% and the response 'Sounds like you are a great person!'. The second row has an NIDF of 18.3% and the response 'So you are a law student?'. The third row has an NIDF of 18.4% and the response 'That sounds like a lot of fun'. The fourth row has an NIDF of 22.8% and the response 'That sounds like a rewarding job!'. The fifth row has an NIDF of 24.4% and the response 'That sounds like a rewarding career!'.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Example of controlling response-relatedness (cosine similarity to input) via weighted decoding. Positive weights (e.g. 5.0) can yield more on-topic responses, but higher weights (e.g. 11.0) can result in nonsensical lists of topically related words. "
        ],
        "context": "Considerate chitchat requires a reciprocal asking and answering of questions – asking too few or too many can 6.4 Question-asking To control response-relatedness with conditional training, we try defining the control variable $z$ to be $\\mathtt{c o s\\_s i m}(\\mathtt{s e n t\\_e m b}(y),\\mathtt{s e n t}$ emb(ℓ)), the overall cosine similarity between the partner’s last utterance $\\ell$ and the model’s response $y$ (again, we discretize $z$ ). However, we find this method ineffective – the CT model learns only a very weak connection between $z$ and the semantic relatedness of the output (see Section 7 for more details).  cos sim is the cosine similarity between the two. In particular, the sentence embedding sent emb $\\left(s\\right)$ for an utterance $s$ is a weighted average of the GloVe embeddings of the words in $s$ , with the first principal component projected out; for full details, see Arora et al. (2017). This method of controlling response-relatedness is similar to that described in (Baheti et al., 2018). We find that weighted decoding is effective to control the semantic relatedness of the model’s response to the partner’s last utterance (see Table 2). As before, we find that extreme weights lead to nonsensical output. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-2db2df383f9e0dcc2bfa014fdfd468b1",
        "description": "The image is a table labeled 'Table 2: Example of controlling response-relatedness (cosine similarity to input) via weighted decoding.' The table has four columns: 'Wt' (Weight), 'Sim' (Similarity), and 'Weighted Decoding Response.' The rows represent different responses with varying weights and their corresponding cosine similarities. The 'Wt' column lists the weights as -10.0, 0.0, 5.0, 8.0, and 11.0. The 'Sim' column shows the cosine similarities as -0.05, -0.02, 0.40, 0.59, and 0.72, respectively. The 'Weighted Decoding Response' column provides the responses for each weight: 'I am a musician.' for -10.0, 'I do, when I am not playing the piano.' for 0.0, 'I do, usually at starbucks.' for 5.0, 'Not usually, especially when you drink latte.' for 8.0, and 'Not often, usually with drinks, espresso, latte, tea, etc.' for 11.0. The table highlights how different weights can influence the semantic relatedness of the model’s response.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_4.jpg",
        "caption": [
            "Figure 2: Controlling question-asking via conditional training. Exact numbers can be found in Appendix F. "
        ],
        "footnote": [],
        "context": "For conditional training, we regard an utterance $y$ as containing a question if and only if $y$ contains a question mark. We train our CT model on a control variable $z$ with 11 possible values: $\\{0,\\ldots,10\\}$ . As discussed in Section 5, we wish to control question-asking at the distributional, dialogue level, rather than at the binary, utterance level. Thus the setting $z=i$ means that the model should produce, on average, utterances containing ‘?’ with probability $i/10$ . During training we randomly assign examples to buckets such that each bucket $i$ is trained on examples What??????? or Who? When? How?). to study these trade-offs. To control question-asking with weighted decoding, we use the binary decoding feature is qn word $(w)$ , which is equal to 1 if and only if the word $w$ is in a pre-defined list of interrogative words (how, what, when, where, which, who, whom, whose, why, ?). We find this is a somewhat effective method to encourage or discourage questions, but with unintended side-effects: a negative weight can discourage valid non-question utterances that happen to contain interrogative words (such as I’m learning how to knit) and a positive weight can result in degenerate utterances (such as ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-2db2df383f9e0dcc2bfa014fdfd468b1",
        "description": "The image is a line graph titled 'Controlling question-asking via conditional training.' The x-axis represents the 'Question-Asking Control Level (CT)' ranging from 0 to 10, with an additional point labeled 'boost' at 10. The y-axis represents the percentage of utterances containing a question mark, ranging from 0% to 100%. There are six lines plotted on the graph, each representing different conditions or baselines. The blue solid line represents 'Question-controlled CT,' starting at approximately 20% at CT=0 and increasing steadily to around 95% at CT=10. The purple dashed line represents 'Question-controlled CT w/ rep ctrl,' starting at around 30% at CT=0 and increasing to about 80% at CT=10. The red dotted line represents the 'Target for question-controlled CT,' which is a straight line starting at 0% at CT=0 and ending at 100% at CT=10. The light blue dash-dotted line represents the 'Beam search baseline,' which remains constant at 80% across all CT levels. The dark purple dashed line represents the 'Repetition-controlled baseline,' which starts at around 40% at CT=0 and increases to about 70% at CT=10. The orange dash-dot-dotted line represents the 'Gold data,' which starts at around 20% at CT=0 and increases to about 60% at CT=10. The graph shows the effectiveness of different methods in controlling the frequency of question-asking in utterances.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_5.jpg",
        "caption": [
            "Figure 3: Calibrated human judgments of engagingness for the baselines and best controlled models (left); for different specificity control settings (middle); and for different question-asking control settings (right). "
        ],
        "footnote": [],
        "context": "Specificity (WD, CT) For our weighted decoding models, the extreme settings (very generic and very specific) score poorly in engagingness due to the frequent presence of degenerate output – see Figure 3 (middle). We find that the weight $=\\,4$ setting (which is more specific than the repetitioncontrolled baseline and about as specific as the gold data) maximizes engagingness. As shown in Figure 3 (left) and Figure 4, this more-specific model is rated more interesting, engaging, and a better listener than the repetition-controlled baseline, but at the cost of reduced fluency and making sense. Our CT model with $z\\,=\\,7$ (which has   G for full details. As shown in Figure 3 (left) and Figure 4, our repetition-controlled model improves hugely over the beam search baseline in all metrics, and achieves close-to-human scores on all metrics except humanness. This striking result demonstrates that repetition is by far the biggest limiting quality factor for naive sequence-to-sequence dialogue agents. The result also emphasizes the importance of multi-turn dialogue evaluation to detect the problem. We refer to this model as the repetitioncontrolled baseline, and use it as a basis for all remaining experiments (i.e., we control specificity, response-relatedness and question-asking on top of these repetition-control settings). ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-44f4fb5bff825e761bf2d6022ff9e493",
        "description": "The image consists of three graphs and a bar chart, all related to the engagingness of different models in dialogue systems. The left graph is a bar chart with five bars representing different models: Greedy, Beam search (WD), Repetition (WD), Specificity (CT), Question (CT), and Human. The engagingness scores range from approximately 2.2 to 3.0. The Human model has the highest score at around 3.0, followed by the Specificity (CT) model at about 2.95, and the Question (CT) model at around 2.9. The Greedy model has the lowest score at around 2.2. The middle graph shows the engagingness for different specificity control levels (WD). The x-axis ranges from -10 to 10, indicating more generic to more specific settings. The y-axis represents engagingness scores ranging from 2.2 to 3.2. The graph includes lines for Specificity-controlled WD, Beam search baseline, Human, and Repetition-controlled baseline. The Specificity-controlled WD line peaks at a specificity level of 4, with an engagingness score of about 3.0. The right graph displays the engagingness for different question-asking control levels (CT). The x-axis ranges from 0 to 10, indicating fewer to more questions. The y-axis represents engagingness scores ranging from 2.2 to 3.2. The graph includes lines for Question-controlled CT, Beam search baseline, Human, and Repetition-controlled baseline. The Question-controlled CT line peaks at a question-asking level of 7, with an engagingness score of about 3.0.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_6.jpg",
        "caption": [
            "Figure 4: Calibrated human judgments of conversational aspects for the baselines and best controlled models. Note: In Figure 3 and here, the Specificity and Question controlled models both include Repetition control, but Question control doesn’t include Specificity control, or vice versa. "
        ],
        "footnote": [],
        "context": "Specificity (WD, CT) For our weighted decoding models, the extreme settings (very generic and very specific) score poorly in engagingness due to the frequent presence of degenerate output – see Figure 3 (middle). We find that the weight $=\\,4$ setting (which is more specific than the repetitioncontrolled baseline and about as specific as the gold data) maximizes engagingness. As shown in Figure 3 (left) and Figure 4, this more-specific model is rated more interesting, engaging, and a better listener than the repetition-controlled baseline, but at the cost of reduced fluency and making sense. Our CT model with $z\\,=\\,7$ (which has  G for full details. As shown in Figure 3 (left) and Figure 4, our repetition-controlled model improves hugely over the beam search baseline in all metrics, and achieves close-to-human scores on all metrics except humanness. This striking result demonstrates that repetition is by far the biggest limiting quality factor for naive sequence-to-sequence dialogue agents. The result also emphasizes the importance of multi-turn dialogue evaluation to detect the problem. We refer to this model as the repetitioncontrolled baseline, and use it as a basis for all remaining experiments (i.e., we control specificity, response-relatedness and question-asking on top of these repetition-control settings).  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-44f4fb5bff825e761bf2d6022ff9e493",
        "description": "The image is a series of bar charts comparing various conversational aspects for different models and human judgments. The x-axis represents different models: Greedy search, Beam search, Repetition-controlled (WD), Specificity-controlled (WD), Question-controlled (CT), and Human. The y-axis represents the scores for each aspect, ranging from 2.0 to 3.5. The aspects evaluated are Avoiding Repetition, Interestingness, Making Sense, Fluency, Listening, Inquisitiveness, and Humanness. Each chart shows the scores for each model in different colors: Green for Greedy search, Blue for Beam search, Purple for Repetition-controlled (WD), Pink for Specificity-controlled (WD), Gray for Question-controlled (CT), and Orange for Human. For example, in the 'Avoiding Repetition' chart, the Human score is around 3.0, while the Greedy search score is around 2.1. The 'Interestingness' chart shows the Human score at around 2.8, with the Repetition-controlled (WD) model scoring slightly higher at around 2.9. The 'Making Sense' chart has the Human score at around 3.6, with the Repetition-controlled (WD) model scoring around 3.4. The 'Fluency' chart shows the Human score at around 3.4, with the Repetition-controlled (WD) model scoring around 3.2. The 'Listening' chart has the Human score at around 3.3, with the Repetition-controlled (WD) model scoring around 3.1. The 'Inquisitiveness' chart shows the Human score at around 2.7, with the Repetition-controlled (WD) model scoring around 2.6. The 'Humanness' chart has the Human score at around 3.5, with the Repetition-controlled (WD) model scoring around 3.3. Overall, the Human scores are consistently higher than the model scores, indicating that humans are rated better in all conversational aspects.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_7.jpg",
        "caption": [],
        "footnote": [
            "Table $3\\colon\\mathrm{A}/\\mathrm{B}$ tests comparing various specificity-controlled models to the repetition-controlled baseline on interestingness. We find all comparisons are significant ( $p<.05$ ; binomial test). "
        ],
        "context": "Though our more-specific models yielded signifi- cant improvements in engagingness, we were surprised that they did not yield clearer improvements in interestingness. To investigate further, we conducted an A/B interestingness evaluation of three specificity-controlled models, compared to the repetition-controlled baseline. Crowdworkers were shown two conversations (from the main human evaluation) and asked to choose which model was more interesting (see Figure 7 for details). We collected 8.3 A/B tests for interestingness For time and budget reasons, we did not evaluate any models controlling both question-asking and specificity. However, we expect it is possible to obtain further improvements by doing so.  and the human-produced gold data $(28.8\\%)$ , brings us closest to human-level engagingness – see Figure 3 (left). Although we find that a rate of approximately $65.7\\%$ questionasking is the most engaging, a lower level $(48.9\\%$ ,or $z=4$ ) is rated the best listener. Lastly, we find that although asking too many questions is less engaging, most crowdworkers will not directly criticize a chatbot that asks questions on every turn – only $11.9\\%$ of crowdworkers judged the $z\\,=\\,10$ (boost) setting, which asks $99.5\\%$ questions, as asking too many questions. For full details of these scores, see Appendix F and H. ",
        "chunk_order_index": 7,
        "chunk_id": "chunk-52f18fc482be6f297ab38f40ffc0749a",
        "description": "The image is a table labeled 'Table 3: A/B tests comparing various specificity-controlled models to the repetition-controlled baseline on interestingness.' The table contains three rows, each representing a different model. The columns are as follows: Model, Win%, and Top 3 reasons for preferring model. The first row shows 'Specificity WD (weight = 6)' with a Win% of 84.1%. The top 3 reasons for preferring this model are 'More information,' 'Better flow,' and 'More descriptive.' The second row shows 'Specificity WD (weight = 4)' with a Win% of 75.5%. The top 3 reasons for preferring this model are 'More information,' 'They describe their life in more detail,' and 'Funny.' The third row shows 'Specificity CT (z = 7)' with a Win% of 56.2%. The top 3 reasons for preferring this model are 'More information,' 'Better flow,' and 'Seems more interested.'",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_8.jpg",
        "caption": [
            "Figure 6: Screenshot of the chat UI, talking with the beam search baseline model. "
        ],
        "footnote": [],
        "context": "Table 4: We define five binary features for controlling different types of repetition via weighted decoding (see Section 5.2). Each feature depends on the word $w$ , the partial hypothesis $y_{<t}$ , and the context $x$ (which includes the  Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT ${\\bf\\nabla}z=7)$ ), (d) Specificity-controlled WD (weight $=4\\AA$ ).   Here are the questions and multiple-choice options used in the human evaluation, in the order presented: B Human evaluation questionnaire design Figure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled baseline model (right).  have will be made public, so act as you would e.g. on a public social network like Twitter. 2. Please do not send long messages: messages cannot exceed 30 words. 3. Please do not reference the task or MTurk itself during the conversation, but speak naturally to the other person. 4. Please do not send any message that could make others uncomfortable, including any level of discrimination, racism, sexism and offensive religious/politics comments, otherwise the submission will be rejected. Note: the user you are chating with may be a human or a bot. Figure 5: Screenshot of the Task Description ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a screenshot of a chat UI featuring a conversation between two users, PERSON_1 and PERSON_2. The chat interface has a light background with alternating light beige and light blue message bubbles. The conversation begins with PERSON_2 expressing their love for coffee. PERSON_1 responds positively, using the phrase 'buzz buzz buzz' to emphasize their enthusiasm. The conversation continues with both users agreeing on their fondness for coffee. PERSON_1 then asks if PERSON_2 speaks French, indicating an interest in learning the language. The conversation shifts to favorite colors, with PERSON_2 mentioning a preference for blue but also liking yellow. On the left side of the image, there is a task description box with a light green background. It outlines the task of chatting with another user while playing the role of a given character. The assigned character details include being a vegetarian, enjoying swimming, having a father who worked for Ford, and a new job in advertising design. The character also studies languages, with Spanish as a current focus and French as the next language to study.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 4: We define five binary features for controlling different types of repetition via weighted decoding (see Section 5.2). Each feature depends on the word $w$ , the partial hypothesis $y_{<t}$ , and the context $x$ (which includes the  Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT ${\\bf\\nabla}z=7)$ ), (d) Specificity-controlled WD (weight $=4\\AA$ ).   Here are the questions and multiple-choice options used in the human evaluation, in the order presented: B Human evaluation questionnaire design Figure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled baseline model (right). have will be made public, so act as you would e.g. on a public social network like Twitter. 2. Please do not send long messages: messages cannot exceed 30 words. 3. Please do not reference the task or MTurk itself during the conversation, but speak naturally to the other person. 4. Please do not send any message that could make others uncomfortable, including any level of discrimination, racism, sexism and offensive religious/politics comments, otherwise the submission will be rejected. Note: the user you are chating with may be a human or a bot. Figure 5: Screenshot of the Task Description  ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a screenshot of a human evaluation questionnaire design for comparing two conversations, one between humans and the other between a human and a repetition-controlled baseline model. The layout includes instructions at the top, followed by two conversation threads side by side. The left thread (Speaker 1) is in blue speech bubbles, while the right thread (Speaker 2) is in green speech bubbles. The grey speech bubbles represent the other speakers. The conversation starts with a greeting and progresses through various topics such as personal interests, hobbies, and lifestyle choices. The participants discuss board games, Twitch streaming, dietary habits, and family backgrounds. The questionnaire asks the evaluator to choose which speaker they find more interesting and provides a space for a brief reason explaining their choice. The options are 'Speaker 1' or 'Speaker 2'.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_10.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 5: Control settings for all configurations that were human-evaluated. ‘wt’ means  Table 4: We define five binary features for controlling different types of repetition via weighted decoding (see Section 5.2). Each feature depends on the word $w$ , the partial hypothesis $y_{<t}$ , and the context $x$ (which includes the model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the condition on the right is true; otherwise 0.  Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT ${\\bf\\nabla}z=7)$ ), (d) Specificity-controlled WD (weight $=4\\AA$ ).  during the conversation, but speak naturally to the other person. 4. Please do not send any message that could make others uncomfortable, including any level of discrimination, racism, sexism and offensive religious/politics comments, otherwise the submission will be rejected. Note: the user you are chating with may be a human or a bot. Figure 5: Screenshot of the Task Description   Figure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled baseline model (right). B Human evaluation questionnaire design Here are the questions and multiple-choice options used in the human evaluation, in the order presented: ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a questionnaire designed for human evaluation of conversational interactions. It contains multiple-choice questions aimed at assessing various aspects of the conversation, including engagingness, interestingness, inquisitiveness, listening, avoiding repetition, fluency, making sense, humanness, and persona retrieval. Each question is followed by a set of options ranging from negative to positive or from low to high. For example, the first question asks about how much the user enjoyed talking to the other user, with options ranging from 'Not at all' to 'A lot'. The second question evaluates the interestingness of the conversation, with options from 'Very boring' to 'Very interesting'. The third question assesses how much the user tried to get to know the evaluator, with options from 'Didn’t ask about me at all' to 'Asked about me too much'. The fourth question measures how much the user paid attention to what the evaluator said, with options from 'Always ignored what I said' to 'Always paid attention to what I said'. The fifth question evaluates how repetitive the user was, with options from 'Repeated themselves over and over' to 'Always said something new'. The sixth question assesses how naturally the user spoke English, with options from 'Very unnatural' to 'Very natural'. The seventh question evaluates how often the user said something that did not make sense, with options from 'Never made any sense' to 'Everything made perfect sense'. The eighth question asks whether the user is a bot or a human, with options from 'Definitely a bot' to 'Definitely a human'. The final question asks which prompt (character) the evaluator thinks the other user was given for this conversation, with the respondent choosing one of two provided personas.",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_11.jpg",
        "caption": [
            "C Example conversations from human evaluation "
        ],
        "footnote": [],
        "context": "Table 5: Control settings for all configurations that were human-evaluated. ‘wt’ means  Table 4: We define five binary features for controlling different types of repetition via weighted decoding (see Section 5.2). Each feature depends on the word $w$ , the partial hypothesis $y_{<t}$ , and the context $x$ (which includes the model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the condition on the right is true; otherwise 0.  Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT ${\\bf\\nabla}z=7)$ ), (d) Specificity-controlled WD (weight $=4\\AA$ ). during the conversation, but speak naturally to the other person. 4. Please do not send any message that could make others uncomfortable, including any level of discrimination, racism, sexism and offensive religious/politics comments, otherwise the submission will be rejected. Note: the user you are chating with may be a human or a bot. Figure 5: Screenshot of the Task Description   Figure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled baseline model (right). B Human evaluation questionnaire design Here are the questions and multiple-choice options used in the human evaluation, in the order presented:  ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a composite of four conversation screenshots labeled (a), (b), (c), and (d). Each screenshot represents a different type of dialogue interaction, likely for the purpose of evaluating conversational AI systems. The conversations are structured as text exchanges between a human and a bot. \\n\\n(a) Baseline: The conversation starts with the human asking about the bot's profession, leading to a discussion about religious beliefs and personal interests. The bot consistently responds with 'I am a buddhist. What about you?' regardless of the human's input. \\n\\n(b) Repetition-controlled baseline: This conversation begins with the human introducing themselves as a little girl living in the cloud and feeling lonely. The bot engages in a friendly chat, discussing hobbies like being a superhero and enjoying shopping and chocolate. \\n\\n(c) Question-controlled CT: The human initiates the conversation by asking how the bot is doing today, leading to a discussion about pets and books. The bot mentions walking dogs for a living and enjoying reading Harry Potter. \\n\\n(d) Specificity-controlled WD: The conversation starts with the human asking about the bot's day, leading to a discussion about music and professions. The bot expresses enjoyment in playing piano and singing along to folk music and mentions working as an auditor.",
        "segmentation": false
    },
    "image_12": {
        "image_id": 12,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_12.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 5: Control settings for all configurations that were human-evaluated. ‘wt’ means the weight used for a weighted decoding feature and $\\mathbf{\\dot{\\omega}}_{z}=\\mathbf{\\dot{\\omega}}$ means the setting (i.e. bucket) for the control variable in conditional  Table 4: We define five binary features for controlling different types of repetition via weighted decoding (see Section 5.2). Each feature depends on the word $w$ , the partial hypothesis $y_{<t}$ , and the context $x$ (which includes the model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the condition on the right is true; otherwise 0. uncomfortable, including any level of discrimination, racism, sexism and offensive religious/politics comments, otherwise the submission will be rejected. Note: the user you are chating with may be a human or a bot. Figure 5: Screenshot of the Task Description   Figure 7: Screenshot of the A/B test UI, comparing a human-human conversation (left) and a Repetition-controlled baseline model (right). B Human evaluation questionnaire design Here are the questions and multiple-choice options used in the human evaluation, in the order presented:   Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT ${\\bf\\nabla}z=7)$ ), (d) Specificity-controlled WD (weight $=4\\AA$ ). ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a table that outlines five binary features used for controlling different types of repetition via weighted decoding. The table has two columns: 'Feature' and 'Condition'. The rows are as follows:\\n1. extrep_bigram(w, y<t, x): This feature is activated if adding the word w to the hypothesis y<t would create a 2-gram that appears in a previous utterance by the model.\\n2. extrep_unigram(w, y<t, x): This feature is triggered if w is a non-stopword and w appears in a previous utterance by the model.\\n3. intrep_bigram(w, y<t, x): This feature is active if adding w to the hypothesis y<t would create a 2-gram that appears earlier in the hypothesis y<t.\\n4. intrep_unigram(w, y<t, x): This feature is set if w is a non-stopword and w appears earlier in the hypothesis y<t.\\n5. partnerrep_bigram(w, y<t, x): This feature is enabled if adding w to the hypothesis y<t would create a 2-gram that appears in a previous utterance by the partner.\\nThe table provides a clear and structured way to understand how these features control repetition in dialogue generation.",
        "segmentation": false
    },
    "image_13": {
        "image_id": 13,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_13.jpg",
        "caption": [],
        "footnote": [],
        "context": "$^{\\ast\\ast}$ Note that the Response-related controlled models additionally introduce repetition controls to block internal bigram repetition and partner bigram repetition. This was necessary to prevent the model from parroting \\* In the setting Question-controlled CT 10 (boost), the feature extrep bigram is not used for weighted decoding during beam search, but it is used to rerank the candidates after beam search. See Section 6.4 for details. Table 5: Control settings for all configurations that were human-evaluated. ‘wt’ means the weight used for a weighted decoding feature and $\\mathbf{\\dot{\\omega}}_{z}=\\mathbf{\\dot{\\omega}}$ means the setting (i.e. bucket) for the control variable in conditional training. and multiple-choice options used in the human evaluation, in the order presented:   Figure 8: Example conversation with (a) Baseline (b) Repetition-controlled baseline (c) Question-controlled CT ${\\bf\\nabla}z=7)$ ), (d) Specificity-controlled WD (weight $=4\\AA$ ).  Table 4: We define five binary features for controlling different types of repetition via weighted decoding (see Section 5.2). Each feature depends on the word $w$ , the partial hypothesis $y_{<t}$ , and the context $x$ (which includes the model’s own persona and the dialogue history). Each of these features is equal to 1 if and only if the condition on the right is true; otherwise 0. ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a table labeled 'Table 5: Control settings for all configurations that were human-evaluated.' The table is structured with multiple columns and rows, detailing various control settings for different models. The columns include 'Repetition,' 'Specificity,' 'Response-rel,' and 'Questions.' Under 'Repetition,' there are sub-columns for 'External Bigram,' 'External Unigram,' 'Internal Bigram,' 'Internal Unigram,' and 'Partner Rep. Bigram.' The 'Specificity' column has a sub-column for 'NIDF,' and the 'Response-rel' column has a sub-column for 'Cos sim.' The 'Questions' column has a sub-column for 'Has '?''.\\n\\nThe table begins with 'Baselines' and lists two methods: 'Greedy Search' and 'Beam Search (beam size 20).' Following this, there are sections for 'Repetition control (WD),' 'Question control (CT),' 'Specificity control (CT),' 'Specificity control (WD),' and 'Response-related control (WD).' Each section contains rows with specific settings and weights (wt) for different features.\\n\\nFor example, in the 'Repetition control (WD)' section, there are entries like 'Extrep bigram WD -0.5' with wt -0.5, and 'Extrep bigram WD -inf' with wt -∞. In the 'Question control (CT)' section, there are entries like 'Question-controlled CT 0' with wt -3.5 and z = 0, and 'Question-controlled CT 10 (boost)' with wt 0 * and z = 10.\\n\\nThe table provides detailed information on how different control settings affect the model's performance, with specific weights assigned to various features to control repetition, specificity, and response-related aspects.",
        "segmentation": false
    },
    "image_14": {
        "image_id": 14,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_14.jpg",
        "caption": [
            "F Automatic metrics for all configurations "
        ],
        "footnote": [
            "Table 6: Automatic metrics (computed over validation set) for all model configurations that were human-evaluated. "
        ],
        "context": "The maximum The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better.  The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better.  is used to rerank the candidates after beam search. See Section 6.4 for details. $^{\\ast\\ast}$ Note that the Response-related controlled models additionally introduce repetition controls to block internal bigram repetition and partner bigram repetition. This was necessary to prevent the model from parroting the partner’s last utterance. In Table 8, we find that just adding these extra repetition controls (here called Responserelated controlled WD 0, i.e. increased repetition control but no response-relatedness control) outperforms our canonical Repetition-controlled baseline. However, given that we discovered this later, our specificity and question controlled models are built on top of the canonical Repetition-controlled baseline. ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a detailed table labeled 'Automatic metrics for all configurations'. The table is divided into several columns and rows, presenting various metrics computed over a validation set for different model configurations. The columns are as follows: 'Repetition', which is further subdivided into 'External' (with sub-columns 'Bigram' and 'Unigram') and 'Internal' (with sub-columns 'Bigram' and 'Unigram'), and 'Partner Rep.' (with a single 'Bigram' column); 'Specificity'; 'Response-rel'; and 'Questions'. Each of these main columns contains multiple rows corresponding to different configurations or baselines. For example, the 'Gold Data and baselines' section includes 'Gold Data', 'Greedy Search', and 'Beam Search (beam size 20)'. Specific values include 'Gold Data' having an External Bigram of 4.65%, External Unigram of 9.62%, Internal Bigram of 0.38%, Internal Unigram of 0.97%, Partner Rep. Bigram of 5.10%, Specificity of 0.2119, Response-rel Cos sim of 0.1691, and Questions Has '?' of 28.80%. Other configurations show varying percentages and values, such as 'Beam Search (beam size 20)' with External Bigram at 46.85% and External Unigram at 44.15%. The table also includes sections for 'Repetition control (WD)', 'Question control (CT)', 'Specificity control (CT)', 'Specificity control (WD)', and 'Response-related control (WD)', each with specific values for the aforementioned metrics.",
        "segmentation": false
    },
    "image_15": {
        "image_id": 15,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_15.jpg",
        "caption": [
            "G Human evaluation results for all configurations "
        ],
        "footnote": [
            "Table 7: Raw scores (mean $\\pm$ std.) for all models and human evaluation metrics. "
        ],
        "context": "The maximum The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better.  The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. is used to rerank the candidates after beam search. See Section 6.4 for details. $^{\\ast\\ast}$ Note that the Response-related controlled models additionally introduce repetition controls to block internal bigram repetition and partner bigram repetition. This was necessary to prevent the model from parroting the partner’s last utterance. In Table 8, we find that just adding these extra repetition controls (here called Responserelated controlled WD 0, i.e. increased repetition control but no response-relatedness control) outperforms our canonical Repetition-controlled baseline. However, given that we discovered this later, our specificity and question controlled models are built on top of the canonical Repetition-controlled baseline.  ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a table labeled 'Table 7: Raw scores (mean ± std.) for all models and human evaluation metrics.' The table presents the results of human evaluations for various configurations of models. The columns represent different evaluation metrics, including Avoiding Repetition, Engage, Fluency, Humanness, Inquisitive, Interesting, Listening, Make Sense, and Persona Retrieval. Each row corresponds to a different model or configuration. The first row represents the Human baseline, followed by Greedy Search and Beam Search (beam size 20). Subsequent rows detail various repetition control, question control, specificity control, and response-related control configurations. For example, the Extrep bigram WD -0.5 model scores 2.66 ± 0.56 in Avoiding Repetition, 2.56 ± 0.92 in Engage, and so on. The maximum score for most metrics is 4, except for Avoiding Repetition which has a maximum of 3, and Inquisitive which has an optimal score of 3. The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. Notable data points include the Question-controlled CT 7 model scoring 3.07 ± 0.90 in Engage and the Specificity-controlled WD 8 model scoring 3.06 ± 0.80 in Listening.",
        "segmentation": false
    },
    "image_16": {
        "image_id": 16,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_16.jpg",
        "caption": [
            "Table 8: Calibrated scores (mean $\\pm$ std.) for all models and human evaluation metrics. "
        ],
        "footnote": [],
        "context": "Note: ‘Repetition-controlled baseline+’ in the rightmost Figure 9: Calibrated human evaluation scores for all models. This is the same data as in Table 8.  Rows marked with \\* are the six models included in Figure 3 (left) and Figure 4. The maximum of each column (excluding Human row) is in bold. The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. 8, we find that just adding these extra repetition controls (here called Responserelated controlled WD 0, i.e. increased repetition control but no response-relatedness control) outperforms our canonical Repetition-controlled baseline. However, given that we discovered this later, our specificity and question controlled models are built on top of the canonical Repetition-controlled baseline.   The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a table labeled 'Table 8: Calibrated scores (mean ± std.) for all models and human evaluation metrics.' The table is structured with rows representing different models and columns representing various evaluation metrics. The metrics include Avoiding Repetition, Engage, Fluency, Humanness, Inquisitive, Interesting, Listening, and Make Sense. Each cell in the table contains a mean score followed by a standard deviation. For example, the Human model has a score of 2.79 ± 0.12 for Avoiding Repetition, 3.04 ± 0.11 for Engage, and so on. The Greedy Search model has a score of 2.08 ± 0.10 for Avoiding Repetition, 2.24 ± 0.11 for Engage, and so on. The Beam Search (beam size 20) model has a score of 2.08 ± 0.11 for Avoiding Repetition, 2.29 ± 0.11 for Engage, and so on. The table also includes scores for various repetition control models, question control models, specificity control models, and response-related control models. The maximum of each column (excluding the Human row) is highlighted in bold. For instance, the Extrep bigram WD -3.5 model has the highest score of 3.56 ± 0.10 for Fluency. The table provides detailed numerical data for each model across the evaluation metrics.",
        "segmentation": false
    },
    "image_17": {
        "image_id": 17,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1170/images/image_17.jpg",
        "caption": [
            "H Plots of human evaluation results for all configurations "
        ],
        "footnote": [],
        "context": "Note: ‘Repetition-controlled baseline+’ in the rightmost column is ‘Response-related controlled WD $\\mathrm{0'}$ in Table 8. See Table 5 for explanation. Figure 9: Calibrated human evaluation scores for all models. This is the same data as in Table 8. which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better.  The first eight columns are Likert metrics on a 1-4 scale (except Avoiding Repetition, which is a 1-3 scale), where higher is better (except Inquisitiveness, which has an optimal score of 3). The last column, Persona Retrieval, is on a scale from 0 to 1 where higher is better. The maximum of each column (excluding Human row) is in bold. Rows marked with \\* are the six models included in Figure 3 (left) and Figure 4. ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-8254eb704a12c68df90e37de4dd27b96",
        "description": "The image is a series of line graphs depicting human evaluation results for various configurations. Each graph has multiple lines representing different models: 'Question-controlled CT', 'Beam search baseline', 'Greedy search baseline', 'Human', and 'Repetition-controlled baseline'. The x-axis varies across the graphs, showing parameters such as 'Fewer Questions', 'More Questions', 'Generic', 'Specific', 'More Generic', 'More Specific', 'More unrelated', 'No control', 'More related'. The y-axis represents the evaluation scores, ranging from 2.0 to 3.6. The graphs are organized into rows and columns, each row corresponding to a different evaluation metric (e.g., Fluency, Engagingness, Humanness, Making Sense, Interestingness, Avoiding Repetition, Listening). The maximum score in each column (excluding the 'Human' row) is highlighted in bold. The graphs show trends where certain models perform better under specific conditions. For example, the 'Question-controlled CT' model generally performs well with more questions, while the 'Repetition-controlled baseline' shows high scores in avoiding repetition.",
        "segmentation": false
    }
}