{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Extractive approaches are typically simpler. They output the probability of each sentence to be selected into the Hence, abstractive summaries can be more coherent and concise than extractive summaries. Figure 1: Comparison of extractive, abstractive, and our unified summaries on a news article. The extractive model picks most important but incoherent or not concise (see blue bold font) sentences. The abstractive summary is readable, concise but still loses or mistakes some facts (see red italics font). The final summary rewritten from fragments (see underline font) has the advantages from both extractive (importance) and abstractive advantage (coherence (see green bold font)). and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation. 1 Introduction Text summarization is the task of automatically condensing a piece of text to a shorter version while maintaining the important points. The ability to condense text information can aid many applications such as creating news digests, presenting search results, and generating reports. There are mainly two types of approaches: extractive and abstractive. Extractive approaches assemble summaries directly from the source text typically selecting one whole sentence at a time. In contrast, abstractive approaches can generate novel words and phrases not copied from the source text. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-bbbf85014b02bd3a8207b00b0a8e5f0f",
        "description": "The image is a table comparing different summarization approaches applied to a news article. The table is divided into four main sections: Original Article, Extractive Approach, Abstractive Approach, and Unified Approach. Each section contains text describing the summary generated by each approach. The Original Article section details McDonald's announcement about the new 'Artisan Grilled Chicken' product, its availability in stores, and changes in ingredients. The Extractive Approach section highlights key sentences from the original article, marked in blue bold font, focusing on the product's availability and ingredient changes. The Abstractive Approach section provides a more coherent summary, marked in red italics font, but with some factual inaccuracies. The Unified Approach section combines the advantages of both extractive and abstractive approaches, marked in green bold font, providing a concise and coherent summary. The table is structured with rows and columns, where each row represents a different summarization approach and each column represents a specific aspect of the summary.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_2.jpg",
        "caption": [
            "Figure 2: Our unified model combines the word-level and sentence-level attentions. Inconsistency occurs when word attention is high but sentence attention is low (see red arrow). "
        ],
        "footnote": [],
        "context": "Our model combines state-of-the-art extractive model (Nallapati et al., 2017) and abstractive model (See et al., 2017) by combining sentencelevel Hierarchical attention. Attention mechanism was first proposed by Bahdanau et al. (2014). Yang et al. (2016) proposed a hierarchical attention mechanism for document classification. We adopt the method of combining sentence-level and word-level attention in Nallapati et al. (2016b). However, their sentence attention is dynamic, which means it will be different for each generated word. Whereas our sentence attention is fixed for all generated words. Inspired by the high performance of extractive summarization, we propose to use fixed sentence attention.  al. (2017) combine pointer networks (Vinyals et al., 2015) into their models to deal with out-of-vocabulary (OOV) words. Chen et al. (2016) and See et al. (2017) restrain their models from attending to the same word to decrease repeated phrases in the generated summary. Paulus et al. (2017) use policy gradient on summarization and state out the fact that high ROUGE scores might still lead to low human evaluation scores. Fan et al. (2017) apply convolutional sequenceto-sequence model and design several new tasks for summarization. Liu et al. (2017) achieve high readability score on human evaluation using generative adversarial networks. ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-1fb531dbc09f8fa236ca5aca4b6abf65",
        "description": "The image is a comparative bar chart illustrating the attention distribution across sentences and words in a model that combines word-level and sentence-level attentions. The chart is divided into two main sections. On the left, there are three groups of bars representing Sentence 1, Sentence 2, and Sentence 3. Each group contains multiple bars of different colors: orange, blue, and green. The orange bars represent the word-level attention, the blue bars represent the sentence-level attention, and the green bars represent the combined attention. A red arrow labeled 'Inconsistent' points to a high word-level attention (orange bar) in Sentence 2 where the sentence-level attention (blue bar) is low. This indicates an inconsistency in the attention distribution. The right section of the chart shows the same three sentences after multiplying and renormalizing the attentions. Here, the inconsistencies are attenuated, as indicated by the red arrow labeled 'Attenuated'. The y-axis ranges from 0 to 1, representing the attention values. The x-axis labels the sentences. The chart highlights the process of combining and normalizing attentions to address inconsistencies.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_3.jpg",
        "caption": [
            "Figure 3: Architecture of the extractor. We treat the sigmoid output of each sentence as sentencelevel attention $\\in[0,1]$ ."
        ],
        "footnote": [],
        "context": "where $\\kappa$ is the set of top $\\mathbf{K}$ attended words and $T$ is the number of words in the summary. This implicitly encourages the distribution $$ L_{i n c}=-\\frac{1}{T}\\sum_{t=1}^{T}\\log(\\frac{1}{\\vert K\\vert}\\sum_{m\\in K}\\alpha_{m}^{t}\\times\\beta_{n(m)}), $$ Instead of only leveraging the complementary nature between sentence-level and word-level attentions, we would like to encourage these two-levels of attentions to be mostly consistent to each other during training as an intrinsic learning target for free (i.e., without additional human annotation). Explicitly, we would like the sentence-level attention to be high when the word-level attention is high. Hence, we design the following inconsistency loss, 3.2 Inconsistency Loss  word attention $\\hat{\\alpha}_{m}^{t}$ is $$ \\hat{\\alpha}_{m}^{t}=\\frac{\\alpha_{m}^{t}\\times\\beta_{n(m)}}{\\sum_{m}\\alpha_{m}^{t}\\times\\beta_{n(m)}}. $$ The multiplication ensures that only when both word-level $\\alpha_{m}^{t}$ and sentence-level $\\beta_{n}$ attentions are high, the updated word attention $\\hat{\\alpha}_{m}^{t}$ can be high. Since the sentence-level attention $\\beta_{n}$ from the extractor already achieves high ROUGE scores, $\\beta_{n}$ intuitively modulates the word-level attention $\\alpha_{m}^{t}$ to mitigate spurious word-level attention such that words in less attended sentences are less likely to be generated (see Fig. 2). As highlighted in Sec. 3.4, the word-level attention $\\hat{\\alpha}_{m}^{t}$ significantly affects the decoding process of the abstracter. Hence, an updated word-level attention is our key to improve abstractive summarization. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-ee09fc2a680ceacea336fecadc98ee8f",
        "description": "The image is a diagram illustrating the architecture of an extractor used in a neural network model for text summarization. The diagram is divided into three main layers: Word-level RNN, Sentence-level RNN, and Sentence-Level Attention. The Word-level RNN layer consists of nine GRU (Gated Recurrent Unit) blocks, each representing a word in a sentence. These blocks are connected sequentially from left to right, indicating the flow of information through the network. Each GRU block is labeled with a 'w' followed by a subscript number (e.g., w1, w2, ..., w9), representing individual words in the input sequence. The Sentence-level RNN layer contains three GRU blocks, which are connected to specific word-level GRU blocks. These connections are depicted by arrows pointing upwards from the word-level GRU blocks to the sentence-level GRU blocks. The Sentence-Level Attention layer is represented by three circles at the top of the diagram, each containing a value (0.9, 0.2, and 0.5). These values represent the attention weights assigned to each sentence by the model. The attention weights are used to determine the importance of each sentence in the overall summary. The diagram also includes dashed lines connecting the word-level GRU blocks to the corresponding sentence-level GRU blocks, indicating the flow of information between these layers. The overall architecture suggests a hierarchical approach to text summarization, where word-level information is first processed, then aggregated at the sentence level, and finally weighted using attention mechanisms to generate a summary.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg",
        "caption": [],
        "footnote": [],
        "context": "Pointer-generator network. The pointergenerator network (See et al., 2017) is a specially designed sequence-to-sequence attentional model that can generate the summary by copying words in the article or generating words from a fixed vocabulary at the same time. The model contains a bidirectional LSTM which serves as an encoder to encode the input words w and a unidirectional LSTM which serves as a decoder to generate the summary y. For details of the network word-by-word. We use the pointer-generator network proposed by See et al. (2017) and combine it with the extractor by combining sentence-level and word-level attentions (Sec. 3.1).  aim to extract a final summary for an article so they use ROUGE F-1 score to select ground-truth sentences; while we focus on high informativity, hence, we use ROUGE recall score to obtain as much information as possible with respect to the reference summary $\\hat{\\bf y}$ . 3.4 Abstracter The second part of our model is an abstracter that reads the article; then, generate a summary Figure 4: Decoding mechanism in the abstracter. In the decoder step $t$ , our updated word attention $\\hat{\\mathbf{\\alpha}}\\alpha^{t}$ is used to generate context vector $h^{\\ast}(\\hat{\\alpha}^{t})$ . Hence, it updates the final word distribution Pfinal. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-49ce552ebe533f90b8b307c591f92a76",
        "description": "The image is a detailed diagram illustrating the decoding mechanism in a pointer-generator network, a type of sequence-to-sequence attentional model used for text summarization. The diagram is divided into several key components: \\n\\n1. **Encoder Hidden States**: Represented by a series of vertical bars at the bottom left, these states are denoted as {h_e^1, ..., h_e^N}, indicating the hidden states generated by a bidirectional LSTM encoder from the input words. \\n2. **Decoder Hidden State**: Shown as a single vertical bar on the right side, labeled as h_d^t, representing the hidden state of the unidirectional LSTM decoder at step t. \\n3. **Context Vector**: Depicted as a single vertical bar in the center, labeled as h*(α̂^t), this vector is generated using the updated word attention α̂^t. \\n4. **Word Distribution p_vocab**: Illustrated by a series of vertical bars on the far right, this distribution represents the probability of generating words from a fixed vocabulary. \\n5. **Final Word Distribution p_final**: Shown by a series of vertical bars at the top, this distribution combines the word distribution from the vocabulary and the context vector to generate the final output. \\n6. **Updated Word Attention α̂^t**: Represented by a series of vertical bars on the left, this attention mechanism updates the context vector based on the current decoder state. \\n7. **p_gen**: A scalar value that determines the probability of generating a word from the vocabulary or copying a word from the input. \\n8. **1 - p_gen**: Another scalar value that complements p_gen, determining the probability of copying a word from the input. \\nThe arrows and connections between these components illustrate the flow of information during the decoding process. The diagram highlights how the model dynamically switches between generating words from a vocabulary and copying words from the input text, depending on the values of p_gen and 1 - p_gen.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note that ground-truth labels show the upper-bound performance since the reference summary to calculate ROUGE-recall is abstractive. All our ROUGE scores have a $95\\%$ confidence interval with at most $\\pm0.33$ . we perform human evaluation and show that our model can provide a better abstractive summary than other baselines. 5.1 Results of Extracted Sentences To evaluate whether our extractor obtains enough information for the abstracter, we use full-length ROUGE recall scores1 between the extracted sentences and reference abstractive summary. High ROUGE recall scores can be obtained if the extracted sentences include more words or sequences overlapping with the reference abstractive summary. For each article, we select sentences with the sentence probabilities $\\beta$ greater than 0.5. We show the results of the ground-truth sentence labels (Sec. 3.3) and our models on the ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-d70cc97368a6b60e1a3179333970577d",
        "description": "The image is a table that presents ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. The table has four columns: Method, ROUGE-1, ROUGE-2, and ROUGE-L. The rows represent different methods used for generating summaries. The first row shows the pre-trained method with ROUGE-1 score of 73.50, ROUGE-2 score of 35.55, and ROUGE-L score of 68.57. The second row shows the end-to-end method without inconsistency loss with ROUGE-1 score of 72.97, ROUGE-2 score of 35.11, and ROUGE-L score of 67.99. The third row shows the end-to-end method with inconsistency loss with ROUGE-1 score of 78.40, ROUGE-2 score of 39.45, and ROUGE-L score of 73.83. The last row shows the ground-truth labels with ROUGE-1 score of 89.23, ROUGE-2 score of 49.36, and ROUGE-L score of 85.46. The table highlights the performance of different methods in generating abstractive summaries.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "test set of the CNN/Daily Mail dataset in Table 1. Note that the ground-truth extracted sentences can’t get ROUGE recall scores of 100 because reference summary is Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores have a $95\\%$ confidence interval with at most $\\pm0.24$ . ‘∗’ indicates the model is trained and evaluated on the anonymized dataset and thus is not strictly comparable with ours. include more words or sequences overlapping with the reference abstractive summary. For each article, we select sentences with the sentence probabilities $\\beta$ greater than 0.5. We show the results of the ground-truth sentence labels (Sec. 3.3) and our models on the  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note that ground-truth labels show the upper-bound performance since the reference summary to calculate ROUGE-recall is abstractive. All our ROUGE scores have a $95\\%$ confidence interval with at most $\\pm0.33$ . ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-d70cc97368a6b60e1a3179333970577d",
        "description": "The image is a table that presents ROUGE F-1 scores of various abstractive summarization methods on the CNN/Daily Mail test set. The table has four columns: Method, ROUGE-1, ROUGE-2, and ROUGE-L. Each row represents a different method and its corresponding ROUGE scores. The methods listed are HierAttn (Nallapati et al., 2016b), DeepRL (Paulus et al., 2017), pointer-generator (See et al., 2017), GAN (Liu et al., 2017), two-stage (ours), end2end w/o inconsistency loss (ours), end2end w/ inconsistency loss (ours), and lead-3 (See et al., 2017). The ROUGE-1 scores range from 32.75 to 40.68, with the highest score achieved by the end2end w/ inconsistency loss model. The ROUGE-2 scores range from 12.21 to 17.97, with the highest score also achieved by the end2end w/ inconsistency loss model. The ROUGE-L scores range from 29.01 to 37.13, with the highest score again achieved by the end2end w/ inconsistency loss model. The table highlights the performance of the two-stage and end2end models developed by the authors, which outperform other baseline models.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "We perform human evaluation on Amazon Mechanical Turk (MTurk)2 to evaluate the informativity, conciseness and readability of the summaries. We compare our best model (end2end with inconsistency loss) with pointer-generator (See et al., 2017), generative adversarial 5.3 Human Evaluation Figure 5: Visualizing the consistency between sentence and word attentions on the original article. We highlight word (bold font) and sentence (underline font) attentions. We compare our methods trained with and without inconsistency loss. Inconsistent fragments (see red bold font) occur when trained without the inconsistency loss.  Table 4: Inconsistency rate of our end-to-end trained model with and without inconsistency loss.  $t$ , if the word with maximum attention belongs to a sentence with low attention (i.e., $\\beta_{n(\\mathrm{argmax}(\\pmb\\alpha^{t}))}<\\mathrm{mean}(\\beta))$ , we define this step as an inconsistent step $t_{i n c}$ . The inconsistency rate $R_{i n c}$ is then defined as the percentage of the inconsistent steps in the summary. $$ R_{i n c}=\\frac{\\mathrm{Count}(t_{i n c})}{T}, $$ where $T$ is the length of the summary. The average inconsistency rates on test set are shown in Table 4. Our inconsistency loss significantly decrease $R_{i n c}$ from about $20\\%$ to $4\\%$ . An example of inconsistency improvement is shown in Fig. 5. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-33370358ad2e323a05afa749363aab02",
        "description": "The image is a table comparing different methods for summarization based on three criteria: informativity, conciseness, and readability. The rows represent different methods: DeepRL (Paulus et al., 2017), pointer-generator (See et al., 2017), GAN (Liu et al., 2017), Ours, and reference. The columns represent the evaluation criteria with their respective scores. For informativity, the scores are as follows: DeepRL has 3.23, pointer-generator has 3.18, GAN has 3.22, Ours has 3.58, and reference has 3.43. For conciseness, the scores are: DeepRL has 2.97, pointer-generator has 3.36, GAN has 3.52, Ours has 3.40, and reference has 3.61. For readability, the scores are: DeepRL has 2.85, pointer-generator has 3.47, GAN has 3.51, Ours has 3.70, and reference has 3.62. The 'Ours' method outperforms the others in informativity and readability, while the reference has the highest score in conciseness.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_8.jpg",
        "caption": [
            "Table 3: Comparing human evaluation results with state-of-the-art methods. "
        ],
        "footnote": [],
        "context": "We perform human evaluation on Amazon Mechanical Turk (MTurk)2 to evaluate the informativity, conciseness and readability of the summaries. We compare our best model (end2end with inconsistency loss) with pointer-generator (See et al., 2017), generative adversarial 5.3 Human Evaluation Figure 5: Visualizing the consistency between sentence and word attentions on the original article. We highlight word (bold font) and sentence (underline font) attentions. We compare our methods trained with and without inconsistency loss. Inconsistent fragments (see red bold font) occur when trained without the inconsistency loss.  Table 4: Inconsistency rate of our end-to-end trained model with and without inconsistency loss. $t$ , if the word with maximum attention belongs to a sentence with low attention (i.e., $\\beta_{n(\\mathrm{argmax}(\\pmb\\alpha^{t}))}<\\mathrm{mean}(\\beta))$ , we define this step as an inconsistent step $t_{i n c}$ . The inconsistency rate $R_{i n c}$ is then defined as the percentage of the inconsistent steps in the summary. $$ R_{i n c}=\\frac{\\mathrm{Count}(t_{i n c})}{T}, $$ where $T$ is the length of the summary. The average inconsistency rates on test set are shown in Table 4. Our inconsistency loss significantly decrease $R_{i n c}$ from about $20\\%$ to $4\\%$ . An example of inconsistency improvement is shown in Fig. 5.  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-33370358ad2e323a05afa749363aab02",
        "description": "The image is a table labeled 'Table 4: Inconsistency rate of our end-to-end trained model with and without inconsistency loss.' The table has two rows and two columns. The first column is labeled 'Method' and the second column is labeled 'avg. $R_{inc}$'. The first row under 'Method' is 'w/o incon. loss' with a corresponding value of 0.198 in the 'avg. $R_{inc}$' column. The second row under 'Method' is 'w/ incon. loss' with a corresponding value of 0.042 in the 'avg. $R_{inc}$' column. The table highlights the significant reduction in the average inconsistency rate ($R_{inc}$) when the model is trained with inconsistency loss compared to when it is trained without.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1013/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": "We perform human evaluation on Amazon Mechanical Turk (MTurk)2 to evaluate the informativity, conciseness and readability of the summaries. We compare our best model (end2end with inconsistency loss) with pointer-generator (See et al., 2017), generative adversarial network (Liu et al., 2017) and deep reinforcement model (Paulus et al., 2017). For 5.3 Human Evaluation Figure 5: Visualizing the consistency between sentence and word attentions on the original article. We highlight word (bold font) and sentence (underline font) attentions. We compare our methods trained with and without inconsistency loss. Inconsistent fragments (see red bold font) occur when trained without the inconsistency loss. attention (i.e., $\\beta_{n(\\mathrm{argmax}(\\pmb\\alpha^{t}))}<\\mathrm{mean}(\\beta))$ , we define this step as an inconsistent step $t_{i n c}$ . The inconsistency rate $R_{i n c}$ is then defined as the percentage of the inconsistent steps in the summary. $$ R_{i n c}=\\frac{\\mathrm{Count}(t_{i n c})}{T}, $$ where $T$ is the length of the summary. The average inconsistency rates on test set are shown in Table 4. Our inconsistency loss significantly decrease $R_{i n c}$ from about $20\\%$ to $4\\%$ . An example of inconsistency improvement is shown in Fig. 5.   Table 4: Inconsistency rate of our end-to-end trained model with and without inconsistency loss. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-33370358ad2e323a05afa749363aab02",
        "description": "The image is a text excerpt comparing two versions of a news article summary, one with inconsistency loss and one without. The text discusses a tornado event near Dallas, Texas, where a photo was taken by Ryan Shepard showing a large black cloud formation reaching the ground. Jamie Moore, head of emergency management in Johnson County, Texas, provides an estimate of the tornado's width. The National Weather Service warned about severe thunderstorms in the area, causing street flooding. The text highlights the differences in detail and accuracy between the summaries with and without inconsistency loss. The version with inconsistency loss contains highlighted words (in red bold font) indicating inconsistent fragments.",
        "segmentation": false
    }
}