## 1. MMGraphRAG Configuration and Usage

### 1.1 Overview of MMGraphRAG

The MMGraphRAG method converts multimodal inputs into a multimodal knowledge graph, retrieves relevant content (including multimodal entities), and enhances the generation task based on the retrieval results. It can be used independently in any multimodal question-answering scenario, requiring only an LLM and an MLLM. In theory, you could use only one MLLM, but the performance is not as good as using both models.

Examples of QA are shown in `./example_input`, and the `response.txt` file compares the results of MMGraphRAG (based on Kimi and QWEN-VL) with directly using GPT-4o. The `./example_output` directory contains the execution outputs, where `graph_merged_image_?.graphml` files are the multimodal knowledge graphs generated by MMGraphRAG. The last numbered file is the final complete multimodal knowledge graph.

* **context.csv**: Logs the retrieval results and model responses for each question.
* **images/**: Stores the images and their knowledge graphs. The numbering follows the image appearance order. For example, for image 1:

  * `images/image_1.jpg`: The image file.
  * `images/image_1/graph_image_1_entity_relation.graphml`: The original image knowledge graph.
  * `images/image_1/enhanced_graph_image_1_entity_relation.graphml`: The image knowledge graph after the second step of enhancement.
  * `images/image_1/new_graph_image_1_entity_relation.graphml`: The image knowledge graph after the third step of global entity alignment.

### 1.2 Configuration

* **Python version**: Python 3.10 is recommended; other versions may cause errors. All dependencies are recorded in `./cache/requirements.sh`.
* **Default YOLOv8 model**: `yolov8n-seg.pt` located in `./cache`.
* **Model parameters**: Modify parameters in `./mmgraphrag/parameter.py`, including answer format, max entity retrieval count, max tokens for entity and relation descriptions, max text chunk tokens, and max multimodal entities (default 3; not recommended to change).
* **Embedding model**: You can replace the embedding model; for quick runs, use `all-MiniLM-L6-v2` (lightweight). For best performance, use `stella-en-1.5B-v5` (ensure to download this model for experiment reproduction).
* **mineru\_dir**: Path for Mineru preprocessing results. To avoid reprocessing the same document, store preprocessing results here so that MMGraphRAG prioritizes retrieval from `mineru_dir` when handling new documents.
* **Model parameters**: The last section in `parameter.py` sets script call options. For testing, you can use API keys; for experiment reproduction, use local loading methods like vLLM.

### 1.3 Usage

After setup, test the project using `mmgraphrag/mmgraphrag_test.py`:

```bash
python mmgraphrag/mmgraphrag_test.py \
  --pdf_path <path/to/document.pdf> \
  --working_dir <output/directory> \
  --question "Your question here"
```

* **input\_mode**: Set to 0, 1, or 2:

  1. `2`: Process PDFs when Mineru is installed.
  2. `0`: Process DOCX files.
  3. `1`: Process well-structured PDFs (poor performance on complex PDFs).
* **query\_mode**: Set to `True` when answering questions.

---

## 2. CMEL Dataset

### 2.1 Dataset Structure

The CMEL dataset is stored in `./fusion_research/fusion_dataset`, organized by domain: `news`, `paper`, and `novel`.

* **Image storage**: As described in Section 1.1.
* **Log files**: Manually inspected logs during knowledge graph construction.
* **aligned\_image\_entity.json**: Ground truth for global entity alignment in images.
* **aligned\_text\_entity.json**: Ground truth for fine-grained image-text entity alignment.
* **kv\_store\_chunk\_knowledge\_graph.json**: Text knowledge graphs.
* **kv\_store\_image\_knowledge\_graph.json**: Image knowledge graphs.
* **kv\_store\_image\_data.json**: Image data, including file paths, context, chunk IDs, and image IDs.
* **kv\_store\_text\_chunks.json**: Original text chunks.

### 2.2 Using the Dataset

If image paths change, run:

```bash
python fusion_research/fusion_dataset/storage_path_change.py
```

Refer to Section 3 for experiment reproduction details.

### 2.3 Dataset Extension

Functions for dataset extension are in `./fusion_research/fusion_dataset_generation`. Follow the steps in the paper appendix. After extension, you can use tools in `./fusion_research/fusion_dataset/tools` to compute statistics such as entity counts or document page numbers.

---

## 3. Reproducing CMEL Experiments

1. Load models locally (e.g., with vLLM) and modify parameters in `./fusion_research/pull_llm.py`.
2. Download the corresponding embedding model and adjust settings in `./fusion_research/fusion_research.py`.
3. In `fusion_research.py`, set:

   * `method`: `embedding`, `llm`, or `clustering`.
   * `dataset_dir`: Path to the dataset.
   * `output_file_name`: e.g., `clustering_pagerank_knn_results.txt`.
4. For `clustering` methods, also specify:

   * `clustering_method`: `kmeans`, `dbscan`, `pagerank`, `leiden`, or `spectral`.
   * `classify_method`: `knn` or `llm`.

---

## 4. Reproducing MMDocQA Experiments

Two datasets are open-source on GitHub:

* [https://github.com/Anni-Zou/DocBench](https://github.com/Anni-Zou/DocBench)
* [https://github.com/EdinburghNLP/MMLongBench](https://github.com/EdinburghNLP/MMLongBench)

### 4.1 DocBench Reproduction

1. Preprocess the DocBench dataset:

   ```bash
   python eval/docbench_eval/mineru_docbench.py
   ```

2. To avoid issues, first run the checker:

   ```bash
   python eval/docbench_eval/check.py
   ```

3. Adjust parameters in `eval/docbench_eval/QA.py`:

   * `data_dir`: Dataset path.

   * `method_name`: `mmgraph`, `graph`, `llm`, `mmllm`, or `naive`.

   * `system`: LLM to use (`llama`, `qwen`, `mistral`, `internvl`, `qwenvl`, `ovis`).

   * `preprocess_dir`: Mineru preprocessing storage.

   * `mmgraphrag_path`: Add MMGraphRAG folder to `sys.path`.

   > **Note 1:** If `method_name` is `mmllm`, specify the model in `eval/eval_llm.py` (other than Ovis).
   > **Note 2:** If using the Ovis model, set `ovis_model_path`.

4. In `eval/eval_llm.py`, modify parameters for local loading of `llama-3.1-70b`.

5. In `eval/docbench_eval/evaluate.py`, set:

   * `data_dir`, `data_file_dir`, and `eval_system` (same as above).

6. Run `result.py` (ensure `res_dir` is set) to print final results.

### 4.2 MMLongBench Reproduction

1. Preprocess the MMLongBench dataset:

   ```bash
   python eval/mmlongbench_eval/mineru_mmlongbench.py
   ```
2. Set parameters for local loading of `llama-3.1-70b` in `eval/mmlongbench_eval/extract_answer.py`.
3. Modify parameters in `eval/mmlongbench_eval/run.py` (most are the same as in Section 4.1, with additional `mmlongbench_eval_dir`).