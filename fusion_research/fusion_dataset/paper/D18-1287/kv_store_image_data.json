{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "The key challenge we address is designing the goal representation. We avoid manually designing a meaning representation, and predict the goal in the Our model decomposes into goal prediction and action generation. Given a natural language instruction and system observations, the model predicts the goal to complete. Given the goal, the model generates a sequence of actions. goal once given the initial observation, and given this goal can then generate the actions required. In this paper, we study a new model that explicitly distinguishes between goal selection and action generation, and introduce two instruction following benchmark tasks to evaluate it.  the next action using a single learned function that is executed repeatedly until task completion. Although executing the same computation at each step simplifies modeling, it exemplifies certain inefficiencies; while the agent needs to decide what action to take at each step, identifying its goal is only required once every several steps or even once per execution. The left instruction in Figure 1 illustrates this. The agent can compute its Figure 1: Example instructions from our two tasks: LANI (left) and CHAI (right). LANI is a landmark navigation task, and CHAI is a corpus of instructions in the CHALET environment. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-87d2e06a31aa878bb79077cd36b9e783",
        "description": "The image consists of two distinct scenes. On the left, there is an outdoor scene featuring a grassy area with a blue fence in the background. In front of the fence, there is a well with a wooden bucket attached to it. To the right of the well, there is a cactus plant. The sky is dark, suggesting it might be nighttime or a stormy day. On the right, there is an indoor kitchen scene with a tiled wall and a countertop. On the countertop, there is a sink with a faucet, a sponge, a dishwashing soap bottle, and a plate with some food on it. Above the sink, there are three cabinets with open doors, and inside one of the cabinets, there is a bottle of wine. The instructions below each scene provide guidance for navigating or performing tasks related to the images.",
        "segmentation": true
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_2.jpg",
        "caption": [
            "Figure 2: An illustration for our architecture (Section 4) for the instruction turn left and go to the red oil drum with a LINGUNET depth of $m\\,=\\,4$ . The instruction $\\textstyle{\\bar{x}}$ is mapped to $\\bar{\\bf x}$ with an RNN, and the initial panorama observation ${\\bf I}_{P}$ to ${\\bf F}_{0}$ with a CNN. LINGUNET generates $\\mathbf{H}_{1}$ , a visual representation of the goal. First, a sequence of convolutions maps the image features ${\\bf F}_{0}$ to feature maps $\\mathbf{F}_{1},\\ldots,\\bar{\\mathbf{F}}_{4}$ . The text representation $\\bar{\\bf x}$ is used to generate the kernels $\\mathbf{K}_{1},\\ldots,\\mathbf{K}_{4}^{}$ , which are convolved to generate the text-conditioned feature maps $\\mathbf{G}_{1},\\ldots,\\mathbf{G}_{4}$ .These feature maps are de-convolved to $\\mathbf{H}_{1},\\dots,\\mathbf{H}_{4}$ . The goal probability distribution $P_{g}$ is computed from $\\mathbf{H}_{1}$ .The goal location is the inferred from the max of $P_{g}$ . Given $l_{g}$ and $p_{t}$ , the pose at step $t$ , the goal mask ${{\\bf{M}}_{t}}$ is computed and passed into an RNN that outputs the action to execute. "
        ],
        "footnote": [],
        "context": "Goal Prediction To predict the goal location, we generate a probability distribution $P_{g}$ over a feature map ${\\bf F}_{0}$ generated using convolutions from the initial panorama observation ${\\bf I}_{P}$ . Each element in the probability distribution $P_{g}$ corresponds to an area in ${\\bf I}_{P}$ . Given the instruction $\\bar{x}$ and panorama ${\\bf I}_{P}$ , we first generate their representations. From the panorama ${\\bf I}_{P}$ , we generate a feature map ${\\bf F}_{0}\\,=\\,[{\\bf C N N}_{0}({\\bf I}_{P});{\\bf F}^{p}]$ , where $\\mathrm{{CNN}_{0}}$ is a two-layer convolutional neural network (CNN; LeCun et al., 1998) with rectified linear units (ReLU; Nair and Hinton, 2010) observe the environment at the beginning of the execution.1 The model includes two main components: goal prediction and action generation. The agent uses the panorama ${\\bf\\cal I}_{P}$ to predict the goal location $l_{g}$ . At each time step $t$ , a projection of the goal location into the agent’s current view ${{\\bf{M}}_{t}}$ is given as input to an RNN to generate actions. The probability of an action $a_{t}$ at time $t$ decomposes to: $$ \\begin{array}{r}{P(a_{t}\\mid\\tilde{s}_{t})=\\displaystyle\\sum_{l_{g}}\\left(P(l_{g}\\mid\\bar{x},\\mathbf{I}_{P})\\right.\\qquad\\qquad\\qquad}\\\\ {\\left.P(a_{t}\\mid l_{g},(\\mathbf{I}_{1},p_{1}),\\dots,(\\mathbf{I}_{t},p_{t}))\\right)\\;\\,,}\\end{array} $$ where the first term puts the complete distribution mass on a single location (i.e., a delta function). Figure 2 illustrates the model. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-678394879a21253bf623b2593fe21b70",
        "description": "The image is a detailed diagram illustrating the architecture for an instruction-following system, specifically designed to interpret and execute the command 'Turn left and go to the red oil drum.' The diagram is divided into several interconnected components. On the top left, there is a panorama image labeled as ${\\\\bf I}_{P}$, which represents the initial observation of the environment. This image is processed by a CNN (Convolutional Neural Network) to generate a feature map ${\\\\bf F}_{0}$. Below this, there are multiple layers of convolutions that further process the feature map, resulting in a sequence of feature maps ${\\\\bf F}_{1}, {\\\\bf F}_{2}, {\\\\bf F}_{3}, {\\\\bf F}_{4}$. These feature maps are then combined with text kernels ${\\\\bf K}_{1}, {\\\\bf K}_{2}, {\\\\bf K}_{3}, {\\\\bf K}_{4}$ generated from the instruction representation $\\\\bar{\\\\bf x}$, which is derived from the original instruction $\\\\bar{x}$ using an RNN (Recurrent Neural Network). The text-conditioned feature maps ${\\\\bf G}_{1}, {\\\\bf G}_{2}, {\\\\bf G}_{3}, {\\\\bf G}_{4}$ are de-convolved to produce ${\\\\bf H}_{1}, {\\\\bf H}_{2}, {\\\\bf H}_{3}, {\\\\bf H}_{4}$. The goal probability distribution $P_{g}$ is computed from ${\\\\bf H}_{1}$, and the goal location $l_{g}$ is inferred from the maximum value in $P_{g}$. Given $l_{g}$ and the current pose $p_{t}$ at step $t$, the goal mask ${\\\\bf M}_{t}$ is computed and passed into an RNN to output the action to execute. The actions shown are 'TURNLEFT', 'TURNLEFT', and 'FORWARD'.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_3.jpg",
        "caption": [],
        "footnote": [],
        "context": "The environment is a fenced, square, grass field. Each instance of the environment contains between 6–13 randomly placed landmarks, sampled from 63 unique landmarks. The agent can take four types of discrete actions: FORWARD, TURNRIGHT, TURNLEFT, and STOP. The field is of size $50\\!\\times\\!50$ The goal of LANI is to evaluate how well an agent can follow navigation instructions. The agent task is to follow a sequence of instructions that specify a path in an environment with multiple landmarks. Figure 1 (left) shows an example instruction. 6.1 LANI 6 Tasks and Data Table 1: Summary statistics of the two corpora. The problem reward provides a positive reward for successful task completion, and a negative reward for incorrect completion or collision. The shaping term is positive when the agent gets closer to the goal position, and negative if it is moving away. The gradient of the objective is: $$ \\begin{array}{r c l}{\\nabla J}&{=}&{\\displaystyle\\sum_{a\\in\\cal A}\\pi(a\\mid\\tilde{s}_{t})\\nabla\\log\\pi(a\\mid\\tilde{s}_{t})R(s_{t},a)}\\\\ &&{\\quad\\quad+\\lambda\\nabla H(\\pi(.\\mid\\tilde{s}_{t})\\,\\,\\,.}\\end{array} $$ We approximate the gradient by sampling an action using the policy (Williams, 1992), and use the gold goal location computed from $s_{g}^{(i)}$ We perform several parallel rollouts to compute gradients and update the parameters using Hogwild! (Recht et al., 2011) and Adam learning rates. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-a5cbc3dc3119bb20857ce5133d72aac4",
        "description": "The image is a table labeled 'Dataset Statistic' that provides statistical information about two datasets: LANI and CHAI. The table is structured with the following columns: Number paragraphs, Mean instructions per paragraph, Mean actions per instruction, Mean tokens per instruction, and Vocabulary size. For the LANI dataset, the values are as follows: Number paragraphs (6,000), Mean instructions per paragraph (4.7), Mean actions per instruction (24.6), Mean tokens per instruction (12.1), and Vocabulary size (2,292). For the CHAI dataset, the values are: Number paragraphs (1,596), Mean instructions per paragraph (7.70), Mean actions per instruction (54.5), Mean tokens per instruction (8.4), and Vocabulary size (1,018). The table highlights the differences in the complexity and size of the two datasets.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_4.jpg",
        "caption": [],
        "footnote": [],
        "context": "Tablepa ss1i ngs thheo bewncsh otnh thee ricgoht rsipdeu, ssto psptinagt riigshtti bcefso.re4 y oEu gaetc toh t hpearagraph corresponds to a single unique instance of the environment. The paragraphs are split into train, test, and development, with a $70\\%\\ /\\ 15\\%\\ /$ $15\\%$ split. Finally, we sample 200 single development instructions for qualitative analysis of the language challenge the corpus serves the environment from a first person view. Figure 3 shows a reference path and the written instruction. This data can be used for evaluating both executing sequences of instructions and single insttrheun chetaido tonwsar dis nth ei bsaorrlela.tion.   second worker creates the final data of segmented instructions and demonstrations. The generated reference path is displayed in both tasks. The second worker could also mark the paragraph as invalid. Both tasks are done from an overhead view of the environment, but workers are instructed to provide instructions for a robot that obFigure 3: Segmented instructions in the LANI domain. The original reference path is marked in red (start) and blue (end). The agent, using a drone icon, is placed at the beginning of the path. The follower path is coded in colors to align to the segmented instruction paragraph. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-a5cbc3dc3119bb20857ce5133d72aac4",
        "description": "The image depicts a top-down view of an outdoor environment enclosed by a red and blue boundary. Inside the enclosure, there is a grassy area with various objects scattered around. Notable objects include a red telephone booth, a small wooden structure resembling a shed, a bench, and a tree. There are also several smaller items such as a suitcase, a backpack, and what appears to be a camera. The ground is marked with a path indicated by colored dots (yellow, purple, and cyan) that guide the movement from one point to another. The path starts at a red dot and ends at a blue dot. A drone icon is placed at the beginning of the path, indicating the starting position. The overall scene suggests a setup for navigation or path-following tasks, possibly for robotics or autonomous systems testing.",
        "segmentation": true
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Tablepa ss1i ngs thheo bewncsh otnh thee ricgoht rsipdeu, ssto psptinagt riigshtti bcefso.re4 y oEu gaetc toh t hpearagraph corresponds to a single unique instance of the environment. The paragraphs are split into train, test, and development, with a $70\\%\\ /\\ 15\\%\\ /$ $15\\%$ split. Finally, we sample 200 single development instructions for qualitative analysis of the language challenge the corpus serves the environment from a first person view. Figure 3 shows a reference path and the written instruction. This data can be used for evaluating both executing sequences of instructions and single insttrheun chetaido tonwsar dis nth ei bsaorrlela.tion.  second worker creates the final data of segmented instructions and demonstrations. The generated reference path is displayed in both tasks. The second worker could also mark the paragraph as invalid. Both tasks are done from an overhead view of the environment, but workers are instructed to provide instructions for a robot that obFigure 3: Segmented instructions in the LANI domain. The original reference path is marked in red (start) and blue (end). The agent, using a drone icon, is placed at the beginning of the path. The follower path is coded in colors to align to the segmented instruction paragraph.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-a5cbc3dc3119bb20857ce5133d72aac4",
        "description": "The image is a set of instructions for navigating an environment. The text is presented in a fragmented and color-coded manner, with each segment highlighted in different colors: red, blue, green, yellow, and purple. The instructions guide the reader to navigate around various objects such as a pillar, a boat, a tree, a cone, a hydrant, and a tree stump. The sequence of actions includes circling around objects clockwise or counterclockwise, passing on specific sides of these objects, and stopping at designated points. For example, one instruction reads: 'Go around the pillar on the right hand side and head towards the boat, circling around it clockwise.' Another instruction states: 'When you are facing the tree, walk towards it, and pass on the right hand side, and the left hand side of the cone. Circle around the cone and then walk past the hydrant on your right, and the tree stump. Circle around the stump and then stop right behind it.' The overall layout suggests a step-by-step guide for a navigation task, possibly for a robot or a drone.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_6.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Qualitative analysis of the LANI and CHAI corpora. We sample 200 single development instructions from each corpora. For each category, we count how many examples of the 200 contained it and show an example. "
        ],
        "context": "We collected a corpus of navigation and manipulation instructions using Amazon Mechanical Turk. We created 36 common household scenarios to provide a familiar context to the task.5 We use two crowdsourcing tasks. First, we provide workers with a scenario and ask them to write instructions. The workers are encouraged to explore the environment and interact with it. We then segment the instructions to sentences automatically. In the second task, workers are presented with the segmented sentences in order and asked to execute them. After finishing a sentence, the workers request the next sentence. The workers do not see the original   manipulation accuracy, which compares the set of manipulation actions to a reference set. When measuring distance, to consider the house plan, we compute the minimal aerial distance for each room that must be visited. Yan et al. (2018) provides the full details of the simulator and evaluation. We use five different houses, each with up to six rooms. Each room contains on average 30 objects. A typical room is of size $6\\!\\times\\!6$ . We set the distance of FORWARD to 0.1, the turn angle to $90^{\\circ}$ , and divide the agent’s view to a $32\\!\\times\\!32$ grid for the INTERACT action. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-ecb5de346dd7867cf3eb231fcde80125",
        "description": "The image is a table labeled 'Table 2: Qualitative analysis of the LANI and CHAI corpora.' It provides a detailed comparison between two corpora, LANI and CHAI, based on various categories. The table is structured with the following columns: Category, Count (with sub-columns for LANI and CHAI), and Example. The rows represent different categories analyzed in the corpora. The categories include Spatial relations between locations, Conjunctions of two or more locations, Temporal coordination of sub-goals, Constraints on the shape of trajectory, Co-reference, and Comparatives. For each category, the table shows the count of examples found in each corpus out of 200 sampled instructions and provides an example sentence from each corpus. For instance, under Spatial relations between locations, LANI has 123 examples, while CHAI has 52. An example from LANI is 'go to the right side of the rock,' and from CHAI is 'pick up the cup next to the bathtub and place it on ...'. The table highlights the differences in the frequency and types of instructions between the two corpora.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_7.jpg",
        "caption": [],
        "footnote": [
            "Figure 4: Scenario and segmented instruction from the CHAI corpus. "
        ],
        "context": "We collected a corpus of navigation and manipulation instructions using Amazon Mechanical Turk. We created 36 common household scenarios to provide a familiar context to the task.5 We use two crowdsourcing tasks. First, we provide workers with a scenario and ask them to write instructions. The workers are encouraged to explore the environment and interact with it. We then segment the instructions to sentences automatically. In the second task, workers are presented with the segmented sentences in order and asked to execute them. After finishing a sentence, the workers request the next sentence. The workers do not see the original  manipulation accuracy, which compares the set of manipulation actions to a reference set. When measuring distance, to consider the house plan, we compute the minimal aerial distance for each room that must be visited. Yan et al. (2018) provides the full details of the simulator and evaluation. We use five different houses, each with up to six rooms. Each room contains on average 30 objects. A typical room is of size $6\\!\\times\\!6$ . We set the distance of FORWARD to 0.1, the turn angle to $90^{\\circ}$ , and divide the agent’s view to a $32\\!\\times\\!32$ grid for the INTERACT action.  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-ecb5de346dd7867cf3eb231fcde80125",
        "description": "The image is a table labeled 'Figure 4: Scenario and segmented instruction from the CHAI corpus.' The table is divided into two main sections: 'Scenario' and 'Written Instructions.' The 'Scenario' section describes a situation where an individual has several hours before guests arrive for a dinner party. The tasks include preparing meat dishes, putting them in the sink, removing items from the kitchen and bathroom that are not suitable for guests to see (like soaps and dish cleaning items), and placing these items in cupboards. Additionally, it involves putting dirty dishes around the house in the dishwasher and closing it. The 'Written Instructions' section provides step-by-step instructions for executing the scenario. These instructions include opening the cupboard above the sink, putting specific items like cereal, sponge, and dishwashing soap into the cupboard, closing the cupboard, picking up meats and putting them into the sink, opening the dishwasher, grabbing dirty dishes on the counter, and putting the dishes into the dishwasher.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_8.jpg",
        "caption": [
            "Table 3: Performance on the development data. "
        ],
        "footnote": [],
        "context": "Evaluation Metrics We evaluate Baselines We compare our approach against the following baselines: (a) STOP: Agent stops immediately; (b) RANDOMWALK: Agent samples actions uniformly until it exhausts the horizon or stops; (c) MOSTFREQUENT: Agent takes the most frequent action in the data, FORWARD for both datasets, until it exhausts the horizon; (d) MISRA17: the approach of Misra et al. (2017); and (e) CHAPLOT18: the approach of Chaplot et al. (2018). We also evaluate goal prediction and compare to the method of Janner et al. (2018) and a CENTER baseline, which always predict the center pixel. Appendix C provides baseline details.  to pick it up via interaction, move to the kitchen door, and finally move within the kitchen. The process of executing an instruction starts with predicting the sequence of goal types. We call our model (Section 4) separately for each goal type. The execution concludes when the final goal is completed. For learning, we create a separate example for each intermediate goal and train the additional RNN separately. The second modification is replacing the backward camera projection for inferring the goal location with ray casting to identify INTERACTION goals, which are often objects that are not located on the ground. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-ecb5de346dd7867cf3eb231fcde80125",
        "description": "The image is a table labeled 'Table 3: Performance on the development data.' The table compares the performance of different methods on two datasets, LANI and CHAI. Each dataset has four metrics: SD (Success Distance), TC (Time to Completion), SD (Success Distance), and MA (Mean Accuracy). The methods evaluated include STOP, RANDOMWALK, MOSTFREQUENT, MISRA17, CHAPLOT18, and Our Approach (OA). For the LANI dataset, the values are as follows: STOP has SD of 15.37 and TC of 8.20; RANDOMWALK has SD of 14.80 and TC of 9.66; MOSTFREQUENT has SD of 19.31 and TC of 2.94; MISRA17 has SD of 10.54 and TC of 22.9; CHAPLOT18 has SD of 9.05 and TC of 31.0; and Our Approach (OA) has SD of 8.65 and TC of 35.72. For the CHAI dataset, the values are: STOP has SD of 2.99 and MA of 37.53; RANDOMWALK has SD of 2.99 and MA of 28.96; MOSTFREQUENT has SD of 3.80 and MA of 37.53; MISRA17 has SD of 2.99 and MA of 32.25; CHAPLOT18 has SD of 2.99 and MA of 37.53; and Our Approach (OA) has SD of 2.75 and MA of 37.53. Additional rows show variations of Our Approach with different modifications, such as OA w/o RNN, OA w/o Language, OA w/joint, and OA w/oracle goals. The table highlights the performance differences among the methods, with Our Approach generally showing better results.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_9.jpg",
        "caption": [],
        "footnote": [
            "Table 5: Development goal prediction performance. We measure distance (Dist) and accuracy (Acc). "
        ],
        "context": "We also measure human performance on a sample of 100 development examples for both tasks. On LANI, we observe a stop distance error (SD) of 5.2 and To isolate navigation performance on CHAI, we limit our train and test data to instructions that include navigation actions only. The STOP baseline on these instructions gives a stop distance (SD) of 3.91, higher than the average for the entire data as these instructions require more movement. Our approach gives a stop distance (SD) of 3.24, a $17\\%$ reduction of error, significantly better than the $8\\%$ reduction of error over the entire corpus.  intermediate goal separately. All other parameters and detailed in Appendix D. 8Results Tables 3 and 4 show development and test results. Both sets of experiments demonstrate similar trends. The low performance of STOP, RANDOMWALK, and MOSTFREQUENT demonstrates the challenges of both tasks, and shows the tasks are robust to simple biases. On LANI, our approach outperforms CHAPLOT18, improving task completion (TC) accuracy by $5\\%$ , and both methods outperform MISRA17. On CHAI, CHAPLOT18 and MISRA17 both fail to learn, while our approach shows an improvement on stop distance (SD). However, all models perform poorly on CHAI, especially on manipulation (MA). ",
        "chunk_order_index": 7,
        "chunk_id": "chunk-6c1df1d042170364405c60ca34c8951c",
        "description": "The image is a table labeled 'Table 4: Performance on the held-out test dataset.' The table is divided into two main sections, LANI and CHAI, each with its own set of metrics. For LANI, the metrics are SD (Stop Distance) and TC (Task Completion), while for CHAI, the metrics are SD (Stop Distance) and MA (Manipulation Accuracy). The rows represent different methods used in the experiments. The methods listed are STOP, RANDOMWALK, MOSTFREQUENT, Misra17, Chaplot18, and Our Approach. The values for each method under the LANI section are as follows: STOP has SD of 15.18 and TC of 8.29; RANDOMWALK has SD of 14.63 and TC of 9.76; MOSTFREQUENT has SD of 19.14 and TC of 3.15; Misra17 has SD of 10.23 and TC of 23.2; Chaplot18 has SD of 8.78 and TC of 31.9; Our Approach has SD of 8.43 and TC of 36.9. Under the CHAI section, the values are: STOP has SD of 3.59 and MA of 39.77; RANDOMWALK has SD of 3.59 and MA of 33.29; MOSTFREQUENT has SD of 4.36 and MA of 39.77; Misra17 has SD of 3.59 and MA of 36.84; Chaplot18 has SD of 3.59 and MA of 39.76; Our Approach has SD of 3.34 and MA of 39.97. The table highlights the performance of different methods on two tasks, LANI and CHAI, with specific focus on stop distance and task completion for LANI, and stop distance and manipulation accuracy for CHAI.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_10.jpg",
        "caption": [],
        "footnote": [],
        "context": "Our ablations (Table 3) demonstrate the importance of each of the components of the model. We ablate the action generation RNN (w/o RNN), completely remove the language input (w/o Language), and train the model jointly (w/joint Learning).8 On CHAI especially, ablations results in models that display ineffective behavior. Of the ablations, we observe the largest benefit judgements on our approach, we correlate the human metric with the SD measure. We observe a Pearson correlation -0.65 $\\scriptstyle({\\mathrm{p}}=5{\\mathrm{e}}-7)$ ), indicating that our automated metric correlates well with human judgment.7 This initial study suggests that our automated evaluation is appropriate for this task.   about automated evaluation. In general, we observe that often measuring execution quality with rigid goals is insufficient. We conduct a human evaluation with 50 development examples from LANI rating human performance and our approach. Figure 5 shows a histogram of the ratings. The mean rating for human followers is 4.38, while our approach’s is 3.78; we observe a similar trend to before with this metric. Using Table 6: Mean goal prediction error for LANI instructions with and without the analysis categories we used in Table 2. The $p$ -values are from two-sided $t$ -tests comparing the means in each row. ",
        "chunk_order_index": 7,
        "chunk_id": "chunk-6c1df1d042170364405c60ca34c8951c",
        "description": "The image is a table labeled 'Table 6: Mean goal prediction error for LANI instructions with and without the analysis categories we used in Table 2.' The table is structured with three main columns: Category, Present, Absent, and p-value. Each row represents a different category of analysis and contains the following values: 'Spatial relations' (Present: 8.75, Absent: 10.09, p-value: .262), 'Location conjunction' (Present: 10.19, Absent: 9.05, p-value: .327), 'Temporal coordination' (Present: 11.38, Absent: 8.24, p-value: .015), 'Trajectory constraints' (Present: 9.56, Absent: 8.99, p-value: .607), 'Co-reference' (Present: 12.88, Absent: 8.59, p-value: .016), and 'Comparatives' (Present: 10.22, Absent: 9.25, p-value: .906). The table highlights the mean goal prediction error for each category when the analysis is present versus absent, along with the statistical significance of the difference as indicated by the p-values.",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_11.jpg",
        "caption": [
            "Figure 5: Likert rating histogram for expert human follower and our approach for LANI. "
        ],
        "footnote": [],
        "context": "Our ablations (Table 3) demonstrate the importance of each of the components of the model. We ablate the action generation RNN (w/o RNN), completely remove the language input (w/o Language), and train the model jointly (w/joint Learning).8 On CHAI especially, ablations results in models that display ineffective behavior. Of the ablations, we observe the largest benefit judgements on our approach, we correlate the human metric with the SD measure. We observe a Pearson correlation -0.65 $\\scriptstyle({\\mathrm{p}}=5{\\mathrm{e}}-7)$ ), indicating that our automated metric correlates well with human judgment.7 This initial study suggests that our automated evaluation is appropriate for this task. about automated evaluation. In general, we observe that often measuring execution quality with rigid goals is insufficient. We conduct a human evaluation with 50 development examples from LANI rating human performance and our approach. Figure 5 shows a histogram of the ratings. The mean rating for human followers is 4.38, while our approach’s is 3.78; we observe a similar trend to before with this metric. Using Table 6: Mean goal prediction error for LANI instructions with and without the analysis categories we used in Table 2. The $p$ -values are from two-sided $t$ -tests comparing the means in each row.   ",
        "chunk_order_index": 7,
        "chunk_id": "chunk-6c1df1d042170364405c60ca34c8951c",
        "description": "The image is a bar chart labeled 'Figure 5: Likert rating histogram for expert human follower and our approach for LANI.' The x-axis represents the rating categories, ranging from 1 to 5. The y-axis indicates the percentage of ratings, with values ranging from 0 to 60%. There are two sets of bars for each rating category: blue bars representing 'Human' and red bars representing 'Our Approach.' The exact percentages for each category are as follows: For rating 1, the Human has approximately 0% and Our Approach has around 10%; for rating 2, Human has around 5% and Our Approach has about 15%; for rating 3, both have around 10%; for rating 4, Human has around 25% and Our Approach has about 15%; and for rating 5, Human has around 50% and Our Approach has about 45%. The chart shows that the Human ratings are generally higher than those of Our Approach, especially in the higher rating categories.",
        "segmentation": false
    },
    "image_12": {
        "image_id": 12,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1287/images/image_12.jpg",
        "caption": [
            "Figure 6: Goal prediction probability maps $P_{g}$ overlaid on the corresponding observed panoramas ${\\bf I}_{P}$ . The top example shows a result on LANI, the bottom on CHAI. gories we used in Table 2 using the same sample of the data. Appendix E includes a similar table for CHAI. We observe that our approach finds instructions with temporal coordination or co-reference challenging. Co-reference is an expected limitation; with single instructions, the model can not resolve references to previous instructions. ",
            "walk over to the cabinets and open the cabinet doors up "
        ],
        "footnote": [],
        "context": "We propose a model for instruction following with explicit separation of goal prediction and action generation. Our representation of goal prediction is easily interpretable, while not requiring the design of logical ontologies and symbolic representations. A potential limitation of our approach is cascading errors. Action generation relies completely on the predicted goal and is not exposed to the language otherwise. This also suggests a second related limitation: the model is unlikely to successfully reason about instructions that include constraints on the execution itself. While the model may reach the final goal correctly, it is unlikely to account for 9 Discussion goals (Table 3). We observe this improves navigation performance significantly on both tasks. However, the model completely fails to learn a reasonable manipulation behavior for CHAI. This illustrates the planning complexity of this domain. A large part of the improvement in measured navigation behavior is likely due to eliminating much of the ambiguity the automated metric often fails to capture. Finally, on goal prediction (Table 5), our approach outperforms the method of Janner et al. (2018). Figure 6 and Appendix Figure 7 show example goal predictions. In Table 6, we break down LANI goal prediction results for the analysis cate ",
        "chunk_order_index": 7,
        "chunk_id": "chunk-6c1df1d042170364405c60ca34c8951c",
        "description": "The image consists of two distinct sections. The top section shows a panoramic view with a probability map overlaid on it, representing the goal prediction probability maps \\( P_g \\) for LANI. The probability map is characterized by a gradient of colors ranging from blue to yellow, indicating different probabilities. The background appears to be an outdoor scene with grass and a fence. The bottom section displays another panoramic view, this time for CHAI, with a similar probability map overlay. This section seems to depict an indoor environment, possibly a kitchen or a room with cabinets and a dining area. The text 'curve around big rock keeping it to your left' is written in the middle of the image, suggesting navigation instructions. The overall image conveys the concept of goal prediction in the context of instruction following tasks.",
        "segmentation": true
    }
}