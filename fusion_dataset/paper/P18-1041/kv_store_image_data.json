{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Hierarchical Pooling Both SWEM-aver and SWEM-max do not take word-order or spatial information into consideration, which could be useful for certain NLP applications. So motivated, we further propose a hierarchical pooling layer. Let $v_{i:i+n-1}$ refer to the local window consisting of $n$ consecutive words words, $v_{i},v_{i+1},...,v_{i+n-1}$ .First, an average-pooling is performed on each local window, $v_{i:i+n-1}$ . The extracted features from all windows are further down-sampled with a global max-pooling operation on top of the representations for every window. We call this approach SWEM-hier due to learned. As a result, the models only exploit intrinsic word embedding information for predictions.  (as the components of the embedding vectors will have small amplitude), unlike SWEM-aver where every word contributes equally to the representation. Considering that SWEM-aver and SWEM-max are complementary, in the sense of accounting for different types of information from text sequences, we also propose a third SWEM variant, where the two abstracted features are concatenated together to form the sentence embeddings, denoted here as SWEM-concat. For all SWEM variants, there are no additional compositional parameters to be Table 1: Comparisons of CNN, LSTM and SWEM architectures. Columns correspond to the number of compositional parameters, computational complexity and sequential operations, respectively. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-b645d9c5da818cb28134c78c33a7c37d",
        "description": "The image is a table labeled 'Table 1: Comparisons of CNN, LSTM and SWEM architectures.' The table is structured with four main columns: Model, Parameters, Complexity, and Sequential Ops. Each row represents a different model architecture and contains the following information: \\n- **CNN**: Parameters are given as \\( n \\cdot K \\cdot d \\), Complexity is \\( \\mathcal{O}(n \\cdot L \\cdot K \\cdot d) \\), and Sequential Ops are \\( \\mathcal{O}(1) \\). \\n- **LSTM**: Parameters are given as \\( 4 \\cdot d \\cdot (K + d) \\), Complexity is \\( \\mathcal{O}(L \\cdot d^2 + L \\cdot K \\cdot d) \\), and Sequential Ops are \\( \\mathcal{O}(L) \\). \\n- **SWEM**: Parameters are given as 0, Complexity is \\( \\mathcal{O}(L \\cdot K) \\), and Sequential Ops are \\( \\mathcal{O}(1) \\). \\nThe table highlights the differences in parameters, computational complexity, and sequential operations between the three models. The context discusses the hierarchical pooling layer proposed for SWEM, which aims to improve upon SWEM-aver and SWEM-max by incorporating word-order and spatial information.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_2.jpg",
        "caption": [],
        "footnote": [],
        "context": "We begin with the task of categorizing documents (with approximately 100 words in average per document). We follow the data split in Zhang et al. (2015b) for comparability. These datasets can be generally categorized into three types: topic categorization (represented by Yahoo! Answer and AG news), sentiment analysis (represented by Yelp Polarity and Yelp Full) and ontology classification (represented by DBpedia). Results are shown in Table 2. Surprisingly, on topic prediction tasks, our SWEM model exhibits stronger performances, relative to both LSTM and CNN compositional architectures, this by leveraging both the average and max-pooling features from word 4.1 Document Categorization   classifier is implemented as an MLP layer with dimension selected from the set [100, 300, 500, 1000], followed by a sigmoid or softmax function, depending on the specific task. Adam (Kingma and Ba, 2014) is used to optimize all models, with learning rate selected from the set $[1\\times10^{-3},3\\times10^{-4},\\bar{2}\\times10^{-4},1\\times10^{-5}]$ (with cross-validation used to select the appropriate parameter for a given dataset and task). Dropout regularization (Srivastava et al., 2014) is employed on the word embedding layer and final MLP layer, with dropout rate selected from the set [0.2, 0.5, 0.7]. The batch size is selected from [2, 8, 32, 128, 512]. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778",
        "description": "The image is a table that presents the performance of various models on different datasets for document categorization tasks. The table has six columns: Model, Yahoo! Ans., AG News, Yelp P., Yelp F., and DBpedia. Each row represents a different model and its corresponding performance metrics on the five datasets. The models listed are Bag-of-means*, Small word CNN*, Large word CNN*, LSTM*, Deep CNN (29 layer)†, fastText ‡, fastText (bigram)‡, SWEM-aver, SWEM-max, SWEM-concat, and SWEM-hier. The performance metrics are numerical values representing accuracy or some other evaluation metric. For example, the Bag-of-means* model scores 60.55 on Yahoo! Ans., 83.09 on AG News, 87.33 on Yelp P., 53.54 on Yelp F., and 90.45 on DBpedia. The SWEM-concat model achieves the highest scores in most categories, with 73.53 on Yahoo! Ans., 92.66 on AG News, 93.76 on Yelp P., 61.11 on Yelp F., and 98.57 on DBpedia. The table highlights the superior performance of the SWEM models compared to other architectures.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_3.jpg",
        "caption": [
            "Table 2: Test accuracy on (long) document classification tasks, in percentage. Results marked with $^*$ are reported in Zhang et al. (2015b), with $\\dagger$ are reported in Conneau et al. (2016), and with $^{\\ddag}$ are reported in Joulin et al. (2016). ",
            "Table 3: Top five words with the largest values in a given word-embedding dimension (each column corresponds to a dimension). The first row shows the (manually assigned) topic for words in each column. "
        ],
        "footnote": [],
        "context": "We begin with the task of categorizing documents (with approximately 100 words in average per document). We follow the data split in Zhang et al. (2015b) for comparability. These datasets can be generally categorized into three types: topic categorization (represented by Yahoo! Answer and AG news), sentiment analysis (represented by Yelp Polarity and Yelp Full) and ontology classification (represented by DBpedia). Results are shown in Table 2. Surprisingly, on topic prediction tasks, our SWEM model exhibits stronger performances, relative to both LSTM and CNN compositional architectures, this by leveraging both the average and max-pooling features from word 4.1 Document Categorization  classifier is implemented as an MLP layer with dimension selected from the set [100, 300, 500, 1000], followed by a sigmoid or softmax function, depending on the specific task. Adam (Kingma and Ba, 2014) is used to optimize all models, with learning rate selected from the set $[1\\times10^{-3},3\\times10^{-4},\\bar{2}\\times10^{-4},1\\times10^{-5}]$ (with cross-validation used to select the appropriate parameter for a given dataset and task). Dropout regularization (Srivastava et al., 2014) is employed on the word embedding layer and final MLP layer, with dropout rate selected from the set [0.2, 0.5, 0.7]. The batch size is selected from [2, 8, 32, 128, 512].  ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778",
        "description": "The image is a table labeled 'Table 3: Top five words with the largest values in a given word-embedding dimension (each column corresponds to a dimension). The first row shows the (manually assigned) topic for words in each column.' The table is structured with eight columns, each representing a different topic: Politics, Science, Computer, Sports, Chemistry, Finance, and Geoscience. Each column contains five rows of words that are associated with the respective topic. For example, under the 'Politics' column, the words listed are 'philipdru', 'justices', 'impeached', 'impeachment', and 'neocons'. Under the 'Science' column, the words are 'coulomb', 'differentiable', 'paranormal', 'converge', and 'antimatter'. The table provides a detailed list of words that are highly associated with each topic based on their word-embedding dimensions.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_4.jpg",
        "caption": [],
        "footnote": [],
        "context": "Interestingly, for the sentiment analysis tasks, both CNN and LSTM compositional functions perform better than SWEM, suggesting that wordorder information may be required for analyzing sentiment orientations. This finding is consistent with Pang et al. (2002), where they hypothesize that the positional information of a word in text sequences may be beneficial to predict sentiment. This is intuitively reasonable since, for instance, the phrase “not really good” and “really not good” convey different levels of negative sentiment, while being different only by their word orderings. Contrary to SWEM, CNN and LSTM Table 4: Speed & Parameters on Yahoo! Answer dataset. On the ontology classification problem (DBpedia dataset), we observe the same trend, that SWEM exhibits comparable or even superior results, relative to CNN or LSTM models. Since there are no compositional parameters in SWEM, our models have an order of magnitude fewer parameters (excluding embeddings) than LSTM or CNN, and are considerably more computationally efficient. As illustrated in Table 4, SWEM-concat achieves better results on Yahoo! Answer than CNN/LSTM, with only 61K parameters (one-tenth the number of LSTM parameters, or one-third the number of CNN parameters), while taking a fraction of the training time relative to the CNN or LSTM.  ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778",
        "description": "The image is a table labeled 'Table 4: Speed & Parameters on Yahoo! Answer dataset.' The table compares three different models: CNN, LSTM, and SWEM. It has three columns: Model, Parameters, and Speed. The rows represent the respective values for each model. For the CNN model, it has 541K parameters and a speed of 171 seconds. The LSTM model has 1.8M parameters and a speed of 598 seconds. The SWEM model has 61K parameters and a speed of 63 seconds. The table highlights the differences in computational efficiency and parameter count among these models.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_5.jpg",
        "caption": [
            "Figure 1: Histograms for learned word embeddings (randomly initialized) of SWEM-max and GloVe embeddings for the same vocabulary, trained on the Yahoo! Answer dataset. "
        ],
        "footnote": [],
        "context": "In this regard, the nature of max-pooling process gives rise to a more interpretable model. For a document, only the word with largest value in each embedding dimension is employed for the fi- nal representation. Thus, we suspect that semantically similar words may have large values in some shared dimensions. So motivated, after training the SWEM-max model on the Yahoo dataset, we selected five words with the largest values, among the entire vocabulary, for each word embedding dimension (these words are selected preferentially in the corresponding dimension, by the max operation). As shown in Table 3, the words chosen wrt word embedding dimensions, for the entire vocabulary. As shown in Figure 1, most of the values are highly concentrated around zero, indicating that the word embeddings learned are very sparse. On the contrary, the GloVe word embeddings, for the same vocabulary, are considerably denser than the embeddings learned from SWEM-max. This suggests that the model may only depend on a few key words, among the entire vocabulary, for predictions (since most words do not contribute to the max-pooling operation in SWEM-max). Through the embedding, the model learns the important words for a given task (those words with non-zero embedding components). ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-156a1e35714463aabe81efd03d2bb137",
        "description": "The image is a histogram comparing the learned word embeddings of SWEM-max and GloVe for the same vocabulary, trained on the Yahoo! Answer dataset. The x-axis represents the embedding amplitude, ranging from -1.5 to 1.5, while the y-axis represents the frequency, scaled to \\(10^7\\). The histogram for GloVe embeddings is depicted in light blue, showing a distribution with multiple peaks and valleys, indicating a denser spread of values around zero. In contrast, the histogram for SWEM-max embeddings is shown in red, with a single, tall peak centered at zero, indicating that most of the values are highly concentrated around zero, making the embeddings very sparse. This suggests that SWEM-max relies on a few key words for predictions, as most words do not contribute significantly to the max-pooling operation.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "alignments between two sequences (Parikh et al., 2016). From this perspective, word-order information becomes much less useful for predicting relationship between sentences. Moreover, considering the simpler model architecture of SWEM, they could be much easier to be optimized than LSTM or CNN-based Table 5: Performance of different models on matching natural language sentences. Results with ⇤are for Bidirectional LSTM, reported in Williams et al. (2017). Our reported results on MultiNLI are only trained MultiNLI training set (without training data from SNLI). For MSRP dataset, we follow the setup in Hu et al. (2014) and do not use any additional features. performs the best among all SWEM variants, consistent with the findings in Nie and Bansal (2017); Conneau et al. (2017), that max-pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset. As a result, with only 120K parameters, our SWEM-max achieves a test accuracy of $83.8\\%$ , which is very competitive among state-ofthe-art sentence encoding-based models (in terms of both performance and number of parameters)1. The strong results of the SWEM approach on these tasks may stem from the fact that when matching natural language sentences, it is suffi- cient in most cases to simply model the word-level ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-156a1e35714463aabe81efd03d2bb137",
        "description": "The image is a table that presents the performance of different models on matching natural language sentences across various datasets. The table is structured with rows representing different models and columns representing different datasets. The datasets are SNLI, MultiNLI (Matched and Mismatched), WikiQA, Quora, and MSRP. Each dataset has specific metrics reported: SNLI and MultiNLI report accuracy (Acc.), WikiQA reports Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), Quora reports accuracy (Acc.), and MSRP reports accuracy (Acc.) and F1 score. The models listed are CNN, LSTM, SWEM-aver, SWEM-max, and SWEM-concat. For example, CNN achieves an accuracy of 82.1% on SNLI, 65.0% on MultiNLI Matched, and 65.3% on MultiNLI Mismatched. LSTM achieves an accuracy of 80.6% on SNLI, 66.9% on MultiNLI Matched, and 66.9% on MultiNLI Mismatched. SWEM-max performs the best among all SWEM variants, achieving an accuracy of 83.8% on SNLI, 68.2% on MultiNLI Matched, and 67.7% on MultiNLI Mismatched. The table highlights the competitive performance of SWEM-max with only 120K parameters.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "that word-order information does not contribute significantly on these two problems, i.e., topic categorization and textual entailment. However, on the Yelp polarity dataset, the results   The results on three distinct tasks are shown in Table 6. Somewhat surprisingly, for Yahoo and SNLI datasets, the LSTM model trained on shuffled training set shows comparable accuracies to those trained on the original dataset, indicating Table 7: Test samples from Yelp Polarity dataset for which LSTM gives wrong predictions with shuffled training data, but predicts correctly with the original training set. Table 6: Test accuracy for LSTM model trained on original/shuffled training set. performance as the CNN or LSTM on a variety of tasks. In this regard, one natural question would be: how important are word-order features for these tasks? To this end, we randomly shuffle the words for every sentence in the training set, while keeping the original word order for samples in the test set. The motivation here is to remove the word-order features from the training set and examine how sensitive the performance on different tasks are to word-order information. We use LSTM as the model for this purpose since it can captures wordorder information from the original training set. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-f513c5de611803566998ac79f309def6",
        "description": "The image is a table labeled 'Table 6: Test accuracy for LSTM model trained on original/shuffled training set.' The table is structured with two main columns: Datasets and the corresponding accuracies for the Original and Shuffled training sets. The rows represent three distinct datasets: Yahoo, Yelp P., and SNLI. The accuracies are as follows: for the Yahoo dataset, the Original training set has an accuracy of 72.78%, and the Shuffled training set has an accuracy of 72.89%. For the Yelp P. dataset, the Original training set has an accuracy of 95.11%, and the Shuffled training set has an accuracy of 93.49%. For the SNLI dataset, the Original training set has an accuracy of 78.02%, and the Shuffled training set has an accuracy of 77.68%. The table highlights the performance comparison between the LSTM model trained on the original dataset and the same model trained on a shuffled dataset, indicating that the word-order information does not significantly affect the test accuracy for the Yahoo and SNLI datasets but shows a slight decrease for the Yelp P. dataset.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_8.jpg",
        "caption": [],
        "footnote": [],
        "context": "Notably, the performance of LSTM on the Yelp dataset with a shuffled training set is very close to our results with SWEM, indicating that the main difference between LSTM and SWEM may be due to the ability of the former to capture word-order features. Both observations are in consistent with our experimental results in the previous section. that word-order information does not contribute significantly on these two problems, i.e., topic categorization and textual entailment. However, on the Yelp polarity dataset, the results drop noticeably, further suggesting that word-order does matter for sentiment analysis (as indicated above from a different perspective).  tasks are to word-order information. We use LSTM as the model for this purpose since it can captures wordorder information from the original training set.  Table 6: Test accuracy for LSTM model trained on original/shuffled training set. The results on three distinct tasks are shown in Table 6. Somewhat surprisingly, for Yahoo and SNLI datasets, the LSTM model trained on shuffled training set shows comparable accuracies to those trained on the original dataset, indicating Table 7: Test samples from Yelp Polarity dataset for which LSTM gives wrong predictions with shuffled training data, but predicts correctly with the original training set. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-f513c5de611803566998ac79f309def6",
        "description": "The image is a table with two sections labeled 'Negative' and 'Positive'. The 'Negative' section states: 'Friendly staff and nice selection of vegetarian options. Food is just okay, not great. Makes me wonder why everyone likes food fight so much.' The 'Positive' section states: 'The store is small, but it carries specialties that are difficult to find in Pittsburgh. I was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights.'",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_9.jpg",
        "caption": [
            "Table 8: Test accuracies with different compositional functions on (short) sentence classifications. "
        ],
        "footnote": [],
        "context": "As demonstrated in Section 4.2.1, word-order information plays a vital role for sentiment analysis tasks. However, according to the case study above, the most important features for sentiment prediction may be some key $n$ -gram phrase/words from the input document. We hypothesize that incorporating information about the local word-order, i.e., $n$ -gram features, is likely to largely mitigate the limitations of the above three SWEM variants. Inspired by this observation, we propose using another simple pooling operation termed as hierarchical (SWEM-hier), as detailed in Section 3.3. We evaluate this method on the two documentlevel sentiment 4.3 SWEM-hier for sentiment analysis  to word-order information, we further show those samples that are wrongly predicted because of the shuffling of training data in Table 7. Taking the first sentence as an example, several words in the review are generally positive, i.e. friendly, nice, okay, great and likes. However, the most vital features for predicting the sentiment of this sentence could be the phrase/sentence ‘is just okay’, ‘not great’ or ‘makes me wonder why everyone likes’, which cannot be captured without considering word-order features. It is worth noting the hints for predictions in this case are actually $n$ -gram phrases from the input document. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-f513c5de611803566998ac79f309def6",
        "description": "The image is a table labeled 'Table 8: Test accuracies with different compositional functions on (short) sentence classifications.' The table is structured with the following columns: Model, MR, SST-1, SST-2, Subj, and TREC. Each row represents a different model and its corresponding test accuracy values for various datasets. The models listed include RAE, MV-RNN, LSTM, RNN, Constituency Tree-LSTM, Dynamic CNN, CNN, DAN-ROOT, SWEM-aver, SWEM-max, and SWEM-concat. The accuracy values are as follows: RAE has accuracies of 77.7 for MR, 43.2 for SST-1, and 82.4 for SST-2; MV-RNN has 79.0 for MR, 44.4 for SST-1, and 82.9 for SST-2; LSTM has 46.4 for SST-1 and 84.9 for SST-2; RNN has 93.7 for Subj and 90.2 for TREC; Constituency Tree-LSTM has 51.0 for SST-1 and 88.0 for SST-2; Dynamic CNN has 48.5 for SST-1 and 86.8 for SST-2; CNN has 81.5 for MR, 48.0 for SST-1, 88.1 for SST-2, 93.4 for Subj, and 93.6 for TREC; DAN-ROOT has 46.9 for SST-1 and 85.7 for SST-2; SWEM-aver has 77.6 for MR, 45.2 for SST-1, 83.9 for SST-2, 92.5 for Subj, and 92.2 for TREC; SWEM-max has 76.9 for MR, 44.1 for SST-1, 83.6 for SST-2, 91.2 for Subj, and 89.0 for TREC; SWEM-concat has 78.2 for MR, 46.1 for SST-1, 84.3 for SST-2, 93.0 for Subj, and 91.8 for TREC. The table highlights the performance of these models across different datasets.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_10.jpg",
        "caption": [],
        "footnote": [],
        "context": "(2) The pre-trained GloVe are frozen for the word embeddings, and only the model parameters are optimized. The results testing. The subspace training yields similar accuracy with direct training for very small $d$ , even when model parameters are not trained at all $\\stackrel{.}{d}=$ 0). This is because the word embeddings have the full degrees of freedom to adjust to achieve good solutions, regardless of the employed models. SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best solutions. According to Occam’s razor, simple models are preferred, if all else are the same.   $d$ that yield a good solution. Two models are studied: the SWEM-max variant, and the CNN model including a convolutional layer followed by a FC layer. We consider two settings: (1) The word embeddings are randomly intialized, and optimized jointly with the model parameters. We show the performance of direct and subspace training on AG News dataset in Figure 2 (a)(b). The two models trained via direct method share almost identical perfomrnace on training and (c) Testing on AG News (d)Testing on Yelp P. Figure 2: Performance of subspace training. Word embeddings are optimized in (a)(b), and frozen in (c)(d). ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-d762a89dc693820486c0957ced302e5e",
        "description": "The image consists of two line graphs plotted side by side. Each graph has a horizontal axis labeled 'Subspace dim d' ranging from 0 to 10 and a vertical axis labeled 'Accuracy' with values ranging from approximately 0.9 to 1.0. The left graph shows the performance of two models, SWEM (blue circles) and CNN (red circles), as the subspace dimension increases. Both models start at an accuracy of around 0.925 when d=0, with SWEM showing a slight increase in accuracy as d increases, reaching close to 1.0 at d=10. CNN also shows an increase but remains slightly below SWEM across all dimensions. The right graph shows the same models but with their word embeddings frozen. Here, both models start at a lower accuracy of around 0.89 when d=0. SWEM again shows a steady increase in accuracy, reaching close to 0.92 at d=10, while CNN shows a more erratic pattern with a peak at d=1 and then a gradual increase. Both graphs include dashed lines representing the direct training performance for comparison, with SWEM direct (blue dashed line) and CNN direct (red dashed line) maintaining a constant accuracy across all dimensions.",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1041/images/image_11.jpg",
        "caption": [],
        "footnote": [],
        "context": "(2) The pre-trained GloVe are frozen for the word embeddings, and only the model parameters are optimized. The results testing. The subspace training yields similar accuracy with direct training for very small $d$ , even when model parameters are not trained at all $\\stackrel{.}{d}=$ 0). This is because the word embeddings have the full degrees of freedom to adjust to achieve good solutions, regardless of the employed models. SWEM seems to have an easier loss landspace than CNN for word embeddings to find the best solutions. According to Occam’s razor, simple models are preferred, if all else are the same.  $d$ that yield a good solution. Two models are studied: the SWEM-max variant, and the CNN model including a convolutional layer followed by a FC layer. We consider two settings: (1) The word embeddings are randomly intialized, and optimized jointly with the model parameters. We show the performance of direct and subspace training on AG News dataset in Figure 2 (a)(b). The two models trained via direct method share almost identical perfomrnace on training and (c) Testing on AG News (d)Testing on Yelp P. Figure 2: Performance of subspace training. Word embeddings are optimized in (a)(b), and frozen in (c)(d).  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-d762a89dc693820486c0957ced302e5e",
        "description": "The image consists of two line graphs side by side, each plotting the accuracy of two models, SWEM and CNN, against the subspace dimension d. The x-axis represents the subspace dimension d, ranging from 0 to 1000 in increments of 200. The y-axis represents the accuracy, ranging from 0.5 to 0.9. Each graph contains four lines: a solid blue line for SWEM, a solid red line for CNN, a dashed blue line for SWEM direct, and a dashed red line for CNN direct. In both graphs, the SWEM and CNN lines start at a low accuracy near d=0 and gradually increase as d increases, approaching an accuracy of around 0.85 to 0.9. The SWEM direct and CNN direct lines are horizontal at the top of the graph, indicating a constant high accuracy regardless of d. The first graph on the left shows the performance when word embeddings are optimized, while the second graph on the right shows the performance when word embeddings are frozen. Both graphs demonstrate that the subspace training yields similar accuracy with direct training for very small d, and SWEM seems to have an easier loss landscape than CNN for word embeddings to find the best solutions.",
        "segmentation": false
    }
}