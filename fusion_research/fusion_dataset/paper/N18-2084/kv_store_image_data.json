{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_1.jpg",
        "caption": [],
        "footnote": [
            "Table 1: Number of sentences for each language pair. "
        ],
        "context": "For our experiments, we use a standard 1-layer encoder-decoder model with attention (Bahdanau et al., 2014) with a beam size The languages in each pair are similar in vocabulary, grammar and sentence structure (Matthews, 1997), which controls for language characteristics and also improves the possibility of transfer learning in multi-lingual models (in $^{\\S7}$ ). They also represent different language families – GL/PT are Romance; AZ/TR are Turkic; BE/RU are Slavic – allowing for comparison across languages with different caracteristics. Tokenization was done using Moses tokenizer4 and hard punctuation symbols were used to identify sentence boundaries. Table 1 shows data sizes. Is it helpful to align the embedding spaces between the source and target languages? ( 6) Q5 Do pre-trained embeddings help more in multilingual systems as compared to bilingual systems? ( 7) 2 Experimental Setup In order to perform experiments in a controlled, multilingual setting, we created a parallel corpus from TED talks transcripts.3 Specifically, we prepare data between English (EN) and three pairs of languages, where the two languages in the pair are similar, with one being relatively lowresourced compared to the other: Galician (GL) and Portuguese (PT), Azerbaijani (AZ) and Turkish (TR), and Belarusian (BE) and Russian (RU). ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-cadfed8974ee21c1b7231ddd6f30482d",
        "description": "The image is a table labeled 'Table 1: Number of sentences for each language pair.' The table is structured with the following columns: Dataset, train, dev, and test. Each row represents a different language pair and its corresponding number of sentences in the train, dev, and test datasets. The rows are as follows: GL → EN with 10,017 sentences in train, 682 in dev, and 1,007 in test; PT → EN with 51,785 sentences in train, 1,193 in dev, and 1,803 in test; AZ → EN with 5,946 sentences in train, 671 in dev, and 903 in test; TR → EN with 182,450 sentences in train, 4,045 in dev, and 5,029 in test; BE → EN with 4,509 sentences in train, 248 in dev, and 664 in test; RU → EN with 208,106 sentences in train, 4,805 in dev, and 5,476 in test. The table highlights the significant variation in dataset sizes across different language pairs.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_2.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Effect of pre-training on BLEU score over six languages. The systems use either random initialization (std) or pre-training $(\\mathtt{p r e})$ on both the source and target sides. "
        ],
        "context": "The gains from pre-training in the higherresource languages are consistent: ${\\approx}3$ BLEU points for all three language pairs. In contrast, for the extremely low-resource languages, the gains are either quite small (AZ The results in Table 2 clearly demonstrate that pre-training the word embeddings in the source and/or target languages helps to increase the BLEU scores to some degree. Comparing the second and third columns, we can see the increase is much more significant with pre-trained source language embeddings. This indicates that the majority of the gain from pre-trained word embeddings results from a better encoding of the source sentence.  baseline models without pre-training, we use Glorot and Bengio (2010)’s uniform initialization. 3 Q1: Efficacy of Pre-training In our first set of experiments, we examine the efficacy of pre-trained word embeddings across the various languages in our corpus. In addition to providing additional experimental evidence supporting the findings of other recent work on using pre-trained embeddings in NMT (Neishi et al., 2017; Artetxe et al., 2017; Gangi and Federico, 2017), we also examine whether pre-training is useful across a wider variety of language pairs and if it is more useful on the source or target side of a translation pair. ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-c08c72c2819b7238dbc77dc7b29b9028",
        "description": "The image is a table labeled 'Table 2: Effect of pre-training on BLEU score over six languages.' The table is structured with rows representing different language pairs and columns indicating the BLEU scores for systems using either random initialization (std) or pre-training (pre) on both the source and target sides. The language pairs are as follows: GL → EN, PT → EN, AZ → EN, TR → EN, BE → EN, and RU → EN. For each language pair, there are four values corresponding to the BLEU scores under different conditions: std → std, pre → std, std → pre, and pre → pre. The exact numbers are as follows: GL → EN has scores of 2.2, 13.2, 2.8, and 12.8 respectively; PT → EN has scores of 26.2, 30.3, 26.1, and 30.8; AZ → EN has scores of 1.3, 2.0, 1.6, and 2.0; TR → EN has scores of 14.9, 17.6, 14.7, and 17.9; BE → EN has scores of 1.6, 2.5, 1.3, and 3.0; RU → EN has scores of 18.5, 21.2, 18.7, and 21.1. The table highlights the significant gains from pre-training in higher-resource languages, with an approximate 3 BLEU points increase for all three language pairs. In contrast, for extremely low-resource languages, the gains are either quite small or negligible.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_3.jpg",
        "caption": [
            "Figure 1: BLEU and BLEU gain by data size. "
        ],
        "footnote": [],
        "context": "The main intuitive hypothesis as to why pretraining works is that the embedding space becomes more consistent, with semantically similar words closer together. We can also make an additional hypothesis: if the two languages in the translation pair are more linguistically similar, the semantic neighborhoods will be more similar between the two languages (i.e. semantic distinctions or polysemy will likely manifest themselves in more similar ways across more similar languages). As a result, we may expect that the gain from pre-training of embeddings may be larger when the source and target languages are more 5 Q3: Effect of Language Similarity  1/2, 1/4 and 1/8 of their original sizes. From the BLEU scores in Figure 1, we can see that for all three languages the gain in BLEU score demonstrates a similar trend to that found in GL in the previous section: the gain is highest when the baseline system is poor but not too poor, usually with a baseline BLEU score in the range of 3-4. This suggests that at least a moderately effective system is necessary before pre-training takes effect, but once there is enough data to capture the basic characteristics of the language, pre-training can be highly effective. ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-c08c72c2819b7238dbc77dc7b29b9028",
        "description": "The image consists of two graphs that illustrate the BLEU scores and the gain in BLEU scores for different language pairs as a function of training set size. The top graph shows the BLEU scores for Portuguese to English (Pt→En), Turkish to English (Tr→En), and Russian to English (Ru→En) translations. The x-axis represents the training set size, ranging from 0.2 to 1.0, while the y-axis represents the BLEU score, ranging from 5 to 30. The lines are color-coded: blue for Pt→En, red for Tr→En, and green for Ru→En. Each language pair has two lines, one solid and one dashed, representing standard (std) and pre-trained (pre) models, respectively. The bottom graph shows the gain in BLEU scores (BLEU(pre) - BLEU(std)) for the same language pairs. The x-axis is the same as the top graph, while the y-axis ranges from 2 to 14. The points are marked with circles and connected by lines, with colors corresponding to the language pairs. The data points indicate that the gain in BLEU scores is highest when the baseline system is poor but not too poor, usually with a baseline BLEU score in the range of 3-4.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Effect of linguistic similarity and pre-training on BLEU. The language family in the second column is the most recent common ancestor of source and target language. "
        ],
        "context": "Until now, we have been using embeddings that 6 Q4: Effect of Word Embedding Alignment From Table 3, we can see that the BLEU scores of ES, FR, and IT do generally follow this hypothesis. As we move to very different languages, RU and HE see larger accuracy gains than their more similar counterparts FR and IT. This can be largely attributed to the observation from the previous section that systems with larger headroom to improve tend to see larger increases; RU and HE have very low baseline BLEU scores, so it makes sense that their increases would be larger.  two languages (i.e. semantic distinctions or polysemy will likely manifest themselves in more similar ways across more similar languages). As a result, we may expect that the gain from pre-training of embeddings may be larger when the source and target languages are more similar. To examine this hypothesis, we selected Portuguese as the target language, which when following its language family tree from top to bottom, belongs to Indo-European, Romance, Western Romance, and West-Iberian families. We then selected one source language from each family above.8 To avoid the effects of training set size, all pairs were trained on 40,000 sentences. ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-c08c72c2819b7238dbc77dc7b29b9028",
        "description": "The image is a table labeled 'Table 3: Effect of linguistic similarity and pre-training on BLEU'. The table is structured with four main columns: Dataset, Lang. Family, std, and pre. Each row represents a different language pair, with the source language (ES, FR, IT, RU, HE) mapped to the target language PT (Portuguese). The 'Lang. Family' column indicates the most recent common ancestor of the source and target languages. The 'std' column shows the standard BLEU scores for each language pair, while the 'pre' column displays the BLEU scores after pre-training, along with the improvement in parentheses. The exact values are as follows: ES → PT has a std score of 17.8 and a pre score of 24.8 (+7.0); FR → PT has a std score of 12.4 and a pre score of 18.1 (+5.7); IT → PT has a std score of 14.5 and a pre score of 19.2 (+4.7); RU → PT has a std score of 2.4 and a pre score of 8.6 (+6.2); HE → PT has a std score of 3.0 and a pre score of 11.9 (+8.9). The table highlights the effect of linguistic similarity and pre-training on BLEU scores across different language pairs.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Finally, it is of interest to consider pre-training in multilingual translation systems that share an encoder or decoder between multiple languages (Johnson et al., 2016; Firat et al., 2016), which is another promising way to use additional data (this time from another language) as a way to improve NMT. Specifically, we train a model using our pairs of similar low-resource and higher-resource languages, and test on only the low-resource language. For 7 Q5: Effect of Multilinguality sary in the context of NMT, since the NMT system can already learn a reasonable projection of word embeddings during its normal training process.   word embeddings of multiple languages to a single space and used these aligned embeddings instead of independent ones. From Table 4, we can see that somewhat surprisingly, the alignment of word embeddings was not beneficial for training, with gains or losses essentially being insignificant across all languages. This, in a way, is good news, as it indicates that $a$ priori alignment of embeddings may not be necesTable 5: Effect of pre-training on multilingual translation into English. bi is a bilingual system trained on only the eval source language and all others are multi-lingual systems trained on two similar source languages. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-6693f64a0d61b07061a158adb3d374d9",
        "description": "The image is a table labeled 'Table 5: Effect of pre-training on multilingual translation into English.' The table is structured with two main columns: 'unaligned' and 'aligned.' Each row represents a different language pair, with the source language (GL, PT, AZ, TR, BE, RU) translated into English (EN). The rows are as follows: GL → EN with values 12.8 for unaligned and 11.5 (-1.3) for aligned; PT → EN with values 30.8 for unaligned and 30.6 (-0.2) for aligned; AZ → EN with values 2.0 for unaligned and 2.1 (+0.1) for aligned; TR → EN with values 17.9 for unaligned and 17.7 (-0.2) for aligned; BE → EN with values 3.0 for unaligned and 3.0 (+0.0) for aligned; RU → EN with values 21.1 for unaligned and 21.4 (+0.3) for aligned. The table highlights the effect of alignment on the translation quality, with some languages showing slight improvements and others showing slight declines.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_6.jpg",
        "caption": [
            "Table 4: Correlation between word embedding alignment and BLEU score in bilingual translation task. "
        ],
        "footnote": [],
        "context": "Finally, it is of interest to consider pre-training in multilingual translation systems that share an encoder or decoder between multiple languages (Johnson et al., 2016; Firat et al., 2016), which is another promising way to use additional data (this time from another language) as a way to improve NMT. Specifically, we train a model using our pairs of similar low-resource and higher-resource languages, and test on only the low-resource language. For 7 Q5: Effect of Multilinguality sary in the context of NMT, since the NMT system can already learn a reasonable projection of word embeddings during its normal training process.  word embeddings of multiple languages to a single space and used these aligned embeddings instead of independent ones. From Table 4, we can see that somewhat surprisingly, the alignment of word embeddings was not beneficial for training, with gains or losses essentially being insignificant across all languages. This, in a way, is good news, as it indicates that $a$ priori alignment of embeddings may not be necesTable 5: Effect of pre-training on multilingual translation into English. bi is a bilingual system trained on only the eval source language and all others are multi-lingual systems trained on two similar source languages.  ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-6693f64a0d61b07061a158adb3d374d9",
        "description": "The image is a table labeled 'Table 4: Correlation between word embedding alignment and BLEU score in bilingual translation task.' The table is structured with the following columns: Train, Eval, bi, std, pre, and align. Each row represents different language pairs and their corresponding BLEU scores under various conditions. The rows are as follows: GL + PT evaluated on GL with scores of 2.2 for bi, 17.5 for std, 20.8 for pre, and 22.4 for align; AZ + Tr evaluated on AZ with scores of 1.3 for bi, 5.4 for std, 5.9 for pre, and 7.5 for align; BE + Ru evaluated on BE with scores of 1.6 for bi, 10.0 for std, 7.9 for pre, and 9.6 for align. The table highlights the correlation between word embedding alignment and BLEU scores in bilingual translation tasks.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "Finally, we performed a comparison of the fmeasure of target words, bucketed by frequency in the training corpus. As displayed in Figure 2, this shows that pre-training manages to improve the accuracy of translation for the entire vocabulary, but particularly Figure 2: The f-measure of target words in bilingual translation task $\\mathrm{PT}\\rightarrow\\mathrm{EN}$  dings were not very consistent, and largely focused on high-frequency words. Table 7: Top $10\\;\\mathrm{n}$ -grams that one system did a better job of producing. The numbers in the figure, separated by a slash, indicate how many times each n-gram is generated by each of the two systems.  and thus increases the uncertainty in predicting next words, generating several phrasal loops which are typical in NMT systems. 8.2 Analysis of Frequently Generated $n$ -grams. We additionally performed pairwise comparisons between the top $10\\;\\mathrm{n}$ -grams that each system (selected from the task $\\mathrm{{GL}\\rightarrow\\mathrm{{EN})}}$ ) is better at generating, to further understand what kind of words pre-training is particularly helpful for. The results displayed in Table 7 demonstrate that pretraining helps both with words of low frequency in the training corpus, and even with function words such as prepositions. On the other hand, the improvements in systems without pre-trained embed ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-6693f64a0d61b07061a158adb3d374d9",
        "description": "The image is a table comparing different translations of a sentence from Portuguese to English. The table has four rows and two columns. The first column is labeled 'source' and contains the original Portuguese text: '( risos ) e é que chris é un grande avogado , pero non sabía case nada sobre lexislación de patentes e absolutamente nada sobre xenética .'. The second column is labeled 'reference' and contains the reference translation in English: '( laughter ) now chris is a really brilliant lawyer , but he knew almost nothing about patent law and certainly nothing about genetics .'. The third row is labeled 'bi:std' and contains the text: '( laughter ) and i ’m not a little bit of a little bit of a little bit of and ( laughter ) and i ’m going to be able to be a lot of years .'. The fourth row is labeled 'multi:pre-align' and contains the text: '( laughter ) and chris is a big lawyer , but i did n’t know almost anything about patent legislation and absolutely nothing about genetic .'.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_8.jpg",
        "caption": [
            "Table 6: Example translations of $\\mathrm{GL\\toEN}$ ."
        ],
        "footnote": [
            "(a) Pairwise comparison between two bilingual models ",
            "(b) Pairwise comparison between two multilingual models "
        ],
        "context": "Finally, we performed a comparison of the fmeasure of target words, bucketed by frequency in the training corpus. As displayed in Figure 2, this shows that pre-training manages to improve the accuracy of translation for the entire vocabulary, but particularly Figure 2: The f-measure of target words in bilingual translation task $\\mathrm{PT}\\rightarrow\\mathrm{EN}$  dings were not very consistent, and largely focused on high-frequency words. Table 7: Top $10\\;\\mathrm{n}$ -grams that one system did a better job of producing. The numbers in the figure, separated by a slash, indicate how many times each n-gram is generated by each of the two systems. and thus increases the uncertainty in predicting next words, generating several phrasal loops which are typical in NMT systems. 8.2 Analysis of Frequently Generated $n$ -grams. We additionally performed pairwise comparisons between the top $10\\;\\mathrm{n}$ -grams that each system (selected from the task $\\mathrm{{GL}\\rightarrow\\mathrm{{EN})}}$ ) is better at generating, to further understand what kind of words pre-training is particularly helpful for. The results displayed in Table 7 demonstrate that pretraining helps both with words of low frequency in the training corpus, and even with function words such as prepositions. On the other hand, the improvements in systems without pre-trained embed  ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-6693f64a0d61b07061a158adb3d374d9",
        "description": "The image is a table labeled 'Table 6: Example translations of GL→EN'. The table is divided into two main sections: (a) Pairwise comparison between two bilingual models and (b) Pairwise comparison between two multilingual models. Each section contains rows with different phrases or words and their corresponding translation statistics. For example, in the bilingual model comparison, the phrase 'so' has a score of 2/0 for bi:std and 0/53 for bi:pre. Similarly, in the multilingual model comparison, the word 'here' has a score of 6/0 for multi:std and 0/14 for multi:pre+align. The table provides detailed scores for various phrases and words, indicating how often each system generates these n-grams. Notable entries include 'laughter', 'people', 'several', 'you’re going', and 'testosterone', among others, with specific scores that highlight the performance differences between the systems.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-2084/images/image_9.jpg",
        "caption": [
            "8.3 F-measure of Target Words "
        ],
        "footnote": [],
        "context": "This paper examined the utility of considering pretrained word embeddings in NMT from a number of angles. Our conclusions have practical effects on the recommendations for when and why pretrained embeddings may be effective in 9 Conclusion Finally, we performed a comparison of the fmeasure of target words, bucketed by frequency in the training corpus. As displayed in Figure 2, this shows that pre-training manages to improve the accuracy of translation for the entire vocabulary, but particularly for words that are of low frequency in the training corpus. Figure 2: The f-measure of target words in bilingual translation task $\\mathrm{PT}\\rightarrow\\mathrm{EN}$ generating, to further understand what kind of words pre-training is particularly helpful for. The results displayed in Table 7 demonstrate that pretraining helps both with words of low frequency in the training corpus, and even with function words such as prepositions. On the other hand, the improvements in systems without pre-trained embed   Table 7: Top $10\\;\\mathrm{n}$ -grams that one system did a better job of producing. The numbers in the figure, separated by a slash, indicate how many times each n-gram is generated by each of the two systems. dings were not very consistent, and largely focused on high-frequency words. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-6693f64a0d61b07061a158adb3d374d9",
        "description": "The image is a bar chart titled 'F-measure of Target Words'. The x-axis represents the frequency in the training corpus, ranging from 0 to 1000+. The y-axis represents the F-measure, ranging from 0 to 0.8. There are two sets of bars for each frequency category: blue bars labeled 'std' and red bars labeled 'pre'. The F-measure values for each frequency category are as follows: 0 (std: 0.0, pre: 0.0), 1-2 (std: 0.0, pre: 0.0), 3 (std: 0.05, pre: 0.1), 4 (std: 0.05, pre: 0.15), 5-9 (std: 0.15, pre: 0.25), 10-99 (std: 0.45, pre: 0.55), 100-999 (std: 0.55, pre: 0.6), and 1000+ (std: 0.65, pre: 0.7). The chart shows that pre-training improves the accuracy of translation for the entire vocabulary, particularly for words of low frequency in the training corpus.",
        "segmentation": false
    }
}