{"question": "How does the paper propose to calculate the coefficient \u03b1 for the Weighted Cross Entropy Loss?", "answer": "The coefficient \u03b1 is calculated using the equation lg((n-nt)/(nt+K)), where nt is the number of samples with class t, n is the total number of samples in the training set, and K is a hyperparameter to tune.", "type": "text-only", "evidence": "\"In this work, we use lg((n-nt)/(nt+K)) to calculate the coefficient \u03b1...\""}
{"question": "Is the OntoNotes4.0 dataset used for the Named Entity Recognition task Chinese or English?", "answer": "Chinese.", "type": "text-only", "evidence": "The specific part in the text mentioning \"Chinese OntoNotes4.0 is a Chinese dataset\" confirms the language of the dataset."}
{"question": "Which model variant has the highest improvement in F1 score for the QuoRef dataset when compared to the base XLNet model?", "answer": "The model variant with the highest improvement in F1 score for the QuoRef dataset compared to the base XLNet model is XLNet+DSC, with a +1.41 increase.", "type": "multimodal-t", "evidence": "The table shows the improvement in parentheses, and the base XLNet model's score on QuoRef is compared to each variant model, with XLNet+DSC showing the highest positive difference in performance in the F1 score."}
{"question": "What data augmentation method resulted in the highest F1-score for the BERT model according to Table 8?", "answer": "The \"+ positive & negative\" data augmentation method resulted in the highest F1-score for the BERT model with a score of 93.14.", "type": "multimodal-t", "evidence": "The question asks for the maximum value in a single row, which is straightforward to find by looking at the BERT row across all data augmentation methods."}
{"question": "What is the highest F1 score achieved on the Chinese OntoNotes4.0 dataset, according to Table 10?", "answer": "The highest F1 score on the Chinese OntoNotes4.0 dataset is 84.67.", "type": "multimodal-t", "evidence": "The question requires identifying the maximum value in a single column, which is straightforward."}
{"question": "Which model achieved the highest F1 score in the English WSJ dataset?", "answer": "BERT-Tagger+DSC achieved the highest F1 score in the English WSJ dataset with a score of 99.38.", "type": "multimodal-t", "evidence": "The question requires examining only the F1 scores in the English WSJ section and identifying the highest one."}
{"question": "What performance boost did BERT+DSC achieve for the MRPC?", "answer": "BERT+DSC achieved a performance boost of +0.92 for the MRPC.", "type": "multimodal-t", "evidence": "In Table 7, it is shown next to BERT+DSC in the MRPC column."}
{"question": "How does the performance of BERT+DL on SST-5 compare to that of BERT+DSC?", "answer": "BERT+DL performs worse on SST-5 with an accuracy of 54.63 compared to BERT+DSC which has an accuracy of 55.19.", "type": "multimodal-t", "evidence": "Direct comparison between the BERT+DL and BERT+DSC accuracy numbers in the SST-5 column shows that BERT+DL has a lower accuracy."}
{"question": "By how much does the accuracy of BERT+CE on SST-2 exceed that of BERT+DL?", "answer": "The accuracy of BERT+CE on SST-2 exceeds that of BERT+DL by 0.53.", "type": "multimodal-t", "evidence": "Subtraction of the BERT+DL accuracy (94.37) from the BERT+CE accuracy (94.90) on SST-2 gives the difference of 0.53."}
{"question": "What is the ratio of negative to positive examples for the Quoref task?", "answer": "The ratio of negative to positive examples for the Quoref task is 169.", "type": "multimodal-t", "evidence": "This information is directly stated in the table under the column \"ratio\" for the Quoref task."}
