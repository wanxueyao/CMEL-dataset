{
  "chunk-cfc4856ec35cb9a74b12e7ab5c8a14fc": {
    "tokens": 1200,
    "content": "# Neural News Recommendation with Long- and Short-term User Representations  \n\n# Mingxiao $\\mathbf{A}\\mathbf{n}^{1,*}$ , Fangzhao $\\mathbf{W}\\mathbf{u}^{2}$ , Chuhan $\\mathbf{W}\\mathbf{u}^{3}$ , Kun Zhang1, Zheng Liu2, Xing Xie2  \n\n1University of Science and Technology of China, Hefei 230026, China 2Microsoft Research Asia, Beijing 100080, China   \n3Department of Electronic Engineering, Tsinghua University, Beijing 100084, China anmx,zhkun@mail.ustc.edu.cn, wufangzhao@gmail.com   \nwuch15@mails.tsinghua.edu.cn, zhengliu,xingx@microsoft.com  \n\n# Abstract  \n\nPersonalized news recommendation is important to help users find their interested news and improve reading experience. A key problem in news recommendation is learning accurate user representations to capture their interests. Users usually have both long-term preferences and short-term interests. However, existing news recommendation methods usually learn single representations of users, which may be insufficient. In this paper, we propose a neural news recommendation approach which can learn both long- and short-term user representations. The core of our approach is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use attention network to select important words. In the user encoder, we propose to learn long-term user representations from the embeddings of their IDs. In addition, we propose to learn short-term user representations from their recently browsed news via GRU network. Besides, we propose two methods to combine long-term and short-term user representations. The first one is using the long-term user representation to initialize the hidden state of the GRU network in short-term user representation. The second one is concatenating both long- and short-term user representations as a unified user vector. Extensive experiments on a real-world dataset show our approach can effectively improve the performance of neural news recommendation.  \n\n# 1 Introduction  \n\nOnline news platforms such as MSN News1 and Google News2 which aggregate news from various sources and distribute them to users have gained huge popularity and attracted hundreds of millions of users (Das et al., 2007; Wang et al., 2018). However, massive news are generated everyday, making it impossible for users to read through all news (Lian et al., 2018). Thus, personalized news recommendation is very important for online news platforms to help users find their interested contents and alleviate information overload (Lavie et al., 2010; Zheng et al., 2018).  \n\n![](images/image_1.jpg)  \nFigure 1: An illustrative example of long-term and short-term interests in news reading.  \n\nLearning accurate user representations is critical for news recommendation (Okura et al., 2017). Existing news recommendation methods usually learn a single representation for each user (Okura et al., 2017; Lian et al., 2018; Wu et al., 2019). For example, Okura et al. (2017) proposed to learn representations of news using denoising autoencoder and learn representations of users from their browsed news using GRU network (Cho et al., 2014). However, it is very difficult for RNN networks such as GRU to capture the entire information of very long news browsing history. Wang et al. (2018) proposed to learn the representations of news using knowledge-aware convolutional neural network (CNN), and learn the representations of users from their browsed news based on the similarities between the candidate news and the browsed news. However, this method needs to store the entire browsing history of each user in the online news recommendation stage, which may bring huge challenge to the storage and may cause heavy latency.  \n\nOur work is motivated by the observation that the interests of online users in news are very diverse. Some user interests may last for a long time and are consistent for the same user (Li et al., 2014). For example, as shown in Fig. 1, if a user is a fan of “Golden State Warriors”, this user may tend to read many basketball news about this NBA team for several years. We call this kind of user preferences as long-term interest. In addition, many user interests may evolve with time and may be triggered by specific contexts or temporal demands. For example, in Fig. 1, the browsing of the news on movie “Bohemian Rhapsody” causes the user reading several related news such as “Rami Malek Wins the 2019 Oscar” since “Rami Malek” is an important actor in this movie, although this user may never read news about “Rami Malek” before. We call this kind of user interests as shortterm interest. Thus, both long-term and shortterm user interests are important for personalized news recommendation, and distinguishing longterm user interests from short-term ones may help learn more accurate user representations.  \n\nIn this paper, we propose a neural news recommendation approach with both long- and shortterm user representations (LSTUR). Our approach contains two major components, i.e., a news encoder and a user encoder. The news encoder is used to learn representations of news articles from their titles and topic categories. We apply attention mechanism to the news encoder to learn informative news representations by selecting important words. The user encoder consists of two modules, i.e., a long-term user representation (LTUR) module and a short-term user representation (STUR) module. In STUR, we use a GRU network to learn short-term representations of users from their recently browsing news. In LTUR, we learn the long-term representations of users from the embeddings of their IDs. In addition, we propose two methods to combine the short-term and longterm user representations. The first",
    "chunk_order_index": 0,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-f08fa90ed86e33cca2697ff366884585": {
    "tokens": 1200,
    "content": "by selecting important words. The user encoder consists of two modules, i.e., a long-term user representation (LTUR) module and a short-term user representation (STUR) module. In STUR, we use a GRU network to learn short-term representations of users from their recently browsing news. In LTUR, we learn the long-term representations of users from the embeddings of their IDs. In addition, we propose two methods to combine the short-term and longterm user representations. The first one is using the long-term user representations to initialize the hidden state of GRU network in the STUR model. The second one is concatenating the long-tern and short-term user representations as a unified user vector. We conducted extensive experiments on a real-world dataset. The experimental results show our approach can effectively improve the performance of news recommendation and consistently outperform many baseline methods.  \n\n# 2 Related Works  \n\nPersonalized news recommendation is an important task in natural language processing field and has wide applications (Zheng et al., 2018). It is critical for news recommendation methods to learn accurate news and user representations (Wang et al., 2018). Many conventional news recommendation methods rely on manual feature engineering to build news and user representations (Phelan et al., 2009; Liu et al., 2010; Li et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015; Lian et al., 2018). For example, Liu et al. (2010) proposed to use the topic categories and interests features predicted by a Bayesian model to represent news, and use the click distribution features of news categories to represent users. Li et al. (2014) used a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) model to generate topic distribution features as the news representations. They represented a session by using the topic distribution of browsed news in this session, and the representations of users were built from their session representations weighted by the time. However, these methods heavily rely on manual feature engineering, which needs massive domain knowledge to craft. In addition, the contexts and orders of words in news are not incorporated, which are important for understanding the semantic meanings of news and learning representations of news and users.  \n\nIn recent years, several deep learning methods were proposed for personalized news recommendation (Wang et al., 2018; Okura et al., 2017; Zheng et al., 2018). For example, Okura et al. (2017) proposed to learn representations of news from news bodies using denoising autoencoder, and learn representations of users from the representations of their browsed news using a GRU network. Wang et al. (2018) proposed to learn representations of news from their titles via a knowledge-aware CNN network, and learn representations of users from the representations of their browsed news articles weighted by their similarities with the candidate news. Wu et al. (2019) proposed to learn news and user representations with personalized word- and news-level attention networks, which exploits the embedding of user ID to generate the query vector for the attentions. However, these methods usually learn a single representation vector for each user, and cannot distinguish the long-term preferences and short-term interests of users in reading news. Thus, the user representations learned in these methods may be insufficient for news recommendation. Different from these methods, our approach can learn both long-term and short-term user representations in a unified framework to capture the diverse interests of users for personalized neural new commendation. Extensive experiments on the real-world dataset validate the effectiveness of our approach and the advantage over many baseline methods.  \n\n# 3 Our Approach  \n\nIn this section, we present our neural news recommendation approach with long- and short-term user representations (LSTUR). Our approach contains two major components, i.e., a news encoder to learn representations of news and a user encoder to learn representations of users. Next, we introduce each component in detail.  \n\n# 3.1 News Encoder  \n\nThe news encoder is used to learn representations of news from their titles, topic and subtopic categories. The architecture of the news encoder in our approach is illustrated in Fig. 2. There are two sub-modules in the news encoder, i.e., a title encoder and a topic encoder.  \n\nThe title encoder is used to learn news representations from titles. There are three layers in the title encoder. The first layer is word embedding, which is used to convert a news title from a word sequence into a sequence of dense semantic vectors. Denote the word sequence in a news title $t$ as $t=[w_{1},w_{2},\\ldots,w_{N}]$ , where $N$ is the length of this title. It is transformed into $[{\\pmb w}_{1},{\\pmb w}_{2},\\dots,{\\pmb w}_{N}]$ via a word embedding matrix.  \n\nThe second layer in title encoder is a convolutional neural network (CNN) (LeCun et al., 2015). Local contexts are very useful for understanding the semantic meaning of news titles. For example, in the news title “Next season of super bowl games”, the local contexts of “bowl” such as “super” and “games” are very important for inferring that it belongs to a sports event name. Thus, we apply a CNN network to learn contextual word representations by capturing the local context information. Denote the contextual representation of $w_{i}$ as $c_{i}$ , which is computed as follows:  \n\n$$\n\\begin{array}{r}{\\pmb{c}_{i}=\\mathrm{ReLU}(\\pmb{C}\\times\\pmb{w}_{[i-M:i+M",
    "chunk_order_index": 1,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-40a50e43f2e71428b8dbff5c388e34ae": {
    "tokens": 1200,
    "content": "very important for inferring that it belongs to a sports event name. Thus, we apply a CNN network to learn contextual word representations by capturing the local context information. Denote the contextual representation of $w_{i}$ as $c_{i}$ , which is computed as follows:  \n\n$$\n\\begin{array}{r}{\\pmb{c}_{i}=\\mathrm{ReLU}(\\pmb{C}\\times\\pmb{w}_{[i-M:i+M]}+\\pmb{b}),}\\end{array}\n$$  \n\nwhere ${\\pmb w}_{[i-M:i+M]}$ is the concatenation of the embeddings of words between position $i\\,-\\,M$ and $i+M$ .${\\cal C}$ and $^{b}$ are the parameters of the convolutional filters in CNN, and $M$ is the window size.  \n\n![](images/image_2.jpg)  \nFigure 2: The framework of the news encoder.  \n\nThe third layer is an attention network (Bahdanau et al., 2015). Different words in the same news title may have different informativeness for representing news. For instance, in the news title “The best NBA moments in $2018^{\\circ}$ , the word “NBA” is very informative for representing this news since it is an important indication of sports news, while the word $^{\\bullet\\bullet}2018^{\\bullet}$ is less informative. Thus, we employ a word-level attention network to select important words in news titles to learn more informative news representations. The attention weight $\\alpha_{i}$ of the $i$ -th word is formulated as follows:  \n\n$$\n\\begin{array}{l}{a_{i}=\\operatorname{tanh}(\\boldsymbol{v}\\times\\boldsymbol{c}_{i}+\\boldsymbol{v}_{b}),}\\\\ {\\alpha_{i}=\\frac{\\exp(a_{i})}{\\sum_{j=1}^{N}\\exp(a_{j})},}\\end{array}\n$$  \n\nwhere $\\pmb{v}$ and $v_{b}$ are the trainable parameters. The final representation of a news title $t$ is the summation of its contextual word representations weighted by their attention weights as follows:  \n\n$$\ne_{t}=\\sum_{i=1}^{N}\\alpha_{i}\\pmb{c}_{i}.\n$$  \n\nThe topic encoder module is used to learn news representations from its topics and subtopics. On many online news platforms such as MSN news, news articles are usually labeled with a topic category (e.g., “Sports”) and a subtopic category (e.g., “Football NFL”) to help target user interests. The topic and subtopic categories of news are also informative for learning representations of news and users. They can reveal the general and detailed topics of the news, and reflect the preferences of users. For example, if a user browsed many news articles with the “Sports” topic category, then we can infer this user is probably interested in sports, and it may be effective to recommend candidate news in the “Sports” topic category to this user. To incorporate the topic and subtopic information into news representation, we propose to learn the representations of topics and subtopics from the embeddings of their IDs, as shown in Fig. 2. Denote $e_{v}$ and $e_{s v}$ as the representations of topic and subtopic. The final representation of a news article is the concatenation of the representations of its title, topic and subtopic, i.e., $\\boldsymbol{e}=[e_{t},e_{v},e_{s v}]$ .  \n\n![](images/image_3.jpg)  \nFigure 3: The two frameworks of our LSTUR approach.  \n\n# 3.2 User Encoder  \n\nThe user encoder is used to learn representations of users from the history of their browsed news. It contains two modules, i.e., a short-term user representation model (STUR) to capture user’s temporal interests, and a long-term user representation model (LTUR) to capture user’s consistent preferences. Next, we introduce them in detail.  \n\n# 3.2.1 Short-Term User Representation  \n\nOnline users may have dynamic short-term interests in reading news articles, which may be influenced by specific contexts or temporal information demands. For example, if a user just reads a news article about “Mission: Impossible 6 – Fallout”, and she may want to know more about the actor “Tom Cruise” in this movie and click news articles related to “Tom Cruise”, although she is not his fan and may never read his news before. We propose to learn the short-term representations of users from their recent browsing history to capture their temporal interests, and use gated recurrent networks (GRU) (Cho et al., 2014) network to capture the sequential news reading patterns (Okura et al., 2017). Denote news browsing sequence from a user sorted by timestamp in ascending order as $\\mathcal{C}=\\{c_{1},c_{2},\\ldots,c_{k}\\}$ , where $k$ is the length of this sequence. We apply the news encoder to obtain the representations of these browsed articles, denoted as $\\{e_{1},e_{2},\\ldots,e_{k}\\}$ . The short-term user representation is computed as follows:  \n\n$$\n\\begin{array}{r l}&{{{r}_{t}}=\\sigma({W}_{r}[{{h}_{t-1}},{{e}_{t}}]),}\\\\ &{{{z}_{t}}=\\sigma({W}_{z}[{{h}_{t-1}},{{e}_{t}}]),}\\\\ &{{{\\tilde{h}}_{t}}=\\operator",
    "chunk_order_index": 2,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-b964f30fcaf33e260b19e3732b140766": {
    "tokens": 1200,
    "content": "2},\\ldots,e_{k}\\}$ . The short-term user representation is computed as follows:  \n\n$$\n\\begin{array}{r l}&{{{r}_{t}}=\\sigma({W}_{r}[{{h}_{t-1}},{{e}_{t}}]),}\\\\ &{{{z}_{t}}=\\sigma({W}_{z}[{{h}_{t-1}},{{e}_{t}}]),}\\\\ &{{{\\tilde{h}}_{t}}=\\operatorname{tanh}({W}_{\\tilde{h}}[{{r}_{t}}\\odot{{h}_{t-1}},{{e}_{t}}]),}\\\\ &{{{h}_{t}}={{z}_{t}}\\odot{{h}_{t}}+(1-{{z}_{t}})\\odot{{\\tilde{h}}_{t}},}\\end{array}\n$$  \n\nwhere $\\sigma$ is the sigmoid function, $\\odot$ is the itemwise product, $W_{r},\\,W_{z}$ and $W_{\\tilde{h}}$ are the parameters of the GRU network. The short-term user representation is the last hidden state of the GRU network, i.e., $u_{s}=h_{k}$ .  \n\n# 3.2.2 Long-Term User Representations  \n\nBesides the temporal interests, online users may also have long-term interests in reading news. For example, a basketball fan may tend to browse many sports news related to NBA in several years. Thus, we propose to learn long-term representations of users to capture their consistent preferences. In our approach the long-term user representations are learned from the embeddings of the user IDs, which are randomly initialized and finetuned during model training. Denote $u$ as the ID of a user and $W_{u}$ as the look-up table for long-term user representation, the long-term user representation of this user is $\\pmb{u}_{l}=\\pmb{W}_{u}[\\boldsymbol{u}]$ .  \n\n# 3.2.3 Long- and Short-Term User Representation  \n\nIn this section, we introduce two methods to combine the long-term and short-term user presentations for unified user representation, which are shown in Fig. 3.  \n\nThe first method is using the long-term user representation to initialize the hidden state of the GRU network in the short-term user representation model, as shown in Fig. 3a. We denote this method as LSTUR-ini. We use the last hidden state of the GRU network as the final user representation. The second method is concatenating the long-term user representation with the short-term user representation as the final user representation, as shown in Fig. 3b. We denote this method as LSTUR-con.  \n\n# 3.3 Model Training  \n\nFor online news recommendation services where user and news representations can be computed in advance, the scoring function should be as simple as possible to reduce latency. Motivated by (Okura et al., 2017), we use the simple dot production to compute the news click probability score. Denote the representation of a user $u$ as $\\pmb{u}$ and the representation of a candidate news article $e_{x}$ as $e_{x}$ , the probability score $s(u,c_{x})$ of this user clicking this news is computed as $s(u,c_{x})=\\pmb{u}^{\\top}\\pmb{e}_{x}$ .  \n\nMotivated by (Huang et al., 2013) and (Zhai et al., 2016), we propose to use the negative sampling technique for model training. For each news browsed by a user (regarded as a positive sample), we randomly sample $K$ news articles from the same impression which are not clicked by this user as negative samples. Our model will jointly predict the click probability scores of the positive news and the $K$ negative news. In this way, the news click prediction problem is reformulated as a pseudo $K+1$ -way classification task. We minimize the summation of the negative log-likelihood of all positive samples during training, which can be formulated as follows:  \n\n$$\n-\\sum_{i=1}^{P}\\log\\frac{\\exp(s(u,c_{i}^{p}))}{\\exp(s(u,c_{i}^{p}))+\\sum_{k=1}^{K}\\exp(s(u,c_{i,k}^{n}))},\n$$  \n\nwhere $P$ is the number of positive training samples, and $c_{i,k}^{n}$ is the $k$ -th negative sample in the same session with the $i$ -th positive sample.  \n\nSince not all users can be incorporated in news recommendation model training (e.g., the new coming users), it is not appropriate to assume all users have long-term representations in our models in the prediction stage. In order to handle this problem, in the model training stage, we randomly mask the long-term representations of users with a certain probability $p$ . When we mask the longterm representations, all the dimensions are set to zero. Thus, the long-term user representation in our LSTUR approach can be reformulated as:  \n\n$$\n\\pmb{u}_{l}=M\\cdot\\pmb{W}_{u}[u],M\\sim B(1,1-p),\n$$  \n\nwhere $B$ is Bernoulli distribution, and $M$ is a random variable that subject to $B(1,1{-}p)$ . We find in experiments that this trick for model training can improve the performance of our approach.  \n\n# 4 Experiments  \n\n# 4.1 Dataset and Experimental Settings  \n\nSince there is no off-the-shelf dataset for news recommendation, we built one by ourselves",
    "chunk_order_index": 3,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-72f78c27dad5114374bbbb67fbfc4777": {
    "tokens": 1200,
    "content": "}[u],M\\sim B(1,1-p),\n$$  \n\nwhere $B$ is Bernoulli distribution, and $M$ is a random variable that subject to $B(1,1{-}p)$ . We find in experiments that this trick for model training can improve the performance of our approach.  \n\n# 4 Experiments  \n\n# 4.1 Dataset and Experimental Settings  \n\nSince there is no off-the-shelf dataset for news recommendation, we built one by ourselves through collecting logs from MSN News in four weeks from December 23rd, 2018 to January 19th, 2019. We used the logs in the first three weeks for model training, and those in the last week for test. We also randomly sampled $10\\%$ of logs from the training set as the validation data. For each sample, we collected the browsing history in last 7 days to learn short-term user representations. The detailed dataset statistics are summarized in Table 1.  \n\nTable 1: Statistics of the dataset in our experiments.   \n\n![](images/image_4.jpg)  \n\nIn our experiments, we used the pretrained GloVe embedding (Pennington et al., 2014) as the initialization of word embeddings. The word embedding dimension is 200. The number of filters in CNN network is 300, and the window size of the filters in CNN network is set to 3. We applied dropout (Srivastava et al., 2014) to each layer in our approach to mitigate overfitting. The dropout rate is 0.2. The default value of long-term user representation masking probability $p$ for model training is 0.5. We used Adam (Kingma and Ba, 2014) to optimize the model, and the learning rate was 0.01. The batch size is set to 400. The number of negative samples for each positive sample is 4. These hyper-parameters were all selected according to the results on validation set. We used impression-based ranking metrics to evaluate the performance, including area under the ROC curve (AUC), mean reciprocal rank (MRR), and normalized discounted cumulative gain (nDCG). We repeated each experiment for 10 times independently, and reported the average results with 0.95 confidence probability.  \n\n# 4.2 Performance Evaluation  \n\nWe evaluate the performance of our approach by comparing it with several baseline methods, including:  \n\n• LibFM (Rendle, 2012), a state-of-the-art matrix factorization method which is widely used in recommendation. In our experiments, the user features are the concatenation of TF-IDF features extracted from the browsed news titles, and the normalized count features from the topics and subtopics of the browsed news. The features for news consists of TFIDF features from its title, and one-hot vectors of its topic and subtopic. The input to LibFM is the concatenation of user features and features of candidate news.   \n• DeepFM (Guo et al., 2017), a widely used method that combines factorization machines and deep neural networks. We use the same features as LibFM.   \n• Wide & Deep (Cheng et al., 2016), another deep learning based recommendation method that combines a wide channel and a deep channel. Again, the same features with LibFM are used for both channels.   \n• DSSM (Huang et al., 2013), deep structured semantic model. The inputs are hashed words via character trigram, where all the browsed news titles are merged as query document.   \n• CNN (Kim, 2014), using CNN with max pooling to learn news representations from the titles of browsed news by keeping the most salient features.   \n• DKN (Wang et al., 2018), a deep news recommendation model which contains CNN and candidate-aware attention on the news browsing histories.   \n• GRU (Okura et al., 2017), learning news representations by a denoising autoencoder and user representations by a GRU network.  \n\nThe results of comparing different methods are summarized in Table 2.  \n\nWe have obtained observations from Table 2. First, the news recommendation methods (e.g. CNN, DKN and LSTUR) which use neural networks to learn news and user representations can significantly outperform the methods using manual feature engineering (e.g. LibFM, DeepFM, Wide & Deep, and DSSM). This is probably because handcrafted features are usually not optimal, and neural networks can capture both global and local semantic contexts in news, which are useful for learning more accurate news and user representations for news recommendation.  \n\nSecond, our LSTUR approach outperforms all baseline methods compared here, including deep learning models such as CNN, GRU and DKN. Our LSTUR approach can capture both the long-term preferences and short-term interests to capture the complex and diverse user interests in news reading, while the baseline methods only learn a single representation for each user, which is insufficient. In addition, our LSTUR approach uses attention mechanism in the news encoder to select important words, which can help learn more informative news representations.  \n\nThird, our proposed two methods to learn longand short-term user representations, i.e., LSTURini and LSTUR-con, can achieve comparable performance and both outperform baseline methods, which validate the effectiveness of these methods. In addtion, the performance of LSTUR-con is more stable than LSTUR-ini, which indicates that using the concatenation of both short-term and long-term user representations is capable of retaining all the information. We also conducted experiments to explore the performance of combining both LSTUR-con and LSTUR-ini in the same model, but the performance improvement is very limited, implying that each of them can fully capture the long- and short-term user",
    "chunk_order_index": 4,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-6ed1a98565ec22824cdc72d53742fe1a": {
    "tokens": 1200,
    "content": "the effectiveness of these methods. In addtion, the performance of LSTUR-con is more stable than LSTUR-ini, which indicates that using the concatenation of both short-term and long-term user representations is capable of retaining all the information. We also conducted experiments to explore the performance of combining both LSTUR-con and LSTUR-ini in the same model, but the performance improvement is very limited, implying that each of them can fully capture the long- and short-term user interests for news recommendation.  \n\n# 4.3 Effectiveness of Long- and Short-Term User Representation  \n\nIn this section, we conducted several experiments to explore the effectiveness of our approach in learning both long-term and short-term user representations. We compare the performance of our LSTUR methods with the long-term user representation model LTUR and the short-term user representation model STUR. The results are summarized in Fig. 4.  \n\n![](images/image_5.jpg)  \n\n![](images/image_6.jpg)  \nTable 2: The performance of different methods on news recommendation.   \nFigure 4: The effectiveness of incorporating long-tern Figure 5: The comparisons of different methods in user representations (LTUR) and short-term user rep- learning short-term user representations from recently resentations (STUR). browsed news articles.  \n\nFrom the results we find both LTUR and STUR are useful for news recommendation, and the STUR model can outperform the LTUR model. According to the statistics in Table 1, the longterm representations of many users in test data are unavailable, which leads to relative weak performance of LTUR on these users. In addition, combining STUR and LTUR using our two longand short-term user representation methods, i.e., LSTUR-ini and LSTUR-con, can effectively improve the performance. This result validates that incorporating both long-term and short-term user representations is useful to capture the diverse user interests more accurately and is beneficial for news recommendation.  \n\n# 4.4 Effectiveness of News Encoders in STUR  \n\nIn our STUR model, GRU is used to learn shortterm user representations from the recent browsing news. We explore the effectiveness of GRU in encoding news by replacing it with several other encoders, including: 1) Average: using the average of all the news representations in recent browsing history; 2) Attention: the summation of news representations weighted by their attention weights; 3) LSTM (Hochreiter and Schmidhuber, 1997), replacing GRU with LSTM. The results are summarized in Fig. 5.  \n\nAccording to Fig. 5, the sequence-based encoders (e.g., GRU, LSTM) outperform the Average and Attention based encoders. This is probably because the sequence-based encoders can capture the sequential new reading patterns to learn short-term representations of users, which is diffi- cult for Average and Attention based encoders. In addition, GRU achieves better performance than LSTM. This may be because GRU contains fewer parameters and has lower risk of overfitting . Thus, we select GRU as the news encoder in STUR.  \n\n# 4.5 Effectiveness of News Title Encoders  \n\nIn this section, we conduct experiments to compare different news title encoders. In our approach, the news encoder is a combination of CNN network and an attention network (denoted as $\\mathrm{CNN+Att})$ . We compare it with several variants, i.e., CNN, LSTM, and LSTM with attention $(\\mathrm{LSTM}{+}\\mathrm{Att})$ , to validate the effectiveness of our approach. The results are summarized in Fig. 6.  \n\n![](images/image_7.jpg)  \nFigure 6: The comparisons of different methods in learning news title representations and the effectiveness of attention machenism in selecting important words.  \n\n![](images/image_8.jpg)  \nFigure 7: The effectiveness of incorporating news topic and subtopic information for news recommendation.  \n\nAccording to Fig. 6, using attention mechanism in both encoders based on CNN and LSTM can achieve better performance. This is probably because the attention network can select important words, which can learn more informative news representations. In addition, encoders using CNN outperform those using LSTM. This may be because local contexts in news titles are more important for learning news representations.  \n\n# 4.5.1 Effectiveness of News Topic  \n\nIn this section, we conduct experiments to validate the effectiveness of incorporating topic and subtopic of news in the news encoder. We compare the performance of our approach with its variants without topic and/or subtopics. The results are shown in Fig. 7.  \n\nAccording to Fig. 7, incorporating either topics or subtopics can effectively improve the performance of our approach. In addition, the news encoder with subtopics outperforms the news encoder with topics. This is probably because subtopics can provide more fine-grained topic information which is more helpful for news recommendation. Thus, the model with subtopics can achieve better news recommendation performance. Moreover, combining topics and subtopics can further improve the performance of our approach. These results validate the effectiveness of our approach in exploiting topic information for news recommendation.  \n\n# 4.5.2 Influence of Masking Probability  \n\nIn this section, we explore the influence of the probability $p$ in Eq. (6) for randomly masking long-term user representation in model training. We vary the value of $p$ from 0.0 to 0.9 with a step of 0.1 for both LSTUR-ini and LSTUR-con. The results are summarized in Fig. 8.  \n\nAccording to Fig. 8, the results of LSTUR-ini and LSTUR-con have similar patterns. The performance of both methods",
    "chunk_order_index": 5,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-278340890fd0f192f781f9e8a3aa3bbf": {
    "tokens": 1200,
    "content": "probability $p$ in Eq. (6) for randomly masking long-term user representation in model training. We vary the value of $p$ from 0.0 to 0.9 with a step of 0.1 for both LSTUR-ini and LSTUR-con. The results are summarized in Fig. 8.  \n\nAccording to Fig. 8, the results of LSTUR-ini and LSTUR-con have similar patterns. The performance of both methods improves when $p$ increases from 0. When $p$ is too small, the model will tend to overfit on the LTUR, since LTUR has many parameters. Thus, the performance is not optimal. However, when $p$ is too large, the performance of both methods starts to decline. This may be be  \n\n![](images/image_9.jpg)  \nFigure 8: The influence of mask probability $p$ on the performance of our approach.  \n\n2019 CES Highlights $:$ Innovations in Enviro-Sensing for Robocars California dries off after storm batter state for days 15 Recipes Inspired By Vintage Movies Texas State Rep . Dennis Bonnen Elected As House Speaker Should You Buy American Express Stock After Earnings ? How Meghan Markle Has Changed Prince Harry Considerably  \n\nFigure 9: Visualization of the word-level attentions.  \n\ncause the useful information in LTUR cannot be effectively incorporated. Thus, the performance is also not optimal. A moderate choice on $p$ (e.g., 0.5) is most appropriate for both LSTUR-ini and LSTUR-con methods, which can properly balance the learning of LTUR and STUR.  \n\n# 5 Visualization of Attention Weights  \n\nIn this section, we visually explore the effectiveness of the word-level attention network in the news encoder. The attention weights in several example news titles are shown in Fig. 9. From the results, we find our approach can effectively recognize important words to learn more informative news representations. For example, the words “Enviro-Sensing” and “Robocars” in the first news title are assigned high attention weights because these words are indications of news on technologies, while the words “2019” and “for” are assigned low attention weights by our approach since they are less informative. These results validate the effectiveness of the attention network in the news encoder.  \n\n# 6 Conclusion  \n\nIn this paper, we propose a neural news recommendation approach which can learn both longand short-term user representations. The core of our model is a news encoder and a user encoder. In the news encoder, we learn representations of news from their titles and topic categories, and use an attention network to highlight important words for informative representation learning. In the user encoder, we propose to learn long-term representations of users from the embeddings of their IDs. In addition, we learn short-term representations of users from their recently browsed news via a GRU network. Besides, we propose two methods to fuse long- and short-term user representations, i.e., using long-term user representation to initialize the hidden state of the GRU network in short-term user representation, or concatenating both longand short-term user representations as a unified user vector. Extensive experiments on a real-world dataset collected from MSN news show our approach can effecitively improve the performance of news recommendation.  \n\n# Acknowledgement  \n\nThe authors would like to thank Microsoft News for providing technical support and data in the experiments, and Jiun-Hung Chen (Microsoft News) and Ying Qiao (Microsoft News) for their support and discussions. We also want to thank Jianqiang Huang for his help in the experiments.  \n\n# References  \n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.   \nTrapit Bansal, Mrinal Das, and Chiranjib Bhattacharyya. 2015. Content driven user profiling for comment-worthy recommendations of news and blog articles. In RecSys, pages 195–202.   \nDavid M Blei, Andrew $\\textrm{Y N g}$ , and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3(Jan):993–1022.   \nHeng-Tze Cheng, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, Hemal Shah, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, and Wei Chai. 2016. Wide & deep learning for recommender systems. In DLRS, pages 7–10.   \nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In EMNLP, pages 1724–1734.   \nAbhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google news personalization: scalable online collaborative filtering. In WWW, pages 271–280.   \nHuifeng Guo, Ruiming TANG, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A factorization-machine based neural network for CTR prediction. In IJCAI, pages 1725–1731.   \nSepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735",
    "chunk_order_index": 6,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  },
  "chunk-d32cfda1064fc979d2b4d17b3455cf82": {
    "tokens": 993,
    "content": ", pages 271–280.   \nHuifeng Guo, Ruiming TANG, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A factorization-machine based neural network for CTR prediction. In IJCAI, pages 1725–1731.   \nSepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780.   \nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM, pages 2333–2338.   \nYoon Kim. 2014. Convolutional neural networks for sentence classification. In EMNLP, pages 1746– 1751.   \nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.  \nTalia Lavie, Michal Sela, Ilit Oppenheim, Ohad Inbar, and Joachim Meyer. 2010. User attitudes towards news content personalization. International Journal of Human-Computer Studies, 68(8):483–495.   \nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature, 521(7553):436–444.   \nLei Li, Li Zheng, Fan Yang, and Tao Li. 2014. Modeling and broadening temporal user interest in personalized news recommendation. Expert Systems with Applications, 41(7):3168–3177.   \nLihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In WWW, pages 661–670.   \nJianxun Lian, Fuzheng Zhang, Xing Xie, and Guangzhong Sun. 2018. Towards better representation learning for personalized news recommendation: a multi-channel deep fusion approach. In IJCAI, pages 3805–3811.   \nJiahui Liu, Peter Dolan, and Elin Rønby Pedersen. 2010. Personalized news recommendation based on click behavior. In IUI, pages 31–40.   \nShumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. 2017. Embedding-based news recommendation for millions of users. In KDD, pages 1933–1942.   \nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In EMNLP, pages 1532–1543.   \nOwen Phelan, Kevin McCarthy, and Barry Smyth. 2009. Using twitter to recommend real-time topical news. In RecSys, pages 385–388.   \nSteffen Rendle. 2012. Factorization machines with libFM. ACM Transactions on Intelligent Systems and Technology, 3(3):1–22.   \nJeong-Woo Son, A-Yeong Kim, and Seong-Bae Park. 2013. A location-based news article recommendation with explicit localized semantic analysis. In SIGIR, pages 293–302.   \nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958.   \nHongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep knowledge-aware network for news recommendation. In WWW, pages 1835– 1844.   \nChuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019. NPA: Neural news recommendation with personalized attention. In KDD.   \nShuangfei Zhai, Keng hao Chang, Ruofei Zhang, and Zhongfei Mark Zhang. 2016. Deepintent: Learning attentions for online advertising with recurrent neural networks. In KDD, pages 1295–1304.   \nGuanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework for news recommendation. In WWW, pages 167–176.",
    "chunk_order_index": 7,
    "full_doc_id": "doc-6d9f5683f4acd5554764f0858efe37a4"
  }
}