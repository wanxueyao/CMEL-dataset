{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The WIKIHOP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in WIKIHOP consists of a collection of documents, a query and a set of candidate answers (Figure 1). Though there is no guarantee that a question cannot be answered by relying just on a single sentence, the authors ensure that it is answerable using relying only on local information cannot achieve competitive performance.  it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems Figure 1: A sample from WIKIHOP where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-fca1831ae9d052bbbba5616ba543e7fa",
        "description": "The image is a screenshot of a text excerpt from a document discussing the WIKIHOP dataset. The excerpt includes two highlighted sentences in green and a dashed arrow connecting them, indicating a relationship or flow of information between the sentences. The first sentence reads: 'Thorildsplan is a small park in Kristineberg in Stockholm, named in 1925 after the writer [...].' The second sentence reads: 'Stockholm is the capital of Sweden and the most populous city in [...].' Below these sentences, there is a query in red text: 'country Thorildsplan.' Underneath the query, there are candidate answers listed as Denmark, Finland, Sweden, Italy, etc., with the correct answer highlighted in blue as 'Sweden.' The overall layout suggests a question-answering task where the user must determine the correct country based on the given information.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_2.jpg",
        "caption": [
            "Figure 2: Supporting documents (dashed ellipses) organized as a graph where nodes are mentions of either candidate entities or query entities. Nodes with the same color indicates they refer to the same entity (exact match, coreference or both). Nodes are connected by three simple relations: one indicating co-occurrence in the same document (solid edges), another connecting mentions that exactly match (dashed edges), and a third one indicating a coreference (bold-red line). "
        ],
        "footnote": [],
        "context": "To each node $v_{i}$ , we associate a continuous annotation $\\mathbf{x}_{i}\\ \\in\\ \\mathbb{R}^{D}$ which represents an entity in the context where it was mentioned (details in Section 2.3). We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges). Note that MATCH edges when connecting mentions in the same document and create one node per mention. This process is based on the following heuristic: 1. we consider mentions spans in $S_{q}$ exactly matching an element of $C_{q}\\cup\\{s\\}$ . Admittedly, this is a rather simple strategy which may suffer from low recall. 2. we use predictions from a coreference resolution system to add mentions of elements in $C_{q}\\cup\\{s\\}$ beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by Lee et al. (2017). 3. we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-43d01e9d8327207075edce24c3998824",
        "description": "The image is a graph representation of supporting documents organized as nodes and edges. The nodes are mentions of either candidate entities or query entities, with the same color indicating they refer to the same entity (exact match, coreference, or both). There are three types of connections between nodes: solid edges indicate co-occurrence in the same document, dashed edges connect mentions that exactly match, and a bold-red line indicates a coreference. The graph is divided into three clusters enclosed by dashed ellipses, each cluster containing nodes of different colors (white, green, and blue). Within each cluster, nodes are connected by various types of edges. For example, within the top cluster, there is a bold-red line connecting two green nodes, indicating a coreference. The bottom-right cluster contains a mix of white, green, and blue nodes, all interconnected by solid and dashed edges. The overall structure suggests a network of entities and their relationships based on document co-occurrence, exact matches, and coreferences.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 1: WIKIHOP dataset statistics from Welbl et al. (2018): number of candidates and documents per sample and document length. "
        ],
        "context": "In this experiment, we compare our EnitityGCN against recent prior work on the same task. We present test and development results (when present) for both versions of the dataset in Table 2. From Welbl et al. (2018), we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017). We also compare against Coref-GRU (Dhingra et al., 2018), MHPGM (Bauer et al., 2018), and Weaver (Raison et al., 2018). Additionally, we include results of MHQA-GRN (Song et al., 2018), from a recent 3.1 Comparison  documents (see Table 1 for additional dataset statistics). WIKIHOP comes in two versions, a standard (unmasked) one and a masked one. The masked version was created by the authors to test whether methods are able to learn lexical abstraction. In this version, all candidates and all mentions of them in the support documents are replaced by random but consistent placeholder tokens. Thus, in the masked version, mentions are always referred to via unambiguous surface forms. We do not use coreference systems in the masked version as they rely crucially on lexical realization of mentions and cannot operate on masked tokens. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-68c4bae8c1bcb46ee6b8cbe64ec9a784",
        "description": "The image is a table labeled 'Table 1: WIKIHOP dataset statistics from Welbl et al. (2018)' that provides statistical information about the WIKIHOP dataset. The table is structured with four main columns: Min, Max, Avg., and Median. Each row represents different metrics of the dataset and contains the following values: '# candidates' (Min: 2, Max: 79, Avg.: 19.8, Median: 14), '# documents' (Min: 3, Max: 63, Avg.: 13.7, Median: 11), and '# tokens/doc.' (Min: 4, Max: 2,046, Avg.: 100.4, Median: 91). The table highlights the variability in the number of candidates, documents, and document length across samples in the WIKIHOP dataset.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pretrained one (ELMo – without fine-tunning it) and applying R-GCN to reason among entities in the text. \\* with coreference for unmasked dataset and without coreference for the masked one. "
        ],
        "context": "Embedding ablation We argue that ELMo is crucial, since we do not rely on any other context encoder. However, it is interesting to explore how our R-GCN performs without it. Therefore, in this experiment, we replace the deep contextualized embeddings of both the query To help determine the sources of improvements, we perform an ablation study using the publicly available validation set (see Table 3). We perform two groups of ablation, one on the embedding layer, to study the effect of ELMo, and one on the edges, to study how different relations affect the overall model performance. 3.2 Ablation Study  $2\\%$ points. We additionally re-ran BiDAF baseline to compare training time: when using a single Titan X GPU, BiDAF and Entity-GCN process 12.5 and 57.8 document sets per second, respectively. Note that Welbl et al. (2018) had to use BiDAF with very small state dimensionalities (20), and smaller batch size due to the scalability issues (both memory and computation costs). We compare applying the same reductions. Eventually, we also report an ensemble of 5 independently trained models. All models are trained on the same dataset splits with different weight initializations. The ensemble prediction is obtained as $\\arg\\operatorname*{max}_{c}\\prod_{i=1}^{5}P_{i}(c|q,C_{q},S_{q})$ from each model. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-68c4bae8c1bcb46ee6b8cbe64ec9a784",
        "description": "The image is a table labeled 'Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set.' The table is structured with two main columns: Unmasked and Masked, each further divided into Test and Dev (Development) sets. The rows represent various models and their respective accuracy scores. The models listed are Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN, Entity-GCN without coreference (single model), Entity-GCN with coreference (single model), and Entity-GCN* (ensemble 5 models). The accuracy scores for the Unmasked Test set range from 25.7% for FastQA to 74.1% for Human. For the Unmasked Dev set, the scores range from 56.0% for Coref-GRU to 68.5% for Entity-GCN*. For the Masked Test set, the scores range from 35.8% for FastQA to 71.6% for Entity-GCN*. For the Masked Dev set, the scores range from 54.5% for BiDAF to 70.5% for Entity-GCN without coreference (single model). The table highlights the performance of Entity-GCN models, which outperform recent prior work without learning any language model but relying on a pretrained one (ELMo – without fine-tuning it) and applying R-GCN to reason among entities in the text.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_5.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Ablation study on WIKIHOP validation set. The full model is our Entity-GCN with all of its components and other rows indicate models trained without a component of interest. We also report baselines using GloVe instead of ELMo with and without R-GCN. For the full model we report mean $\\pm1$ std over 5 runs. "
        ],
        "context": "Next, we ablate each type of relations independently, that We then inspect our model’s effectiveness in making use of the structure encoded in the graph. We start naively by fully-connecting all nodes within and across documents without distinguishing edges by type (No relation types in Table 3). We observe only marginal improvements with respect to ELMo alone (No R-GCN in Table 3) in both the unmasked and masked setting suggesting that a GCN operating over a naive entity graph would not add much to this task and a more informative graph construction and/or a more sophisticated parameterization is indeed needed.  and processed by the RGCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connecting mentions in the supporting documents (i.e., using only self-loops – No R-GCN in Table 3). The results suggest that WIKIPHOP genuinely requires multihop inference, as our best model is $6.1\\%$ and $8.4\\%$ more accurate than this local model, in unmasked and masked settings, respectively.4 However, it also shows that ELMo representations capture predictive context features, without being explicitly trained for the task. It confirms that our goal of getting away with training expensive document encoders is a realistic one. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-5a1a3958377fab2e9f4d27bf18183d9a",
        "description": "The image is a table labeled 'Table 3: Ablation study on WIKIHOP validation set.' The table compares the performance of different models in two settings: unmasked and masked. The first row shows the full model (ensemble) with a score of 68.5 in the unmasked setting and 71.6 in the masked setting. The second row shows the full model (single) with a mean score of 65.1 ± 0.11 in the unmasked setting and 70.4 ± 0.12 in the masked setting. The subsequent rows show various ablations of the full model, such as GloVe with R-GCN, GloVe without R-GCN, No R-GCN, No relation types, No DOC-BASED, No MATCH, No COREF, No COMPLEMENT, and Induced edges. For example, GloVe with R-GCN scores 59.2 in the unmasked setting and 11.1 in the masked setting, while GloVe without R-GCN scores 51.2 in the unmasked setting and 11.6 in the masked setting. The table highlights the importance of each component in the full model by showing how the removal of each component affects the performance.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_6.jpg",
        "caption": [],
        "footnote": [
            "Table 4: Accuracy and precision at K ( $\\mathrm{P}@\\mathrm{K}$ in the table) analysis overall and per query type. Avg. $|C_{q}|$ indicates the average number of candidates with one standard deviation. "
        ],
        "context": "In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates. We show results in Table 4. We observe that questions regarding places (birth and death) are considered harder for EntityGCN. We then inspect samples where our model fails while assigning highest 4 Error Analysis  al. (2018) used WIKIPEDIA links for masking). Indeed, in the masked version, an entity is always referred to via the same unique surface form (e.g., MASK1) within and across documents. In the unmasked setting, on the other hand, mentions to an entity may differ (e.g., “US” vs “United States”) and they might not be retrieved by the coreference system we are employing, making the task harder for all models. Therefore, as we rely mostly on exact matching when constructing our graph for the masked case, we are more effective in recovering coreference links on the masked rather than unmasked version. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-93e927343e457bf92ee7a74ecfafb2b1",
        "description": "The image is a table labeled 'Table 4: Accuracy and precision at K (P@K in the table) analysis overall and per query type.' The table is structured with the following columns: Relation, Accuracy, P@2, P@5, Avg. |C_q|, and Supports. The rows are divided into two main sections: overall performance and specific query types. For the overall performance, there are two entries: 'overall (ensemble)' and 'overall (single model).' The 'overall (ensemble)' row shows an accuracy of 68.5%, P@2 of 81.0%, P@5 of 94.1%, Avg. |C_q| of 20.4 ± 16.6, and 5129 supports. The 'overall (single model)' row shows an accuracy of 65.3%, P@2 of 79.7%, P@5 of 92.9%, Avg. |C_q| of 20.4 ± 16.6, and 5129 supports. The specific query types section lists the three best and three worst performing relations. The three best performing relations are 'member_of_political_party' with an accuracy of 85.5%, P@2 of 95.7%, P@5 of 98.6%, Avg. |C_q| of 5.4 ± 2.4, and 70 supports; 'record_label' with an accuracy of 83.0%, P@2 of 93.6%, P@5 of 99.3%, Avg. |C_q| of 12.4 ± 6.1, and 283 supports; and 'publisher' with an accuracy of 81.5%, P@2 of 96.3%, P@5 of 100.0%, Avg. |C_q| of 9.6 ± 5.1, and 54 supports. The three worst performing relations are 'place_of_birth' with an accuracy of 51.0%, P@2 of 67.2%, P@5 of 86.8%, Avg. |C_q| of 27.2 ± 14.5, and 309 supports; 'place_of_death' with an accuracy of 50.0%, P@2 of 67.3%, P@5 of 89.1%, Avg. |C_q| of 25.1 ± 14.3, and 159 supports; and 'inception' with an accuracy of 29.9%, P@2 of 53.2%, P@5 of 83.1%, Avg. |C_q| of 21.9 ± 11.0, and 77 supports.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_7.jpg",
        "caption": [
            "(a) Candidates set size $\\mathbf{\\dot{\\rho}}_{\\mathbf{X}}$ -axis) and accuracy ( $\\mathrm{~y~}$ -axis). Pearson’s correlation of $-0.687$ $(p<10^{-7})$ ."
        ],
        "footnote": [],
        "context": "Table 6: Samples from WIKIHOP set where Entity-GCN fails. $p$ indicates the predicted likelihood.   Figure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the top) and nodes set size (on the bottom) on the validation set. Rescaled data distributions (orange) per number of candidate $(t o p)$ and nodes (bottom). Dashed lines indicate average accuracy.  accuracy. We report the best results of each experiment based on accuracy on validation set. BError Analysis In Table 6, we report three samples from WIKIHOP development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. CAblation Study In Figure 3, we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of candidate answers or the number of nodes increases. ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-58bed125e330c1a287e0baef935f2136",
        "description": "The image is a bar chart with two sets of data. The x-axis represents the candidate set size, ranging from 0 to 65. The y-axis represents accuracy, ranging from 0 to 1.0. The blue bars represent the accuracy of the best single model with respect to the candidate set size on the validation set. The orange bars represent the rescaled data distributions per number of candidates. A dashed black line at approximately 0.687 indicates the average accuracy. The Pearson’s correlation coefficient between the candidate set size and accuracy is -0.687 (p<10^-7), indicating a significant negative correlation.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_8.jpg",
        "caption": [
            "(b) Nodes set size $\\bf\\Tilde{x}$ -axis) and accuracy (y-axis). Pearson’s correlation of $-0.385$ $(p<10^{-7})$ ). "
        ],
        "footnote": [],
        "context": "Table 6: Samples from WIKIHOP set where Entity-GCN fails. $p$ indicates the predicted likelihood.   Figure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the top) and nodes set size (on the bottom) on the validation set. Rescaled data distributions (orange) per number of candidate $(t o p)$ and nodes (bottom). Dashed lines indicate average accuracy. accuracy. We report the best results of each experiment based on accuracy on validation set. BError Analysis In Table 6, we report three samples from WIKIHOP development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases. CAblation Study In Figure 3, we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of candidate answers or the number of nodes increases.  ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-58bed125e330c1a287e0baef935f2136",
        "description": "The image is a graph that plots the accuracy of a model against the nodes set size. The x-axis represents the nodes set size, ranging from 0 to 200. The y-axis represents the accuracy, ranging from 0 to 1.0. The blue bars represent the accuracy values for different nodes set sizes. The orange bars represent the rescaled data distributions per number of nodes. A dashed black line at approximately 0.65 on the y-axis indicates the average accuracy. The accuracy values fluctuate around the average line, with some values above and some below. The rescaled data distribution bars show a decreasing trend as the nodes set size increases, starting high and gradually decreasing to almost zero at the higher end of the nodes set size.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_9.jpg",
        "caption": [
            "Table 5: Model architecture. "
        ],
        "footnote": [],
        "context": "Table 6: Samples from WIKIHOP set where Entity-GCN fails. $p$ indicates the predicted likelihood.  where is not. We commented these samples explaining why our model might fail in these cases. CAblation Study In Figure 3, we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of candidate answers or the number of nodes increases.   Figure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the top) and nodes set size (on the bottom) on the validation set. Rescaled data distributions (orange) per number of candidate $(t o p)$ and nodes (bottom). Dashed lines indicate average accuracy. ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-58bed125e330c1a287e0baef935f2136",
        "description": "The image is a table labeled 'Table 5: Model architecture' that outlines the architecture of a machine learning model. The table is structured with multiple rows and columns, detailing the components of the model. The first row specifies the input as 'query ELMo 3072-dim' and 'candidates ELMo 3072-dim'. The second row describes the layers used in the model: '2 layers bi-LSTM [256, 128]-dim' for the query and '1 layer FF 256-dim' for the candidates. The third row indicates a concatenation step resulting in a 512-dimensional vector. The fourth row shows a two-layer feed-forward (FF) network with dimensions [1024, 512]-dim, producing outputs denoted as '{x̂}_i^N'. The fifth row lists three layers of R-GCN, each with 512 dimensions and shared parameters. The sixth row mentions a concatenation with q resulting in a 768-dimensional vector. The seventh row details a three-layer feed-forward network with dimensions [256, 128, 1]-dim. The final row specifies the output as 'probabilities over C_q'.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1240/images/image_10.jpg",
        "caption": [],
        "footnote": [
            "(c) In this sample, there is ambiguity between two entities since the city Eslo¨v is located in the Scania County (English name of Ska˚ne County). The model assigning high probability to the city and it cannot select the county. "
        ],
        "context": "Table 6: Samples from WIKIHOP set where Entity-GCN fails. $p$ indicates the predicted likelihood. where is not. We commented these samples explaining why our model might fail in these cases. CAblation Study In Figure 3, we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of candidate answers or the number of nodes increases.   Figure 3: Accuracy (blue) of our best single model with respect to the candidate set size (on the top) and nodes set size (on the bottom) on the validation set. Rescaled data distributions (orange) per number of candidate $(t o p)$ and nodes (bottom). Dashed lines indicate average accuracy.  ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-58bed125e330c1a287e0baef935f2136",
        "description": "The image is a table labeled 'Table 6: Samples from WIKIHOP set where Entity-GCN fails.' The table is structured with multiple rows and columns. Each row represents a sample ID, query, gold answer, predicted answer, and their respective probabilities. The columns are as follows: 'ID', 'Query', 'Gold answer', 'Predicted answer', and the probability values for both the gold and predicted answers. \\n\\nFor example, the first row has the following details: \\n- ID: WH_dev_2257 \\n- Query: inception (of) Derrty Entertainment \\n- Gold answer: 2003 (p = 14.1) \\n- Predicted answer: 2000 (p = 15.8) \\n\\nSupport texts are provided for each sample to explain the context of the query and the answers. For instance, Support 1 for the first sample states that Derrty Entertainment is a record label founded by [...], and the first album released under Derrty Entertainment was Nelly's Country Grammar. Support 2 mentions that Country Grammar is the debut single by American rapper Nelly, produced by Jason Epperson, and released in 2000. \\n\\nThe footnote explains that there is ambiguity between two entities since the city Eslov is located in the Scania County (English name of Skane County). The model assigns high probability to the city and cannot select the county. \\n\\nThe context discusses the performance of the Entity-GCN model and its failure cases in the WIKIHOP dataset.",
        "segmentation": false
    }
}