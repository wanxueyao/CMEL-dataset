{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/C18-1182/images/image_1.jpg",
        "caption": [
            "Figure 1: Word level NN architecture for NER "
        ],
        "footnote": [],
        "context": "In DrugNER (Segura Bedmar et al., 2013), Liu et al. (2015) achieved state-of-the-art results by using a CRF with features like lexicon resources from Food and Drug Administration (FDA), DrugBank, Jochem Agerri and Rigau (2016) developed a semi-supervised system by presenting NER classifiers with features including orthography, character n-grams, lexicons, prefixes, suffixes, bigrams, trigrams, and unsupervised cluster features from the Brown corpus, Clark corpus and $\\mathbf{k}$ -means clustering of open text using word embeddings (Mikolov et al., 2013). They achieved near state of the art performance on CoNLL datasets: $84.16\\%$ ,$85.04\\%$ ,$91.36\\%$ ,$76.42\\%$ on Spanish, Dutch, English, and German, respectively.  3 words to the left and to the right of the central word, and tags of the last 3 words as features to the SVM. The final tag was decided by the voting of multiple one-vs-one SVM outputs. Ando and Zhang (2005a) implemented structural learning (Ando and Zhang, 2005b) to divide the main task into many auxiliary tasks, for example, predicting labels by looking just at the context and masking the current word. The best classifier for each auxiliary task was selected based on its confidence. This model had achieved $89.31\\%$ and $75.27\\%$ F score on English and German, respectively. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-22376ad0f4b05f5f20412a982321c613",
        "description": "The image is a diagram illustrating the word-level neural network architecture for Named Entity Recognition (NER). The diagram is structured into five layers: Label, Word Representation, Word LSTM-B, Word LSTM-F, and Word Embedding. Each layer is represented by a series of boxes connected by arrows, indicating the flow of information through the network. The 'Label' layer at the top shows the tags assigned to each word, such as B-ORG, I-ORG, O, B-PER, and I-PER. The 'Word Representation' layer below it contains boxes representing the input words. The 'Word LSTM-B' and 'Word LSTM-F' layers are bidirectional Long Short-Term Memory networks that process the word representations in both forward and backward directions. The 'Word Embedding' layer at the bottom represents the initial word embeddings. The words 'Best', 'Buy', 's', 'CEO', 'Hubert', and 'Joly' are shown at the bottom, with their corresponding labels above them.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/C18-1182/images/image_2.jpg",
        "caption": [
            "Figure 2: Character level NN architecture for NER "
        ],
        "footnote": [],
        "context": "In this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2). Character labels transformed into word labels via post processing. The potential of character NER neural models 6.4.2 Character level architectures With slight variations, Yan et al. (2016) implemented word level feed forward NN, bi-directional LSTM (bi-LSTM) and window bi-LSTM for NER of English, German and Arabic. They also highlighted the performance improvement after adding various features like CRF, case, POS, word embeddings and achieved $88.91\\%$ F1 score on English and $76.12\\%$ on German. embeddings. Huang et al. (2015) presented a word LSTM model (Figure 1) and showed that adding a CRF layer to the top of the word LSTM improved performance, achieving $84.26\\%$ F1 score on English CoNLL 2003 dataset. Similar systems were applied to other domains: DrugNER by Chalapathy et al. (2016) achieving $85.19\\%$ F1 score (under an unofficial evaluation) on MedLine test data (Segura Bedmar et al., 2013), and medical NER by $\\mathrm{Xu}$ et al. (2017) achieving $80.22\\%$ F1 on disease NER corpus using this architecture. In similar tasks, Plank et al. (2016) implemented the same model for multilingual POS tagging. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-ae2df4ba0c6d1016722d83ce8d5a38ab",
        "description": "The image is a diagram illustrating the character-level neural network architecture for Named Entity Recognition (NER). The diagram is structured in four rows. The top row, labeled 'Label', shows a sequence of labels: B-ORG repeated eight times followed by two O labels. The second row, labeled 'Char Representation', displays a series of boxes with arrows pointing upwards, indicating the representation of characters. The third row, labeled 'Char LSTM-B', shows a sequence of blue boxes connected by arrows, representing the forward pass of the Bi-directional Long Short-Term Memory (Bi-LSTM) network. The fourth row, labeled 'Char LSTM-F', mirrors the third row but represents the backward pass of the Bi-LSTM network. The bottom row, labeled 'Characters', shows the actual characters corresponding to the labels: 'B', 'e', 's', 't', '_', 'B', 'u', 'y', ',', and 's'. Each character is represented by a red box. The diagram visually explains how each character is processed through the Bi-LSTM layers to predict the label for each character.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/C18-1182/images/image_3.jpg",
        "caption": [
            "Label Word CRF Word LSTM-B Word LSTM-F Word Representation Word Features Char LSTM-B Char LSTM-F Char Embedding Characters ",
            "Figure 3: Word $^+$ character level NN architecture for NER "
        ],
        "footnote": [],
        "context": "Limsopatham and Collier (2016) implemented a character+word level NER model for Twitter NER (Baldwin et al., 2015) by concatenating a CNN over characters, a CNN over orthographic features of characters, a word embedding, and a word orthographic feature embedding. This concatenated representation is passed through another Bi-LSTM layer and the output is given to CRF for predicting. This model achieved $65.89\\%$ F score on segmentation alone and $52.41\\%$ F score This model has also been utilized for NER in languages like Japanese where Misawa et al. (2017) showed that this architecture outperformed other neural architectures on the organization entity class.  this model to achieve $91.21\\%$ F1 score on the CoNLL 2003 English dataset and $97.55\\%$ POS-tagging accuracy on the WSJ portion of PTB (Marcus et al., 1993). They also showed lower performance by this model for out of vocabulary words. Chiu and Nichols (2015) achieved $91.62\\%$ F1 score on the CoNLL 2003 English dataset and $86.28\\%$ F score on Onto notes 5.0 dataset (Pradhan et al., 2013) by adding lexicons and capitalization features to this model. Lexicon feature were encoded in the form or B(begin), I(inside) or E(end) PER, LOC, ORG and MISC depending upon the match from the dictionary. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-7fb7bd65b722433093ae9f1c85b9fe4e",
        "description": "The image is a detailed diagram illustrating a word + character level neural network architecture for Named Entity Recognition (NER). The diagram is divided into three main sections labeled B-ORG, I-ORG, and O. Each section contains a sequence of nodes representing different components of the model. The nodes are color-coded: purple nodes represent word-level features, blue nodes represent character-level features, and red nodes represent label predictions. The arrows between the nodes indicate the flow of information through the network. The B-ORG section starts with a node labeled 'Best' followed by a sequence of nodes representing individual characters ('B', 'e', 's', 't'). The I-ORG section starts with a node labeled 'Buy' followed by a sequence of nodes representing individual characters ('B', 'u', 'y'). The O section represents other entities and follows a similar structure. The diagram also includes connections between the word-level and character-level features, as well as connections to the label prediction nodes. The overall architecture shows how the model processes both word and character-level information to predict entity labels.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/C18-1182/images/image_4.jpg",
        "caption": [
            "Label Word CRF Word LSTM-B Word LSTM-F Word Representation Word Features Char LSTM-B Char LSTM-F Char Embedding Characters ",
            "Figure 4: Word+character+affix level NN architecture for NER "
        ],
        "footnote": [],
        "context": "Yadav et al. (2018) implemented a model that augments the character+word NN architecture with one of the most successful features from feature-engineering approaches: affixes. Affix features were used in early NER systems for CoNLL 2002 (Tjong Kim Sang, 2002; Cucerzan and Yarowsky, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003) and for biomedical NER (Saha et al., 2009), but had not been used in neural NER systems. They extended the Lample et al. (2016) character+word model to learn affix embeddings alongside the word embeddings and character RNNs (Figure 4). They considered 6.4.4 Character $^+$ Word $^+$ affix model  ( $85.81\\%$ F score) on Spanish CoNLL 2002 dataset. A slightly improved system focusing on multi-task and multi-lingual joint learning was proposed by Yang et al. (2016) where word representation given by GRU (Gated Recurrent Unit) cell over characters plus word embedding was passed through another RNN layer and the output was given to CRF models trained for different tasks like POS, chunking and NER. Yang et al. (2017) further proposed transfer learning for multi-task and multi-learning, and showed small improvements on CoNLL 2002 and 2003 NER data, achieving $85.77\\%$ ,$85.19\\%$ ,$91.26\\%$ F scores on Spanish, Dutch and English, respectively. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-7fb7bd65b722433093ae9f1c85b9fe4e",
        "description": "The image is a diagram illustrating the architecture of a neural network model for Named Entity Recognition (NER). The diagram is divided into three main sections labeled B-ORG, I-ORG, and O. Each section contains a sequence of colored boxes representing different components of the model. The boxes are connected by arrows indicating the flow of information through the network. The B-ORG section has a sequence of blue boxes with labels 'Bes', 'est', 'Best'. The I-ORG section has a sequence of blue boxes with labels 'Buy', 'Buy', 'Buy'. The O section has a sequence of blue boxes with labels '\\u03A6', '\\u03A6', 's'. Each box in the sequence is connected to a larger box above it, which represents the output of the model. The larger boxes are connected by arrows to other components of the model, including LSTM layers and CRF layers. The diagram also includes red boxes at the bottom, which represent the input tokens. The overall structure of the model is designed to process sequences of words and characters to identify named entities.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/C18-1182/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Our final finding is that there is still interesting progress to be made by incorporating key features of past feature-engineered models into modern NN architectures. Yadav et al. (2018)’s simple extension of Lample et al. (2016) to incorporate affix features yields a very strong new model, achieving a new state-of-the-art al. (2016), is $2.64\\%$ better than the best word-based model, Yan et al. (2016), and $2.54\\%$ better than the best character-based model, (Kuru et al., 2016). In DrugNER, the word+character hybrid model is better than the word model by $14.25\\%$ on MedLine test data and $1.81\\%$ on DrugBank test data.   both word-based and character-based models. For example, the best hybrid NN model for English, Chiu and Nichols (2015), is $0.52\\%$ better than the best word-based model, Huang et al. (2015), and $5.12\\%$ better than the best character-based model, (Kuru et al., 2016). Similarly, the best hybrid NN model for German, Lample et Table 2: DrugNER results on the MedLine and DrugBank test data $(80.10\\%$ and $19.90\\%$ of the test data, respectively). The Yadav et al. (2018) experiments report no decimal places because they were run after the end of shared task, and the official evaluation script outputs no decimal places. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-cf36c8fd2948cda996a7b1897c0e732d",
        "description": "The image is a table that compares the performance of various machine learning and neural network models for named entity recognition tasks. The table is divided into four main sections: Feature-engineered machine learning systems, Feature-inferring neural network word models, Feature-inferring neural network character models, and Feature-inferring neural network word + character models. Each section lists different models along with their respective performance metrics on different datasets (Dict, SP, DU, EN, GE). For example, in the Feature-engineered machine learning systems section, Carreras et al. (2002) binary AdaBoost classifiers achieve 81.39% on Dict and 77.05% on SP. In the Feature-inferring neural network word models section, Collobert et al. (2011) Vanilla NN +SLL / Conv-CRF achieves 81.47% on EN. The table highlights the best-performing models in bold, such as Agerri and Rigau (2016) achieving 84.16% on Dict, 85.04% on SP, 91.36% on EN, and 76.42% on GE. The table also includes a final section for Feature-inferring neural network word + character + affix models, where Yadav et al. (2018) report results with 100 and 150 Epochs.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/C18-1182/images/image_6.jpg",
        "caption": [
            "Table 1: Comparison of NER systemsin four languages: CoNLL 2002 Spanish (SP), CoNLL 2002 Dutch (DU), CoNLL 2003 English (EN), and CoNLL 2003 German (GE). Dict indicates whether or not the approach makes use of dictionary lookups. Best performance in each category is highlighted in bold. "
        ],
        "footnote": [],
        "context": "Our final finding is that there is still interesting progress to be made by incorporating key features of past feature-engineered models into modern NN architectures. Yadav et al. (2018)’s simple extension of Lample et al. (2016) to incorporate affix features yields a very strong new model, achieving a new state-of-the-art al. (2016), is $2.64\\%$ better than the best word-based model, Yan et al. (2016), and $2.54\\%$ better than the best character-based model, (Kuru et al., 2016). In DrugNER, the word+character hybrid model is better than the word model by $14.25\\%$ on MedLine test data and $1.81\\%$ on DrugBank test data.  both word-based and character-based models. For example, the best hybrid NN model for English, Chiu and Nichols (2015), is $0.52\\%$ better than the best word-based model, Huang et al. (2015), and $5.12\\%$ better than the best character-based model, (Kuru et al., 2016). Similarly, the best hybrid NN model for German, Lample et Table 2: DrugNER results on the MedLine and DrugBank test data $(80.10\\%$ and $19.90\\%$ of the test data, respectively). The Yadav et al. (2018) experiments report no decimal places because they were run after the end of shared task, and the official evaluation script outputs no decimal places.  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-cf36c8fd2948cda996a7b1897c0e732d",
        "description": "The image is a table labeled 'Table 1: Comparison of NER systems in four languages: CoNLL 2002 Spanish (SP), CoNLL 2002 Dutch (DU), CoNLL 2003 English (EN), and CoNLL 2003 German (GE)'. The table is structured with multiple columns and rows. The columns are labeled as follows: 'Dict', 'MedLine (80.10%)', 'DrugBank (19.90%)', and 'Complete dataset'. Each of these main columns is further divided into sub-columns labeled 'P' (Precision), 'R' (Recall), and 'F1'. The rows represent different NER systems and their performance metrics. The first row under 'Feature-engineered machine learning systems' shows the results for Rocktäschel et al. (2013), with 'Yes' indicating the use of dictionary lookups. The precision, recall, and F1 scores for MedLine are 60.70, 55.80, and 58.10 respectively; for DrugBank, they are 88.10, 87.50, and 87.80; and for the complete dataset, they are 73.40, 69.80, and 71.50. The next three rows show the results for Liu et al. (2015) with different configurations: baseline, MED. emb., and state of the art. The last two rows under 'NN word model' and 'NN word + character model' show the results for Chalapathy et al. (2016) and Yadav et al. (2018). The best performance in each category is highlighted in bold. For example, the highest F1 score for MedLine is 69 for the NN word + character + affix model by Yadav et al. (2018).",
        "segmentation": false
    }
}