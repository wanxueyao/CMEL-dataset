{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_1.jpg",
        "caption": [
            "a) Sample ConceptNet for specific subgraphs "
        ],
        "footnote": [],
        "context": "b) Crowd source corresponding natural language questions and two additional distractors Where on a river can When humans answer questions, they capitalize on their common sense and background knowledge about spatial relations, causes and effects, scientific facts and social conventions. For instance, given the question “Where was Simon when he heard the lawn mower?”, one can infer that the lawn mower is close to Simon, and that it is probably outdoors and situated at street level. This type of knowledge seems trivial for humans, but is still out of the reach of current natural language understanding (NLU) systems. 1 Introduction from CONCEPTNET (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains $56\\%$ accuracy, well below human performance, which is $89\\%$ . ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-d14992c2a3231645807ccbf44cdcdad5",
        "description": "The image is a concept map illustrating the spatial relationships of various elements associated with a river. The central node, labeled 'river', is connected to other nodes through directed edges labeled 'AtLocation'. The nodes connected to the 'river' node are 'pebble', 'stream', 'bank', and 'canyon' on one side, and 'waterfall', 'bridge', and 'valley' on the other side. The connections from 'river' to 'pebble', 'stream', 'bank', and 'canyon' are depicted with red arrows, while the connections to 'waterfall', 'bridge', and 'valley' are shown with blue dashed arrows. This diagram represents a specific subgraph from ConceptNet, indicating the locations where these elements can be found relative to a river.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_2.jpg",
        "caption": [
            "Figure 2: COMMONSENSEQA generation process. The input is CONCEPTNET knowledge base, and the output is a set of multiple-choice questions with corresponding relevant context (snippets). "
        ],
        "footnote": [],
        "context": "Extraction from CONCEPTNET CONCEPTNET is a graph knowledge-base $G\\subseteq\\mathcal{C}\\times\\mathcal{R}\\times\\mathcal{C}$ ,where the nodes $\\mathcal{C}$ represent natural language concepts, and edges $\\mathcal{R}$ represent commonsense relations. Triplets $\\left(c_{1},r,c_{2}\\right)$ carry commonsense knowledge such The entire data generation process is summarized in Figure 2. We now elaborate on each of the steps: each with one source concept and three target concepts. 2. We ask crowdsourcing workers to author three questions per subgraph (one per target concept), to add two additional distractors per question, and to verify questions’ quality. 3. We add textual context to each question by querying a search engine and retrieving web snippets. however, they also need specialized scientific knowledge. In contrast to these efforts, our work studies common sense without requiring additional information. SQUABU created a small handcurated test of common sense and science questions (Davis, 2016), which are difficult for current techniques to solve. In this work, we create similarly well-crafted questions but at a larger scale. 3 Dataset Generation Our goal is to develop a method for generating questions that can be easily answered by humans without context, and require commonsense knowledge. We generate multiple-choice questions in a process that comprises the following steps. 1. We extract subgraphs from CONCEPTNET, ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-cb4af706ef03830180bcf9f857a43b1a",
        "description": "The image is a flowchart illustrating the generation process of multiple-choice questions for COMMONSENSEQA using the CONCEPTNET knowledge base. The process is divided into several steps. Initially, subgraphs are extracted from CONCEPTNET, which is represented by a graph with nodes and edges. The nodes are colored in red, green, and blue, representing different concepts, while the edges are depicted as lines connecting these nodes. Next, crowdworkers are involved to author questions based on these subgraphs. For example, one question asks about dust in a house, with options including attic, yard, street, bed, and desert. Another question inquires about finding glass outside, with options like bar, fork, car, sand, and wine. A third question explores what makes someone happy, with options such as laugh, sad, fall, blue, and feel. Crowdworkers also add distractors to each question. Following this, the questions are filtered by quality, with some being discarded due to low quality. Finally, relevant snippets are collected via a search engine to provide context for each question. The flowchart uses arrows to indicate the sequence of these steps, and the overall process is designed to generate high-quality, commonsense-based multiple-choice questions.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 1: Key statistics for COMMONSENSEQA "
        ],
        "context": "Adding textual context To examine whether web text is useful for answering commonsense questions, we add textual information to each question in the following way: We issue a web query to Google search for every question and candidate answer, concatenating the answer to the question, Verifying questions quality We train a disjoint group of workers to verify the generated questions. Verifiers annotate a question as unanswerable, or choose the right answer. Each question is verified by 2 workers, and only questions verified by at least one worker that answered correctly are used. This processes filters out $15\\%$ of the questions.  distractors To make the task more difficult, we ask crowd-workers to add two additional incorrect answers to each formulated question. One distractor is selected from a set of answer concepts with the same relation to the question concept in CONCEPTNET (Figure 1, in red). The second distractor is formulated manually by the workers themselves (Figure 1, in purple). Workers were encouraged to formulate a distractor that would seem plausible or related to the question but easy for humans to dismiss as incorrect. In total, each formulated question is accompanied with five candidate answers, including one correct answer and four distractors. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-cb4af706ef03830180bcf9f857a43b1a",
        "description": "The image is a table labeled 'Measurement' that provides key statistics for COMMONSENSEQA. The table is structured with two main columns: Measurement and Value. Each row represents a specific measurement and its corresponding value. The measurements and their values are as follows: '# CONCEPTNET distinct question nodes' (2,254), '# CONCEPTNET distinct answer nodes' (12,094), '# CONCEPTNET distinct nodes' (12,107), '# CONCEPTNET distinct relation labels' (22), 'average question length (tokens)' (13.41), 'long questions (more than 20 tokens)' (10.3%), 'average answer length (tokens)' (1.5), '# answers with more than 1 token' (44%), '# of distinct words in questions' (14,754), and '# of distinct words in answers' (4,911). The table highlights the dataset's significant size and the detailed annotations across its splits.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Top CONCEPTNET relations in COMMONSENSEQA, along with their frequency in the data and an example question. The first answer (A) is the correct answer "
        ],
        "context": "Commonsense Skills To We analyzed the distribution of first and second words in the formulated questions along with example questions. Figure 4 presents the breakdown. Interestingly, only $44\\%$ of the first words are WHwords. In about $5\\%$ of the questions, formulators used first names to create a context story, and in $7\\%$ they used the word ${^\\ast}i f^{\\ast}$ to present a hypothetical question. This suggests high variability in the question language. Question formulation Question formulators were instructed to create questions with high language variation. 122 formulators contributed to question generation. However, 10 workers formulated more than $85\\%$ of the questions.   dog, house, or row boat, connected by relations such as Causes, CapableOf, or Antonym. The top-5 question concepts in COMMONSENSEQA are ‘Person’ $(3.1\\%)$ , ‘People’ $(2.0\\%)$ , ‘Human’ $(0.7\\%)$ , ‘Water’ $(0.5\\%)$ and ‘Cat’ $(0.5\\%)$ . In addition, we present the main relations along with the percentage of questions generated from them in Table 2. It’s worth noting that since question formulators were not shown the CONCEPTNET relation, they often asked questions that probe other relationships between the concepts. For example, the question “What do audiences clap for?” was generated from the AtLocation relation, but focuses on social conventions instead. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-301b78a85599c18fac1069528e3a5e54",
        "description": "The image is a table labeled 'Table 2: Top CONCEPTNET relations in COMMONSENSEQA, along with their frequency in the data and an example question.' The table is structured with three main columns: Relation, Formulated question example, and %. Each row represents a different relation and its corresponding formulated question example and percentage. The relations listed are AtLocation (47.3%), Causes (17.3%), CapableOf (9.4%), Antonym (8.5%), HasSubevent (3.6%), HasPrerequisite (3.3%), CausesDesire (2.1%), Desires (1.7%), PartOf (1.6%), and HasProperty (1.2%). The example questions for each relation are provided, with the first answer (A) being the correct answer.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_5.jpg",
        "caption": [
            "Figure 3: Examples of manually-annotated questions, with the required skills needed to arrive at the answers (red circles). Skills are labeled edges, and concepts are nodes. "
        ],
        "footnote": [],
        "context": "Commonsense Skills To We analyzed the distribution of first and second words in the formulated questions along with example questions. Figure 4 presents the breakdown. Interestingly, only $44\\%$ of the first words are WHwords. In about $5\\%$ of the questions, formulators used first names to create a context story, and in $7\\%$ they used the word ${^\\ast}i f^{\\ast}$ to present a hypothetical question. This suggests high variability in the question language. Question formulation Question formulators were instructed to create questions with high language variation. 122 formulators contributed to question generation. However, 10 workers formulated more than $85\\%$ of the questions.  dog, house, or row boat, connected by relations such as Causes, CapableOf, or Antonym. The top-5 question concepts in COMMONSENSEQA are ‘Person’ $(3.1\\%)$ , ‘People’ $(2.0\\%)$ , ‘Human’ $(0.7\\%)$ , ‘Water’ $(0.5\\%)$ and ‘Cat’ $(0.5\\%)$ . In addition, we present the main relations along with the percentage of questions generated from them in Table 2. It’s worth noting that since question formulators were not shown the CONCEPTNET relation, they often asked questions that probe other relationships between the concepts. For example, the question “What do audiences clap for?” was generated from the AtLocation relation, but focuses on social conventions instead.  ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-301b78a85599c18fac1069528e3a5e54",
        "description": "The image is a diagram consisting of three separate flowcharts, each addressing a different question. The first flowchart asks, 'Where are Rosebushes typically found outside of large buildings?' It shows a sequence of nodes connected by labeled edges. The nodes are: Building, Courtyard, Flowers, and Rosebushes. The edges are labeled as 'Has parts', 'Spatial', and 'Is member of'. The second flowchart asks, 'Where would you get a Balalaika if you do not have one?' It has nodes for Balalaika, Instrument, Music store, and Get instruments. The edges are labeled 'Is member of', 'Spatial', and 'Purpose'. The third flowchart asks, 'I want to use string to keep something from moving, how should I do it?' It includes nodes for Something, String, Tie around, and Keep from moving. The edges are labeled 'Spatial', 'Activity', and 'Cause & effect'. Each node is represented by a circle, and the edges are depicted as lines connecting the circles with labels indicating the relationship between the concepts.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_6.jpg",
        "caption": [
            "Table 3: Skills and their frequency in the sampled data. As each example can be annotated with multiple skills, the total frequency does not sum to $100\\%$ ."
        ],
        "footnote": [],
        "context": "For each question, we explicitly annotated the types of commonsense skills that a human uses to answer the question. We allow multiple commonsense skills per questions, with an average of 1.75 skills per question. Figure 3 provides three example annotations. Each annotation contains a node for the answer concept, and other nodes for concepts that appear in the question or latent concepts. Labeled edges describe the commonsense skill that relates the two nodes. We defined commonsense skills based on the analysis of LoBue and Yates (2011), with slight modifications to accommodate the phenomena in our data. Table 3 presents the  the questions. We analyzed the distribution of first and second words in the formulated questions along with example questions. Figure 4 presents the breakdown. Interestingly, only $44\\%$ of the first words are WHwords. In about $5\\%$ of the questions, formulators used first names to create a context story, and in $7\\%$ they used the word ${^\\ast}i f^{\\ast}$ to present a hypothetical question. This suggests high variability in the question language. Commonsense Skills To analyze the types of commonsense knowledge needed to correctly answer questions in COMMONSENSEQA, we randomly sampled 100 examples from the development set and performed the following analysis. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-301b78a85599c18fac1069528e3a5e54",
        "description": "The image is a table labeled 'Table 3: Skills and their frequency in the sampled data.' The table categorizes different types of commonsense skills and their respective frequencies. The categories listed are: Spatial (41%), Cause & Effect (23%), Has parts (23%), Is member of (17%), Purpose (18%), Social (15%), Activity (8%), Definition (6%), and Preconditions (3%). Each row represents a category of commonsense skill, with the first column listing the category name and the second column providing the definition of the skill. The third column shows the percentage frequency of each skill in the sampled data. For example, 'Spatial' is defined as 'Concept A appears near Concept B' and occurs 41% of the time, while 'Preconditions' is defined as 'Concept A must hold true in order for Concept B to take place' and occurs 3% of the time.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_7.jpg",
        "caption": [
            "Figure 4: Distribution of the first and second words in questions. The inner part displays words and their frequency and the outer part provides example questions. "
        ],
        "footnote": [],
        "context": "b LM1B Inspired by Trinh and Le (2018), we employ a large language model (LM) from Jozefowicz et al. (2016), which was pre-trained on the One Billion Words Benchmark (Chelba et al., 2013). We use this model in two variations. In the first (LM1B-CONCAT), we simply concatenate each answer to the question. In the second (LM1B-REP), a VECSIM A model that chooses the answer with highest cosine similarity to the question, where the question and answers are represented by an average of pre-trained word embeddings. and (b) whether context (web snippets) is used. We now elaborate on the different baselines.   their definition and their frequency in the analyzed examples. 5 Baseline Models Our goal is to collect a dataset of commonsense questions that are easy for humans, but hard for current NLU models. To evaluate this, we experiment with multiple baselines. Table 4 summarizes the various baseline types and characterizes them based on (a) whether training is done on COMMONSENSEQA or the model is fully pre-trained, Table 4: Baseline models along with their characteristics. Training states whether the model was trained on COMMONSENSEQA, or was only trained a different dataset. Context states whether the model uses extra context as input. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-301b78a85599c18fac1069528e3a5e54",
        "description": "The image is a detailed pie chart titled 'Distribution of the first and second words in questions.' The chart is divided into multiple segments, each representing different categories of questions. The inner part of the chart displays words and their frequency, while the outer part provides example questions. The segments are color-coded and labeled with percentages indicating the frequency of each category. For instance, 'What' constitutes 21% of the questions, 'Where' accounts for 18%, 'When' makes up 7%, 'If' represents 7%, 'Why' comprises 2%, 'The' takes up 13%, 'James' and 'John' each contribute 2%, and 'Other' covers 37%. Each segment also includes an example question related to the category. For example, under 'What,' an example question is 'What could 1.5% have what?'. The chart provides a comprehensive overview of the distribution and examples of the first and second words in questions.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_8.jpg",
        "caption": [],
        "footnote": [],
        "context": "b LM1B Inspired by Trinh and Le (2018), we employ a large language model (LM) from Jozefowicz et al. (2016), which was pre-trained on the One Billion Words Benchmark (Chelba et al., 2013). We use this model in two variations. In the first (LM1B-CONCAT), we simply concatenate each answer to the question. In the second (LM1B-REP), a VECSIM A model that chooses the answer with highest cosine similarity to the question, where the question and answers are represented by an average of pre-trained word embeddings. and (b) whether context (web snippets) is used. We now elaborate on the different baselines.  their definition and their frequency in the analyzed examples. 5 Baseline Models Our goal is to collect a dataset of commonsense questions that are easy for humans, but hard for current NLU models. To evaluate this, we experiment with multiple baselines. Table 4 summarizes the various baseline types and characterizes them based on (a) whether training is done on COMMONSENSEQA or the model is fully pre-trained, Table 4: Baseline models along with their characteristics. Training states whether the model was trained on COMMONSENSEQA, or was only trained a different dataset. Context states whether the model uses extra context as input.  ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-301b78a85599c18fac1069528e3a5e54",
        "description": "The image is a table labeled 'Table 4: Baseline models along with their characteristics.' The table has three columns: Model, Training, and Context. Each row represents a different model and its corresponding characteristics. The models listed are VecSim, LM1B, QABILINEAR, QACOMPARE, ESIM, GPT, BERT, and BIDAF++. The 'Training' column indicates whether the model was trained on COMMONSENSEQA or not, marked with a checkmark (✓) for yes and an 'X' for no. The 'Context' column shows whether the model uses extra context as input, also marked with a checkmark (✓) for yes and an 'X' for no. Specifically, VecSim, LM1B, QABILINEAR, QACOMPARE, ESIM, GPT, and BERT do not use extra context, while BIDAF++ does. VecSim, LM1B, QABILINEAR, QACOMPARE, ESIM, GPT, and BERT were not trained on COMMONSENSEQA, whereas BIDAF++ was.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": "Baseline analysis To understand the performance of BERT-LARGE, we analyzed 100 examples from the development set Lastly, all SANITY models that were trained on COMMONSENSEQA achieve very high performance $92\\%$ for BERT-LARGE), showing that selecting difficult distractors is crucial. Performance on the random split is five points lower than the question concept split on average across all trained models. We hypothesize that this is because having questions in the development/test set that share a question concept with the training set, but have a different answer, creates difficulty for networks that memorize the relation between a question concept and an answer.   quite low. The middle part describes models that were trained on COMMONSENSEQA, where BERT-LARGE obtains best performance, as mentioned above. ESIM models follow BERT-LARGE and GPT, and obtain much lower performance. We note that ELMo representations did not improve performance compared to GloVe embeddings, possibly because we were unable to improve performance by back-propagating into the representations themselves (as we do in BERT-LARGE and GPT). The bottom part shows results for $\\mathrm{BIDAF}++$ that uses web snippets as context. We observe that using snippets does not lead to high performance, hinting that they do not carry a lot of useful information. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-cfaffefba68322e03269130cbe10d877",
        "description": "The image is a table that compares the performance of various models on two different splits: Random split and Question concept split. The table has four columns: Model, Accuracy for Random split, SANITY for Random split, Accuracy for Question concept split, and SANITY for Question concept split. The rows list different models and their respective performance metrics. For example, BERT-LARGE achieves an accuracy of 55.9% and a SANITY score of 92.3% on the Random split, and 63.6% accuracy with 93.2% SANITY on the Question concept split. Other models listed include VecSim+Numberbatch, LM1B-REP, LM1B-CONCAT, VecSim+GloVe, GPT, ESIM+ELMo, ESIM+GloVe, QABilinear+GloVe, ESIM+Numberbatch, QABilinear+Numberbatch, QACompare+GloVe, QACompare+Numberbatch, and BiDAF++. The human performance is also included at the bottom, achieving 88.9% accuracy. The table highlights the superior performance of BERT-LARGE compared to other models.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_10.jpg",
        "caption": [
            "Table 5: Test set accuracy for all models. ",
            "Table 6: BERT-LARGE baseline analysis. For each category we provide two examples, the correct answer, one distractor, model accuracy and frequency in the dataset. The predicted answer is in bold. "
        ],
        "footnote": [],
        "context": "Baseline analysis To understand the performance of BERT-LARGE, we analyzed 100 examples from the development set Lastly, all SANITY models that were trained on COMMONSENSEQA achieve very high performance $92\\%$ for BERT-LARGE), showing that selecting difficult distractors is crucial. Performance on the random split is five points lower than the question concept split on average across all trained models. We hypothesize that this is because having questions in the development/test set that share a question concept with the training set, but have a different answer, creates difficulty for networks that memorize the relation between a question concept and an answer.  quite low. The middle part describes models that were trained on COMMONSENSEQA, where BERT-LARGE obtains best performance, as mentioned above. ESIM models follow BERT-LARGE and GPT, and obtain much lower performance. We note that ELMo representations did not improve performance compared to GloVe embeddings, possibly because we were unable to improve performance by back-propagating into the representations themselves (as we do in BERT-LARGE and GPT). The bottom part shows results for $\\mathrm{BIDAF}++$ that uses web snippets as context. We observe that using snippets does not lead to high performance, hinting that they do not carry a lot of useful information.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-cfaffefba68322e03269130cbe10d877",
        "description": "The image is a table labeled 'Table 5: Test set accuracy for all models' and 'Table 6: BERT-LARGE baseline analysis'. The table is divided into six columns: Category, Formulated question example, Correct answer, Distractor, Accuracy, and %. Each row represents a different category of questions. The categories include Surface clues, Negation / Antonym, Factoid knowledge, Bad granularity, and Conjunction. For each category, two examples of formulated questions are provided along with the correct answer, one distractor, model accuracy, and frequency in the dataset. The predicted answer is in bold. For instance, under the Surface clues category, the first example question is 'If someone laughs after surprising them they have a good sense of what?' with the correct answer being 'humor' and the distractor being 'laughter'. The accuracy for this category is 77.7 and the frequency is 35%. Similarly, other categories provide their respective examples, answers, distractors, accuracies, and frequencies. The table highlights the performance of BERT-LARGE on different types of questions, showing varying levels of accuracy across categories.",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/N19-1421/images/image_11.jpg",
        "caption": [],
        "footnote": [],
        "context": "We present COMMONSENSEQA, a new QA dataset that contains 12,247 examples and aims to test commonsense knowledge. We describe a process for generating difficult questions at scale using CONCEPTNET, perform a detailed analysis of the dataset, which elucidates the unique properties of our dataset, and extensively evaluate on a strong suite of baselines. We find that the best model is a pre-trained LM tuned for our task and obtains $55.9\\%$ accuracy, dozens of points lower than human accuracy. We hope that this dataset facilitates future work in incorporating commonsense knowledge into NLU systems. 7 Conclusion stantially lower than human performance.  how current models might perform with more data, we evaluated BERT-large on the development set, training with varying amounts of data. The resulting learning curves are plotted in figure 5. For each training set size, hyper-parameters were identical to section 5, except the number of epochs was varied to keep the number of mini-batches during training constant. To deal with learning instabilities, each data point is the best of 3 runs. We observe that the accuracy of BERT-LARGE is expected to be roughly $75\\%$ assuming $100\\mathrm{k}$ examples, still subFigure 5: Development accuracy for BERT-LARGE trained with varying amounts of data. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-cfaffefba68322e03269130cbe10d877",
        "description": "The image is a graph that plots the development accuracy of BERT-LARGE trained with varying amounts of data. The x-axis represents the number of instances on a logarithmic scale, ranging from 10^2 to 10^5. The y-axis represents the development accuracy, ranging from 0.2 to 1.0. There are three lines plotted: a dashed blue line labeled 'question concept', an orange dashed line labeled 'random', and a dotted cyan line labeled 'human performance'. The 'question concept' line starts at approximately 0.4 accuracy with 10^2 instances and increases steadily to about 0.75 accuracy with 10^5 instances. The 'random' line starts at approximately 0.3 accuracy with 10^2 instances and increases to about 0.8 accuracy with 10^5 instances. The 'human performance' line is a horizontal line at approximately 0.9 accuracy, indicating the upper bound of performance.",
        "segmentation": false
    }
}