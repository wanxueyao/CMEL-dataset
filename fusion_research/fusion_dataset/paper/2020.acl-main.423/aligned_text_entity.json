{
    "image_1": [
        {
            "merged_entity_name": "BERT",
            "entity_type": "ORGANIZATION",
            "description": "BERT, an acronym for Bidirectional Encoder Representations from Transformers, is a neural language model that uses a Transformer encoder and is capable of contextualized word embeddings. It is the basis for SenseBERT, which incorporates word-supersense information into its pre-training. BERT is also referenced in the context of various word embedding techniques and its architecture includes an internal Transformer encoder and an external mapping to the word vocabulary space.",
            "source_image_entities": [
                "BERT"
            ],
            "source_text_entities": [
                "BERT",
                "BERT",
                "BERT"
            ]
        },
        {
            "merged_entity_name": "SenseBERT",
            "entity_type": "ORGANIZATION",
            "description": "SenseBERT is a neural language model developed by AI21 Labs that extends BERT by incorporating sense embeddings in addition to word embeddings and positional embeddings, and outputs both word and sense predictions. It predicts masked words and their WordNet supersenses, achieving state-of-the-art performance on the Word in Context task, improving the score of BERTLARGE by 2.5 points.",
            "source_image_entities": [
                "SENSEBERT"
            ],
            "source_text_entities": [
                "SENSEBERT",
                "SENSEBERT"
            ]
        }
    ],
    "image_2": [],
    "image_3": [
    ],
    "image_4": [
    ],
    "image_5": [
        {
            "merged_entity_name": "SEMEVAL-SS",
            "entity_type": "EVENT",
            "description": "SemEval-SS is an evaluation task for semantic role labeling where the performance of supersense prediction models is evaluated. It is a supersense-based variant of the SemEval WSD test sets, where a model predicts the supersense of a marked word.",
            "source_image_entities": [
                "SEMEVAL-SS"
            ],
            "source_text_entities": [
                "SEMEVAL-SS"
            ]
        }
    ],
    "image_6": [
    ],
    "image_7": [
        {
            "merged_entity_name": "BERT_LARGE",
            "entity_type": "ORGANIZATION",
            "description": "BERT_LARGE refers to a version of BERT with 340M parameters and a different embedding dimension, used for various natural language processing tasks, including the prediction of word senses and subword tokens.",
            "source_image_entities": [
                "BERT_LARGE"
            ],
            "source_text_entities": [
                "BERTLARGE"
            ]
        },
        {
            "merged_entity_name": "WORD IN CONTEXT (WIC)",
            "entity_type": "EVENT",
            "description": "WiC is an event or task where models determine whether underlined words are used in the same or different supersenses, requiring a high level of lexical semantic understanding.",
            "source_image_entities": [
                "WORD IN CONTEXT"
            ],
            "source_text_entities": [
                "WORD IN CONTEXT (WIC)",
                "WORD IN CONTEXT (WIC) TASK"
            ]
        }
    ],
    "image_8": [
        {
            "entity_name": "SENSEBERT",
            "entity_type": "ORGANIZATION",
            "description": "SenseBERT is a model that demonstrates the ability to assign semantically meaningful categories and has been tested for its performance on lexical semantic categorization tasks, achieving a score of 72.1 in the Word in Context task.",
            "source_image_entities": [
                "SENSEBERT"
            ],
            "source_text_entities": [
                "SENSEBERT"
            ]
        },
        {
            "entity_name": "WORD IN CONTEXT (WIC) TASK",
            "entity_type": "EVENT",
            "description": "The WiC task is a recently introduced binary classification task that requires a model to determine whether an underlined word is used in the same or different supersense within two sentences, as defined over supersenses by Pilehvar and Camacho-Collados (2019).",
            "source_image_entities": [
                "WORD IN CONTEXT"
            ],
            "source_text_entities": [
                "WORD IN CONTEXT (WIC) TASK"
            ]
        }
    ],
    "image_9": [
    ],
    "image_10": [
    ],
    "image_11": [
        {
            "entity_name": "Table 5",
            "entity_type": "CONCEPT",
            "description": "Table 5 is a comprehensive list of WordNet supersenses presented in the paper's documentation, as mentioned in the text and visualized in the associated image files.",
            "source_image_entities": [
                "TABLE"
            ],
            "source_text_entities": [
                "TABLE 5"
            ]
        }
    ]
}