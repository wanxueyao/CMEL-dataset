{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/L18-1269/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR and both binary and fine-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation   in order to avoid discrepancies in reported results; and • easy access for anyone, meaning: a straightforward interface in Python, and scripts necessary to download and preprocess the relevant datasets. In addition, we provide examples of models, such as a simple bag-of-words model. These could potentially also be used to extrinsically evaluate the quality of word embeddings in NLP tasks. 3. Evaluations Our aim is to obtain general-purpose sentence embeddings that capture generic information, which should be useful for a broad set of tasks. To evaluate the quality of these representations, we use them as features in various transfer tasks. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-4069c8d93eb853f9b33b67a5c37a147d",
        "description": "The image is a table that provides details on various classification tasks used in natural language processing (NLP). The table has seven rows and six columns. The columns are labeled as follows: 'name', 'N', 'task', 'C', 'examples', and 'label(s)'. The rows provide the following information:\\n\\n1. **MR**: \\n   - N: 11k\\n   - Task: sentiment (movies)\\n   - C: 2\\n   - Examples: 'Too slow for a younger crowd, too shallow for an older one.'\\n   - Label(s): neg\\n\\n2. **CR**: \\n   - N: 4k\\n   - Task: product reviews\\n   - C: 2\\n   - Examples: 'We tried it out christmas night and it worked great .'\\n   - Label(s): pos\\n\\n3. **SUBJ**: \\n   - N: 10k\\n   - Task: subjectivity/objectivity\\n   - C: 2\\n   - Examples: 'A movie that doesn’t aim too high, but doesn’t need to.'\\n   - Label(s): subj\\n\\n4. **MPQA**: \\n   - N: 11k\\n   - Task: opinion polarity\\n   - C: 2\\n   - Examples: 'don’t want'; 'would like to tell'\\n   - Label(s): neg, pos\\n\\n5. **TREC**: \\n   - N: 6k\\n   - Task: question-type\\n   - C: 6\\n   - Examples: 'What are the twin cities ?'\\n   - Label(s): LOC:city\\n\\n6. **SST-2**: \\n   - N: 70k\\n   - Task: sentiment (movies)\\n   - C: 2\\n   - Examples: 'Audrey Tautou has a knack for picking roles that magnify her [..]'\\n   - Label(s): pos\\n\\n7. **SST-5**: \\n   - N: 12k\\n   - Task: sentiment (movies)\\n   - C: 5\\n   - Examples: 'nothing about this movie works.'\\n   - Label(s): 0\\n\\nThe table summarizes different datasets used for various NLP tasks, including sentiment analysis, product reviews, subjectivity/objectivity, opinion polarity, and question-type classification.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/L18-1269/images/image_2.jpg",
        "caption": [
            "Table 1: Classification tasks. C is the number of classes and $\\mathbf{N}$ is the number of samples. ",
            "Table 2: Natural Language Inference and Semantic Similarity tasks. NLI labels are contradiction, neutral and entailment. STS labels are scores between 0 and 5. PD=paraphrase detection, $\\mathrm{ICR}{=}\\mathrm{i}$ mage-caption retrieval. "
        ],
        "footnote": [],
        "context": "Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR and both binary and fine-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation  in order to avoid discrepancies in reported results; and • easy access for anyone, meaning: a straightforward interface in Python, and scripts necessary to download and preprocess the relevant datasets. In addition, we provide examples of models, such as a simple bag-of-words model. These could potentially also be used to extrinsically evaluate the quality of word embeddings in NLP tasks. 3. Evaluations Our aim is to obtain general-purpose sentence embeddings that capture generic information, which should be useful for a broad set of tasks. To evaluate the quality of these representations, we use them as features in various transfer tasks.  ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-4069c8d93eb853f9b33b67a5c37a147d",
        "description": "The image is a table labeled 'Table 2: Natural Language Inference and Semantic Similarity tasks.' The table is structured with the following columns: name, N (number of samples), task, output, premise, hypothesis, and label. Each row represents a different dataset or task. The rows are as follows:\\n- SNLI: 560k samples, NLI task, output 3, premise 'A small girl wearing a pink jacket is riding on a carousel.', hypothesis 'The carousel is moving.', label entailment.\\n- SICK-E: 10k samples, NLI task, output 3, premise 'A man is sitting on a chair and rubbing his eyes', hypothesis 'There is no man sitting on a chair and rubbing his eyes', label contradiction.\\n- SICK-R: 10k samples, STS task, output [0, 5], premise 'A man is singing a song and playing the guitar', hypothesis 'A man is opening a package that contains headphones', label 1.6.\\n- STS14: 4.5k samples, STS task, output [0, 5], premise 'Liquid ammonia leak kills 15 in Shanghai', hypothesis 'Liquid ammonia leak kills at least 15 in Shanghai', label 4.6.\\n- MRPC: 5.7k samples, PD task, output 2, premise 'The procedure is generally performed in the second or third trimester.', hypothesis 'The technique is used during the second and, occasionally, third trimester of pregnancy.', label paraphrase.\\n- COCO: 565k samples, ICR task, output sim, premise shows an image of a group of people on horses riding through the beach, hypothesis 'A group of people on some horses riding through the beach.', label rank.\\nThe table provides detailed information about various natural language inference and semantic similarity tasks, including the number of samples, type of task, output format, and specific examples of premises and hypotheses along with their labels.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/L18-1269/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Transfer test results for various baseline methods. We include supervised results trained directly on each task (no transfer). Results 1 correspond to AdaSent (Zhao et al., 2015), 2 to BLSTM-2DCNN (Zhou et al., 2016), 3 to TF-KLD (Ji and Eisenstein, 2013) and  to Illinois-LH system (Lai and Hockenmaier, 2014). "
        ],
        "context": "Datasets In order to obtain the data and You may also pass additional parameters to the params object in order which will further be accessible from the prepare and batcher functions (e.g a pretrained model). params['classifier'] $=$ {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128, 'tenacity': 3, 'epoch_size': 2} For use cases where there are multiple calls to SentEval, e.g when evaluating the sentence encoder at every epoch of training, we propose the following prototyping set of parameters, which will lead to slightly worse results but will make the evaluation significantly faster: • dropout (float): dropout rate in the case of MLP. the classifier include: • nhid (int): number of hidden units of the MLP; if nhid $>\\ 0$ , a Multi-Layer Perceptron with one hidden layer and a Sigmoid nonlinearity is used. • optim (str): classifier optimizer (default: adam). • batch size (int): batch size for training the classifier (default: 64). • tenacity (int): stopping criterion; maximum number of times the validation error does not decrease. • epoch size (int): number of passes through the training set for one epoch. $^5\\mathrm{Or}$ any other programming language, as long as the vectors can be passed to, or loaded from, code written in Python. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-93d4d0771d13ab485a2ecb6bfe9432e8",
        "description": "The image is a table labeled 'Table 3: Transfer test results for various baseline methods.' The table compares the performance of different models on various tasks. The rows represent different models, including GloVe LogReg, GloVe MLP, fastText LogReg, fastText MLP, SkipThought, and InferSent. The columns represent different datasets or tasks, such as MR, CR, SUBJ, MPQA, SST-2, SST-5, TREC, MRPC, and SICK-E. Each cell contains a numerical value representing the performance of the model on that task. For example, the GloVe LogReg model scores 77.4 on the MR task, 78.7 on the CR task, and so on. The last row labeled 'SOTA' (State Of The Art) provides the best-known results for each task, with values like 83.1 for MR, 86.3 for CR, and so on. The footnote indicates that the results correspond to specific systems: Result 1 corresponds to AdaSent, Result 2 to BLSTM-2DCNN, Result 3 to TF-KLD, and Result 4 to Illinois-LH system.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/L18-1269/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Supervised methods directly trained for each task (no transfer) PP-Proj 60.01 56.81 71.31 74.81 86.82 "
        ],
        "context": "Adi, Y., Kermany, E., Belinkov, Y., Lavi, O., and Goldberg, Y. (2017). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In Proceedings of ICLR Conference Track, Toulon, France. Published online: https://openreview.net/group?id $=$ ICLR.cc/2017/conference. Agirre, 7. Bibliographical References Table 4: Evaluation of sentence representations on the semantic textual similarity benchmarks. Numbers reported are Pearson correlations $\\mathbf{x}100$ . We use the average of Pearson correlations for STS’12 to STS’16 which are composed of several subtasks. Charagram-phrase numbers were taken from (Wieting et al., 2016). Results 1 correspond to PP-Proj (Wieting et al., 2015) and 2 from Tree-LSTM (Tai et al., 2015b). possible: sentence encoders can be evaluated by implementing a simple Python interface, and we provide a script to download the necessary evaluation datasets. In future work, we plan to enrich SentEval with additional tasks as the consensus on the best evaluation for sentence embeddings evolves. In particular, tasks that probe for specific linguistic properties of the sentence embeddings (Shi et al., 2016; Adi et al., 2017) are interesting directions towards understanding how the encoder understands language. We hope that our toolkit will be used by the community in order to ensure that fully comparable results are published in research papers. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-ae5c6eb12fe892506cd4d97ec68970a5",
        "description": "The image is a table that evaluates various sentence representation learning models on different semantic textual similarity benchmarks. The table has the following columns: Model, SST'12, SST'13, SST'14, SST'15, SST'16, SICK-R, and SST-B. The rows represent different models used for representation learning. The values in the table are Pearson correlations multiplied by 100. The models listed are GloVe BoW, fastText BoW, SkipThought-LN, InferSent, and Char-phrase. For example, GloVe BoW scores 52.1 on SST'12, 49.6 on SST'13, 54.6 on SST'14, 56.1 on SST'15, 51.4 on SST'16, 79.9 on SICK-R, and 64.7 on SST-B. Similarly, fastText BoW scores 58.3 on SST'12, 57.9 on SST'13, 64.9 on SST'14, 67.6 on SST'15, 64.3 on SST'16, 82.0 on SICK-R, and 70.2 on SST-B. The table highlights the performance of these models across different benchmarks, with some models performing better on certain benchmarks than others.",
        "segmentation": false
    }
}