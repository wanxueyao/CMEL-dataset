<?xml version='1.0' encoding='UTF-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="order" attr.type="long"/>
<key id="d5" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;IMAGE_6&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image is a table that presents the performance of different models on matching natural language sentences across various datasets. The table is structured with rows representing different models and columns representing different datasets. The datasets are SNLI, MultiNLI (Matched and Mismatched), WikiQA, Quora, and MSRP. Each dataset has specific metrics reported: SNLI and MultiNLI report accuracy (Acc.), WikiQA reports Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), Quora reports accuracy (Acc.), and MSRP reports accuracy (Acc.) and F1 score. The models listed are CNN, LSTM, SWEM-aver, SWEM-max, and SWEM-concat. For example, CNN achieves an accuracy of 82.1% on SNLI, 65.0% on MultiNLI Matched, and 65.3% on MultiNLI Mismatched. LSTM achieves an accuracy of 80.6% on SNLI, 66.9% on MultiNLI Matched, and 66.9% on MultiNLI Mismatched. SWEM-max performs the best among all SWEM variants, achieving an accuracy of 83.8% on SNLI, 68.2% on MultiNLI Matched, and 67.7% on MultiNLI Mismatched. The table highlights the competitive performance of SWEM-max with only 120K parameters."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;CNN&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"A type of neural network model used for various tasks including image and text recognition. In this context, it is used for natural language inference tasks as indicated by the table."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;LSTM&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"A type of recurrent neural network (RNN) that is well-suited to classify, process and predict time series given time lags of unknown size and duration. It is used here for natural language inference tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;SWEM-AVER&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"A model that averages word embeddings for sentence representation in natural language inference tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;SWEM-MAX&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"A model that uses the maximum value of word embeddings for sentence representation in natural language inference tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;SWEM-CONCAT&quot;">
  <data key="d0">"MODEL"</data>
  <data key="d1">"A model that concatenates word embeddings for sentence representation in natural language inference tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;SNLI&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"The Stanford Natural Language Inference dataset used for evaluating models on natural language inference tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;MULTINLI&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"The Multi-genre Natural Language Inference dataset used for evaluating models on natural language inference tasks. It has matched and mismatched sections."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;WIKIQA&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"A question answering dataset used for evaluating models on question answering tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;QUORA&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"A dataset from Quora used for evaluating models on question pair similarity tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<node id="&quot;MSRP&quot;">
  <data key="d0">"DATASET"</data>
  <data key="d1">"The Microsoft Research Paraphrase Corpus used for evaluating models on paraphrase identification tasks."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
</node>
<edge source="&quot;IMAGE_6&quot;" target="&quot;CNN&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"CNN是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;LSTM&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"LSTM是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;SWEM-AVER&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"SWEM-aver是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;SWEM-MAX&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"SWEM-max是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;SWEM-CONCAT&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"SWEM-concat是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;SNLI&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"SNLI是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;MULTINLI&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"MultiNLI是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;WIKIQA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"WikiQA是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;QUORA&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Quora是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_6&quot;" target="&quot;MSRP&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"MSRP是从image_6中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CNN&quot;" target="&quot;SNLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The CNN model was evaluated on the SNLI dataset for accuracy in natural language inference tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CNN&quot;" target="&quot;MULTINLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The CNN model was evaluated on the MultiNLI dataset for accuracy in natural language inference tasks, both matched and mismatched sections."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CNN&quot;" target="&quot;WIKIQA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The CNN model was evaluated on the WikiQA dataset for MAP and MRR scores in question answering tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CNN&quot;" target="&quot;QUORA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The CNN model was evaluated on the Quora dataset for accuracy in question pair similarity tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CNN&quot;" target="&quot;MSRP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The CNN model was evaluated on the MSRP dataset for accuracy and F1 score in paraphrase identification tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LSTM&quot;" target="&quot;SNLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The LSTM model was evaluated on the SNLI dataset for accuracy in natural language inference tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LSTM&quot;" target="&quot;MULTINLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The LSTM model was evaluated on the MultiNLI dataset for accuracy in natural language inference tasks, both matched and mismatched sections."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LSTM&quot;" target="&quot;WIKIQA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The LSTM model was evaluated on the WikiQA dataset for MAP and MRR scores in question answering tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LSTM&quot;" target="&quot;QUORA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The LSTM model was evaluated on the Quora dataset for accuracy in question pair similarity tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;LSTM&quot;" target="&quot;MSRP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The LSTM model was evaluated on the MSRP dataset for accuracy and F1 score in paraphrase identification tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-AVER&quot;" target="&quot;SNLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SWEM-aver model was evaluated on the SNLI dataset for accuracy in natural language inference tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-AVER&quot;" target="&quot;MULTINLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SWEM-aver model was evaluated on the MultiNLI dataset for accuracy in natural language inference tasks, both matched and mismatched sections."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-AVER&quot;" target="&quot;WIKIQA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-aver model was evaluated on the WikiQA dataset for MAP and MRR scores in question answering tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-AVER&quot;" target="&quot;QUORA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-aver model was evaluated on the Quora dataset for accuracy in question pair similarity tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-AVER&quot;" target="&quot;MSRP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-aver model was evaluated on the MSRP dataset for accuracy and F1 score in paraphrase identification tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-MAX&quot;" target="&quot;SNLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SWEM-max model was evaluated on the SNLI dataset for accuracy in natural language inference tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-MAX&quot;" target="&quot;MULTINLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SWEM-max model was evaluated on the MultiNLI dataset for accuracy in natural language inference tasks, both matched and mismatched sections."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-MAX&quot;" target="&quot;WIKIQA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-max model was evaluated on the WikiQA dataset for MAP and MRR scores in question answering tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-MAX&quot;" target="&quot;QUORA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-max model was evaluated on the Quora dataset for accuracy in question pair similarity tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-MAX&quot;" target="&quot;MSRP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-max model was evaluated on the MSRP dataset for accuracy and F1 score in paraphrase identification tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-CONCAT&quot;" target="&quot;SNLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SWEM-concat model was evaluated on the SNLI dataset for accuracy in natural language inference tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-CONCAT&quot;" target="&quot;MULTINLI&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The SWEM-concat model was evaluated on the MultiNLI dataset for accuracy in natural language inference tasks, both matched and mismatched sections."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-CONCAT&quot;" target="&quot;WIKIQA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-concat model was evaluated on the WikiQA dataset for MAP and MRR scores in question answering tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-CONCAT&quot;" target="&quot;QUORA&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-concat model was evaluated on the Quora dataset for accuracy in question pair similarity tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;SWEM-CONCAT&quot;" target="&quot;MSRP&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The SWEM-concat model was evaluated on the MSRP dataset for accuracy and F1 score in paraphrase identification tasks."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1041/images/image_6.jpg</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>
