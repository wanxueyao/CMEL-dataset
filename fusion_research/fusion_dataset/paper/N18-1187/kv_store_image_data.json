{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_1.jpg",
        "caption": [
            "Figure 1: Proposed end-to-end task-oriented dialogue system architecture. "
        ],
        "footnote": [],
        "context": "A dialogue policy selects the next system action in response to the user’s input based on the current dialogue state. We use a deep neural network 3.4 Dialogue Policy Once the KB query results are returned, we save the retrieved entities to a queue and encode the result summary to a vector. Rather then encoding the real KB entity values as in (Bordes and Weston, 2017; Eric and Manning, 2017), we only encode a summary of the query results (i.e. item availability and number of matched items). This encoding serves as a part of the input to the policy network.  our model sends symbolic queries to the KB and leaves the ranking of the KB entities to an external recommender system. Entity ranking in real world systems can be made with much richer features (e.g. user profiles, local context, etc.) in the back-end system other than just following entity posterior probabilities conditioning on a user utterance. Hence ranking of the KB entities is not a part of our proposed neural dialogue model. In this work, we assume that the model receives a ranked list of KB entities according to the issued query and other available sources, such as user models. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-de4707ebd56153a05119e590392c169a",
        "description": "The image is a flowchart illustrating the proposed end-to-end task-oriented dialogue system architecture. The process begins with a user input, 'Movie for the day after tomorrow, please', which is fed into a Bi-LSTM Utterance Encoder. This encoder processes the user's utterance and generates an encoding at turn k. Simultaneously, the system dialogue act embedding from the previous turn (k-1) is considered. The encoded user utterance and the system dialogue act are then passed to an LSTM Dialogue State module, which updates the dialogue state tracking. The updated dialogue state is used to query the Knowledge Base, which returns results based on the date (Thursday) and time (none). The query results are encoded and fed into the Policy Network, which selects the next system action. In this case, the system responds with 'Ok, what time do you prefer?', which is generated by the Natural Language Generator. The overall architecture integrates multiple neural network components and a knowledge base to manage dialogue state tracking and policy decisions in a conversational system.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_2.jpg",
        "caption": [
            "Figure 2: Dialogue state and policy network. "
        ],
        "footnote": [],
        "context": "The emitted system action is finally used to produce a system response in natural language format by combining the state tracker outputs and the retrieved KB entities. We use a template based NLG in this work. The delexicalised tokens in the NLG template are replaced by the values from either the estimated user goal values or the where $v_{k}$ represents the concatenated log probabilities of candidate values for each goal slot, $E_{k}$ is the encoding of query results, and PolicyNet is a single hidden layer MLP with softmax activation function over all system actions. $$ P(a_{k}\\mid U_{\\leq k},~A_{<k},~E_{\\leq k})=\\mathrm{PolicyNet}(s_{k},v_{k},E_{k}) $$  a part of the input to the policy network. 3.4 Dialogue Policy A dialogue policy selects the next system action in response to the user’s input based on the current dialogue state. We use a deep neural network to model the dialogue policy. There are three inputs to the policy network, (1) the dialogue-level LSTM state $s_{k}$ , (2) the log probabilities of candidate values from the belief tracker $v_{k}$ , and (3) the encoding of the query results summary $E_{k}$ . The policy network emits a system action in the form of a dialogue act conditioning on these inputs: ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-de4707ebd56153a05119e590392c169a",
        "description": "The image is a flowchart illustrating the dialogue state and policy network in a conversational system. The diagram consists of several interconnected components. At the bottom, there is a rectangular box labeled 'LSTM Dialogue State, $S_{k}$'. Above this box, there are two other boxes: one on the left labeled 'Slot value logits' with a dashed outline containing multiple smaller rectangles representing different slot values, and one on the right labeled 'Query results encoding, $E_{k}$'. These three components feed into a larger box labeled 'Policy Network', which is depicted as a stack of three horizontal rectangles. The output of the Policy Network is a single rectangle labeled 'System action at turn k, $a_{k}$'. The arrows indicate the flow of information from the LSTM Dialogue State, through the Slot value logits and Query results encoding, to the Policy Network, and finally to the System action. The overall structure suggests a sequential process where the current dialogue state, along with the slot value logits and query results encoding, are used to determine the next system action.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_3.jpg",
        "caption": [],
        "footnote": [],
        "context": "Task Success Rate As shown in the learning curves in Figure 3, the SL model performs poorly. This might largely due Evaluations of interactive learning with imitation and reinforcement learning are made on metrics of (1) task success rate, (2) dialogue turn size, and (3) DST accuracy. Figures 3, 4, and 5 show the learning curves for the three evaluation metrics. In addition, we compare model performance on task success rate using two different RL training settings, the end-to-end training and the policyonly training, to show the advantages of performing end-to-end system optimization with RL. 4.4 Imitation and RL Results  on DSTC2 corpus, on both individual slot tracking and joint slot tracking, comparing to the recent published results using RNN (Henderson et al., 2014b) and neural belief tracker (NBT) (Mrkˇsi´c et al., 2016). In the movie booking domain, our model also achieves promising performance on both individual slot tracking and joint slot tracking accuracy. Instead of using ASR hypothesis as model input as in DSTC2, here we use text based input which has much lower noise level in the evaluation of the movie booking tasks. This partially explains the higher DST accuracy in the movie booking domain comparing to DSTC2.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-2b703dd999933ff1b24acfd0a93e0d01",
        "description": "The image is a table comparing the performance of different models on various metrics. The table has four rows and five columns. The first column lists the models: RNN, RNN+sem. dict, NBT, and Our SL model. The subsequent columns are labeled 'Area', 'Food', 'Price', and 'Joint'. The values in the 'Area' column are 92 for RNN, 92 for RNN+sem. dict, 90 for NBT, and 90 for Our SL model. The 'Food' column shows 86 for RNN, 86 for RNN+sem. dict, 84 for NBT, and 84 for Our SL model. The 'Price' column has 86 for RNN, 92 for RNN+sem. dict, 94 for NBT, and 92 for Our SL model. The 'Joint' column displays 69 for RNN, 71 for RNN+sem. dict, 72 for NBT, and 72 for Our SL model. The table highlights the performance differences among the models across different metrics.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_4.jpg",
        "caption": [
            "Table 1: Dialogue state tracking results on DSTC2 "
        ],
        "footnote": [
            "Table 2: DST results on movie booking dataset "
        ],
        "context": "Task Success Rate As shown in the learning curves in Figure 3, the SL model performs poorly. This might largely due Evaluations of interactive learning with imitation and reinforcement learning are made on metrics of (1) task success rate, (2) dialogue turn size, and (3) DST accuracy. Figures 3, 4, and 5 show the learning curves for the three evaluation metrics. In addition, we compare model performance on task success rate using two different RL training settings, the end-to-end training and the policyonly training, to show the advantages of performing end-to-end system optimization with RL. 4.4 Imitation and RL Results on DSTC2 corpus, on both individual slot tracking and joint slot tracking, comparing to the recent published results using RNN (Henderson et al., 2014b) and neural belief tracker (NBT) (Mrkˇsi´c et al., 2016). In the movie booking domain, our model also achieves promising performance on both individual slot tracking and joint slot tracking accuracy. Instead of using ASR hypothesis as model input as in DSTC2, here we use text based input which has much lower noise level in the evaluation of the movie booking tasks. This partially explains the higher DST accuracy in the movie booking domain comparing to DSTC2.   ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-2b703dd999933ff1b24acfd0a93e0d01",
        "description": "The image is a table labeled 'Table 1: Dialogue state tracking results on DSTC2'. The table is structured with two main columns: 'Goal slot' and 'Accuracy'. Under the 'Goal slot' column, there are six rows listing different parameters: 'Num of Tickets', 'Movie', 'Theater Name', 'Date', 'Time', and 'Joint'. Correspondingly, under the 'Accuracy' column, the values for each parameter are as follows: 'Num of Tickets' has an accuracy of 98.22%, 'Movie' has 91.86%, 'Theater Name' has 97.33%, 'Date' has 99.31%, 'Time' has 97.71%, and 'Joint' has 84.57%. The table provides detailed accuracy metrics for various goal slots in dialogue state tracking tasks.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_5.jpg",
        "caption": [
            "Figure 3: Interactive learning curves on task success rate. "
        ],
        "footnote": [],
        "context": "Average Dialogue Turn Size Figure 4 shows the curves for the average turn size of successful dialogues. We observe decreasing number of dialogue turns in completing a task along the growing number of interactive learning sessions. This shows that the dialogue agent learns better strategies in successfully completing the task with fewer number of dialogue turns. The red curve with RL applied directly after supervised pre-training model gives the lowest average number of turns at the end of the interactive learning cycles, comparing to models with imitation dialogue learning. This seems to be contrary to our observation in Figure 3  shows the performance of the model that has 500 episodes of imitation learning over the SL model and continues with RL optimization. It is clear from the results that applying imitation learning on supervised training model efficiently improves task success rate. RL optimization after imitation learning increases the task success rate further. The blue curve $\\mathrm{~\\textit~{~\\textcent~}~}+\\mathrm{~\\textit~{~I~L~}~}1000\\mathrm{~\\textit~{~+~}~}$ RL) shows the performance of the model that has 1000 episodes of imitation learning over the SL model and continues with RL. Similarly, it shows hints that imitation learning may effectively adapt the supervised training model to the dialogue state distribution during user interactions. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-2b703dd999933ff1b24acfd0a93e0d01",
        "description": "The image is a line graph titled 'Task Success Rate over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10000. The y-axis represents the task success rate, ranging from 0.3 to 0.7. Four different curves are plotted on the graph, each representing a different model configuration: SL Baseline (dotted cyan line), SL + RL (solid red line with pentagon markers), SL + IL 500 + RL (solid yellow line with triangle markers), and SL + IL 1000 + RL (solid blue line with star markers). The SL Baseline curve remains relatively flat near the bottom of the graph, indicating a low and stable task success rate. The SL + RL curve starts lower but gradually increases, reaching a plateau around 0.55. The SL + IL 500 + RL curve starts higher than the SL + RL curve and also increases, reaching a plateau around 0.62. The SL + IL 1000 + RL curve starts the highest and increases further, reaching a plateau around 0.65. Two specific points are highlighted: one on the SL + IL 1000 + RL curve at around 2000 sessions with a task success rate of approximately 0.6, and another on the SL + IL 500 + RL curve at around 2000 sessions with a task success rate of approximately 0.58.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_6.jpg",
        "caption": [
            "Figure 4: Interactive learning curves on average dialogue turn size. "
        ],
        "footnote": [],
        "context": "Dialogue State Tracking Accuracy Similar to the results on task success rate, we see that imitation learning with human teaching quickly improves dialogue state tracking accuracy in just a few hundred interactive learning sessions. The joint slots tracking accuracy in the evaluation of SL model using fixed corpus is $84.57\\%$ as in Table 4.3. The accuracy drops to $50.51\\%$ in the interactive evaluation with the introduction of new NLG templates. Imitation learning with human teaching effectively adapts the neural dialogue model to the new user input and dialogue state distributions, improving the DST accuracy to $67.47\\%$ after only 500 imitation  with human teaching helps in achieving higher task success rate. By looking into the generated dialogues, we find that the SL $+$ RL model can handle easy tasks well but fails to complete more challenging tasks. Such easy tasks typically can be handled with fewer number of turns, which result in the low average turn size for the $\\begin{array}{r l r}{{\\mathrm{SL}}}&{{}+}&{{\\mathrm{RL}}}\\end{array}$ model. On the other hand, the imitation plus RL models attempt to learn better strategies to handle those more challenging tasks, resulting in higher task success rates and also slightly increased dialogue length comparing to $\\begin{array}{r l r}{{\\mathrm{S}}\\,\\mathrm{L}}&{{}+}&{\\mathrm{RL}}\\end{array}$ model. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-e66ce441e74f5f71549e88941907166c",
        "description": "The image is a line graph titled 'Average Turn Size over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis represents the average turn size, ranging from 6.0 to 9.0. There are four lines representing different models: SL Baseline (dashed cyan line with star markers), SL + RL (solid red line with square markers), SL + IL 500 + RL (solid yellow line with triangle markers), and SL + IL 1000 + RL (solid blue line with pentagon markers). The SL Baseline line remains relatively constant at around 8.5 throughout the sessions. The SL + RL line starts at around 8.5 and gradually decreases to about 7.0 by 10,000 sessions. The SL + IL 500 + RL line starts at around 8.5 and decreases more slowly to about 7.5 by 10,000 sessions. The SL + IL 1000 + RL line starts at around 8.5 and decreases more gradually to about 7.5 by 10,000 sessions. The graph shows that the addition of imitation learning (IL) and reinforcement learning (RL) helps in reducing the average turn size over time, with the SL + IL 1000 + RL model showing the most gradual decrease.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_7.jpg",
        "caption": [
            "Figure 5: Interactive learning curves on dialogue state tracking accuracy. "
        ],
        "footnote": [],
        "context": "End-to-End RL Optimization To further show the benefit of performing end-to-end optimization of dialogue agent, we compare models with two different RL training settings, the end-to-end training and the policy-only training. End-to-end RL training is what we applied in previous evaluation sections, in which the gradient propagates from system action output layer all the way back to the natural language user input layer. Policy-only training refers to only updating the policy network parameters during interactive learning with RL, with all the other underlying system parameters fixed. The evaluation results are shown in Figure 6. From these learning curves, we see   4.3. The accuracy drops to $50.51\\%$ in the interactive evaluation with the introduction of new NLG templates. Imitation learning with human teaching effectively adapts the neural dialogue model to the new user input and dialogue state distributions, improving the DST accuracy to $67.47\\%$ after only 500 imitation dialogue learning sessions. Another encouraging observation is that RL on top of SL model and IL model not only improves task success rate by optimizing dialogue policy, but also further improves dialogue state tracking performance. This shows the benefits of performing endto-end optimization of the neural dialogue model with RL during interactive learning. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-e66ce441e74f5f71549e88941907166c",
        "description": "The image is a line graph titled 'Average DST Accuracy over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis represents the average DST accuracy, ranging from 0.50 to 0.80. Four different lines are plotted on the graph, each representing a different training method: SL Baseline (dotted cyan line with star markers), SL + RL (solid red line with square markers), SL + IL 500 + RL (solid yellow line with triangle markers), and SL + IL 1000 + RL (solid blue line with circle markers). The SL Baseline line remains constant at approximately 0.50 throughout the sessions. The SL + RL line starts at around 0.55 and gradually increases to about 0.65 by 10,000 sessions. The SL + IL 500 + RL line starts at around 0.70 and increases to about 0.75 by 10,000 sessions. The SL + IL 1000 + RL line starts at around 0.75 and maintains this level throughout the sessions. There are two circles highlighting specific points on the SL + IL 500 + RL and SL + IL 1000 + RL lines, indicating significant data points.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_8.jpg",
        "caption": [
            "Figure 6: Interactive learning curves on task success rate with different RL training settings. "
        ],
        "footnote": [],
        "context": "End-to-End RL Optimization To further show the benefit of performing end-to-end optimization of dialogue agent, we compare models with two different RL training settings, the end-to-end training and the policy-only training. End-to-end RL training is what we applied in previous evaluation sections, in which the gradient propagates from system action output layer all the way back to the natural language user input layer. Policy-only training refers to only updating the policy network parameters during interactive learning with RL, with all the other underlying system parameters fixed. The evaluation results are shown in Figure 6. From these learning curves, we see  4.3. The accuracy drops to $50.51\\%$ in the interactive evaluation with the introduction of new NLG templates. Imitation learning with human teaching effectively adapts the neural dialogue model to the new user input and dialogue state distributions, improving the DST accuracy to $67.47\\%$ after only 500 imitation dialogue learning sessions. Another encouraging observation is that RL on top of SL model and IL model not only improves task success rate by optimizing dialogue policy, but also further improves dialogue state tracking performance. This shows the benefits of performing endto-end optimization of the neural dialogue model with RL during interactive learning.  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-e66ce441e74f5f71549e88941907166c",
        "description": "The image is a line graph titled 'Task Success Rate over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis represents the task success rate, ranging from 0.3 to 0.7. There are five different lines representing different RL training settings: SL Baseline (dotted cyan line with crosses), SL + policy-only RL (solid red line with pentagons), SL + end-to-end RL (solid blue line with stars), SL + IL 1000 + policy-only RL (dashed blue line with stars), and SL + IL 1000 + end-to-end RL (solid blue line with stars). The SL Baseline starts at around 0.32 and remains relatively flat throughout the sessions. The SL + policy-only RL starts at around 0.35 and shows a gradual increase, reaching around 0.55 by 10,000 sessions. The SL + end-to-end RL starts at around 0.35 and shows a more significant increase, reaching around 0.65 by 10,000 sessions. The SL + IL 1000 + policy-only RL starts at around 0.45 and shows a steady increase, reaching around 0.6 by 10,000 sessions. The SL + IL 1000 + end-to-end RL starts at around 0.45 and shows the highest increase, reaching around 0.68 by 10,000 sessions.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/N18-1187/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": "In this work, we focus on training task-oriented dialogue systems through user interactions, where the agent improves through communicating with users and learning from the mistake it makes. We propose a hybrid learning approach for such systems using end-to-end trainable neural network model. We present a hybrid imitation and reinforcement learning method, where we firstly train a dialogue agent in a supervised manner by learning from dialogue corpora, and continuously to improve it by learning from user teaching and feedback with 5 Conclusions Table 3: Human evaluation results. Mean and standard deviation of crowd worker scores (between 1 to 5). read a dialogue between our model and user simulator and rate each system turn on a scale of 1 (frustrating) to 5 (optimal way to help the user). Each turn is rated by 3 different judges. We collect and rate 100 dialogues for each of the three models: (i) SL model, (ii) SL model followed by 1000 episodes of IL, (iii) SL and IL followed by RL. Table 3 lists the mean and standard deviation of human scores overall system turns. Performing interactive learning with imitation and reinforcement learning clearly improves the quality of the model according to human judges. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-e66ce441e74f5f71549e88941907166c",
        "description": "The image is a table labeled 'Table 3: Human evaluation results.' It presents the mean and standard deviation of crowd worker scores for different models used in training a dialogue system. The table has two columns: 'Model' and 'Score.' The rows under 'Model' are as follows: 'SL' with a score of 3.987 ± 0.086, 'SL + IL 1000' with a score of 4.378 ± 0.082, and 'SL + IL 1000 + RL' with a score of 4.603 ± 0.067. The scores range from 1 to 5, where 1 indicates a frustrating interaction and 5 indicates an optimal way to help the user. Each turn is rated by 3 different judges, and the data is collected from 100 dialogues for each model. The table highlights that the addition of interactive learning (IL) and reinforcement learning (RL) improves the quality of the model according to human judges.",
        "segmentation": false
    }
}