{
    "image_1": {
        "entity_name": "Unified Model",
        "entity_type": "CONCEPT",
        "description": "The Unified Model combines sentence-level and word-level attentions to leverage both extractive and abstractive summarization approaches.",
        "reason": "The image clearly shows a comparison of different summarization approaches, with the unified model being highlighted as combining the advantages of both extractive and abstractive methods. This matches the description of the 'Unified Model' in the text information.",
        "matched_chunk_entity_name": "Unified Model"
    },
    "image_2": {
        "entity_name": "Unified Model",
        "entity_type": "CONCEPT",
        "description": "The Unified Model combines sentence-level and word-level attentions to leverage both extractive and abstractive summarization approaches.",
        "reason": "The image illustrates the process of combining and normalizing attentions in a model that integrates both sentence-level and word-level attentions, which aligns with the description of the Unified Model in the text.",
        "matched_chunk_entity_name": "Unified Model"
    },
    "image_3": {
        "entity_name": "Architecture of the Extractor",
        "entity_type": "EVENT",
        "description": "The Architecture of the Extractor is described in the text, featuring a hierarchical bidirectional GRU and a classification layer.",
        "reason": "The image clearly shows the architecture of an extractor used in a neural network model for text summarization. The diagram includes Word-level RNN, Sentence-level RNN, and Sentence-Level Attention layers, which align with the description of the extractor's architecture in the text.",
        "matched_chunk_entity_name": "Architecture of the Extractor"
    },
    "image_4": {
        "entity_name": "Pointer-generator network",
        "entity_type": "ORGANIZATION",
        "description": "The pointer-generator network, proposed by See et al. (2017), is a sequence-to-sequence attentional model that can generate summaries by copying words from the article or generating words from a fixed vocabulary.",
        "reason": "The image clearly illustrates the decoding mechanism of a pointer-generator network, which is a key component in text summarization tasks. The diagram shows the flow of information through the encoder and decoder, the context vector, and the final word distribution, all of which are integral parts of the pointer-generator network as described in the text.",
        "matched_chunk_entity_name": "Pointer-generator network"
    },
    "image_5": {
        "entity_name": "Table 2",
        "entity_type": "GEO",
        "description": "Table 2 contains ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set.",
        "reason": "The image is a table that presents ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set, which directly aligns with the description of Table 2 in the text information.",
        "matched_chunk_entity_name": "Table 2"
    },
    "image_6": {
        "entity_name": "Table 2",
        "entity_type": "GEO",
        "description": "Table 2 contains ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set.",
        "reason": "The image is a table that presents ROUGE F-1 scores of various abstractive summarization methods on the CNN/Daily Mail test set, which directly corresponds to Table 2 in the text information.",
        "matched_chunk_entity_name": "Table 2"
    },
    "image_7": {
        "entity_name": "Table 3",
        "entity_type": "GEO",
        "description": "Table 3 compares human evaluation results with state-of-the-art methods.",
        "reason": "The image is a table comparing different methods for summarization based on informativity, conciseness, and readability. This aligns with the description of Table 3 in the text information.",
        "matched_chunk_entity_name": "Table 3"
    },
    "image_8": {
        "entity_name": "Table 4",
        "entity_type": "GEO",
        "description": "Table 4 shows the inconsistency rate of the end-to-end trained model with and without inconsistency loss.",
        "reason": "The image is a table labeled 'Table 4: Inconsistency rate of our end-to-end trained model with and without inconsistency loss.' The table content matches the description provided in the text, showing the significant reduction in the average inconsistency rate ($R_{inc}$) when the model is trained with inconsistency loss compared to when it is trained without.",
        "matched_chunk_entity_name": "TABLE 4"
    },
    "image_9": {
        "entity_name": "Inconsistency Loss",
        "entity_type": "EVENT",
        "description": "Inconsistency Loss is a technique used to train the model and reduce inconsistency in generated summaries.",
        "reason": "The image clearly shows two versions of a news article summary, one with inconsistency loss and one without. The text discusses the differences in detail and accuracy between these summaries, highlighting the impact of inconsistency loss on the quality of the generated summaries.",
        "matched_chunk_entity_name": "Inconsistency Loss"
    }
}