{
    "image_1": [
        {
            "entity_name": "IMAGE_1",
            "entity_type": "ORI_IMG",
            "description": "The image is a table labeled 'Table 1: Dataset description' that provides detailed statistics for four different datasets, each related to restaurant and laptop reviews. The table is structured with the following columns: Dataset, Pos (Positive), Neg (Negative), and Neu (Neutral). Each row represents a specific dataset split into train and test sets. The datasets are as follows: D1 includes Restaurant14-Train with 2164 positive, 807 negative, and 637 neutral reviews; Restaurant14-Test with 728 positive, 196 negative, and 196 neutral reviews. D2 includes Laptop14-Train with 994 positive, 870 negative, and 464 neutral reviews; Laptop14-Test with 341 positive, 128 negative, and 169 neutral reviews. D3 includes Restaurant15-Train with 1178 positive, 382 negative, and 50 neutral reviews; Restaurant15-Test with 439 positive, 328 negative, and 35 neutral reviews. D4 includes Restaurant16-Train with 1620 positive, 709 negative, and 88 neutral reviews; Restaurant16-Test with 597 positive, 190 negative, and 38 neutral reviews. The table highlights the distribution of sentiment in each dataset, showing a significant number of positive reviews across all datasets."
        },
        {
            "entity_name": "TABLE",
            "entity_type": "GEO",
            "description": "A table displaying datasets and their respective positive, negative, and neutral classifications for different years and domains."
        },
        {
            "entity_name": "DATASET TYPES",
            "entity_type": "ORGANIZATION",
            "description": "Four types of datasets are listed: Restaurant14, Laptop14, Restaurant15, and Restaurant16."
        },
        {
            "entity_name": "CLASSIFICATION TYPES",
            "entity_type": "ORGANIZATION",
            "description": "Three types of classifications are listed: Positive (Pos), Negative (Neg), and Neutral (Neu)."
        },
        {
            "entity_name": "YEARS",
            "entity_type": "EVENT",
            "description": "The datasets are categorized by the years 2014, 2015, and 2016."
        }
    ],
    "image_2": [
        {
            "entity_name": "IMAGE_2",
            "entity_type": "ORI_IMG",
            "description": "The image is a table that presents the performance of different methods on four datasets (D1, D2, D3, and D4) in terms of accuracy (Acc.) and Macro-F1 scores. The table includes the following rows: 'Tang et al. (2016a)', 'Wang et al. (2016)', 'Tang et al. (2016b)', 'Chen et al. (2017)', 'LSTM', 'LSTM+ATT', 'Ours: PRET', 'Ours: MULT', and 'Ours: PRET+MULT'. Each row contains the Acc. and Macro-F1 values for each dataset. For example, 'Tang et al. (2016a)' has an Acc. of 75.37 and a Macro-F1 of 64.51 for D1, an Acc. of 68.25 and a Macro-F1 of 65.96 for D2, an Acc. of 76.39 and a Macro-F1 of 58.70 for D3, and an Acc. of 82.16 and a Macro-F1 of 54.21 for D4. The highest values are highlighted in bold, such as 'Ours: PRET+MULT' with an Acc. of 79.11 and a Macro-F1 of 69.73 for D1, an Acc. of 71.15 and a Macro-F1 of 67.46 for D2, an Acc. of 81.30 and a Macro-F1 of 68.74 for D3, and an Acc. of 85.58 and a Macro-F1 of 69.76 for D4."
        },
        {
            "entity_name": "TABLE",
            "entity_type": "OBJECT",
            "description": "A table displaying the performance of various methods across four datasets (D1, D2, D3, D4) with metrics Accuracy and Macro-F1. The table includes methods from Tang et al. (2016a), Wang et al. (2016), Tang et al. (2016b), Chen et al. (2017), LSTM, LSTM+ATT, Ours: PRET, Ours: MULT, and Ours: PRET+MULT."
        }
    ],
    "image_3": [
        {
            "entity_name": "IMAGE_3",
            "entity_type": "ORI_IMG",
            "description": "The image is a table labeled 'Table 3: PRET with different transferred layers.' It presents the average accuracies and Macro-F1 scores over 5 runs with random initialization for different settings. The table has six rows and five columns. The first column lists the settings: LSTM only, Embeddings only, Output layer only, Without LSTM, Without embeddings, and Without output layer. The subsequent four columns are labeled D1, D2, D3, and D4, each containing two sub-columns for Acc. (Accuracy) and Macro-F1. The values are as follows: For LSTM only, Acc. and Macro-F1 are 78.09 and 67.85 for D1, 71.04 and 66.80 for D2, 78.95 and 65.30 for D3, and 83.85 and 67.11 for D4. For Embeddings only, the values are 77.12 and 67.19 for D1, 69.12 and 65.06 for D2, 80.13 and 67.04 for D3, and 84.12 and 70.11 for D4. For Output layer only, the values are 76.88 and 66.81 for D1, 69.63 and 66.07 for D2, 78.30 and 64.49 for D3, and 82.55 and 62.83 for D4. For Without LSTM, the values are 77.45 and 67.25 for D1, 69.82 and 66.63 for D2, 80.27 and 68.02 for D3, and 84.80 and 70.27 for D4. For Without embeddings, the values are 77.97 and 67.96 for D1, 70.59 and 67.16 for D2, 79.08 and 65.56 for D3, and 83.94 and 68.79 for D4. For Without output layer, the values are 78.36 and 68.06 for D1, 71.10 and 67.87 for D2, 80.82 and 67.68 for D3, and 84.71 and 70.48 for D4."
        },
        {
            "entity_name": "TABLE",
            "entity_type": "OBJECT",
            "description": "A table displaying the performance of different model settings across four datasets (D1, D2, D3, D4). The table includes columns for 'Settings', 'Acc.', and 'Macro-F1' for each dataset."
        },
        {
            "entity_name": "LSTM ONLY",
            "entity_type": "ORGANIZATION",
            "description": "A model setting where only the LSTM component is used. Performance metrics are 78.09% accuracy and 67.85 Macro-F1 for D1, 71.04% accuracy and 66.80 Macro-F1 for D2, 78.95% accuracy and 65.30 Macro-F1 for D3, and 83.85% accuracy and 67.11 Macro-F1 for D4."
        },
        {
            "entity_name": "EMBEDDINGS ONLY",
            "entity_type": "ORGANIZATION",
            "description": "A model setting where only the embeddings component is used. Performance metrics are 77.12% accuracy and 67.19 Macro-F1 for D1, 69.12% accuracy and 65.06 Macro-F1 for D2, 80.13% accuracy and 67.04 Macro-F1 for D3, and 84.12% accuracy and 70.11 Macro-F1 for D4."
        },
        {
            "entity_name": "OUTPUT LAYER ONLY",
            "entity_type": "ORGANIZATION",
            "description": "A model setting where only the output layer component is used. Performance metrics are 76.88% accuracy and 66.81 Macro-F1 for D1, 69.63% accuracy and 66.07 Macro-F1 for D2, 78.30% accuracy and 64.49 Macro-F1 for D3, and 82.55% accuracy and 62.83 Macro-F1 for D4."
        },
        {
            "entity_name": "WITHOUT LSTM",
            "entity_type": "ORGANIZATION",
            "description": "A model setting where the LSTM component is excluded. Performance metrics are 77.45% accuracy and 67.25 Macro-F1 for D1, 69.82% accuracy and 66.63 Macro-F1 for D2, 80.27% accuracy and 68.02 Macro-F1 for D3, and 84.80% accuracy and 70.27 Macro-F1 for D4."
        },
        {
            "entity_name": "WITHOUT EMBEDDINGS",
            "entity_type": "ORGANIZATION",
            "description": "A model setting where the embeddings component is excluded. Performance metrics are 77.97% accuracy and 67.96 Macro-F1 for D1, 70.59% accuracy and 67.16 Macro-F1 for D2, 79.08% accuracy and 65.56 Macro-F1 for D3, and 83.94% accuracy and 68.79 Macro-F1 for D4."
        },
        {
            "entity_name": "WITHOUT OUTPUT LAYER",
            "entity_type": "ORGANIZATION",
            "description": "A model setting where the output layer component is excluded. Performance metrics are 78.36% accuracy and 68.06 Macro-F1 for D1, 71.10% accuracy and 67.87 Macro-F1 for D2, 80.82% accuracy and 67.68 Macro-F1 for D3, and 84.71% accuracy and 70.48 Macro-F1 for D4."
        }
    ],
    "image_4": [
        {
            "entity_name": "IMAGE_4",
            "entity_type": "ORI_IMG",
            "description": "The image consists of two line graphs, each depicting the performance metrics of different datasets as a function of the percentage of document-level training examples. The top graph shows the accuracy (%) for four datasets labeled D1, D2, D3, and D4. The x-axis represents the percentage of document-level training examples, ranging from 0 to 1, while the y-axis represents the accuracy in percentage, ranging from 65% to 85%. The lines for D1 (blue squares), D2 (orange circles), D3 (gray diamonds), and D4 (red crosses) show an increasing trend with the increase in the percentage of document-level training examples. The bottom graph shows the macro-F1 scores (%) for the same datasets. The x-axis is the same as the top graph, while the y-axis ranges from 60% to 70%. The lines for D1 (blue squares), D2 (orange circles), D3 (gray diamonds), and D4 (red crosses) also show an increasing trend, with some fluctuations. Notably, D3 and D4 exhibit sharp increases in macro-F1 scores when the percentage of document-level training examples changes from 0 to 0.4."
        },
        {
            "entity_name": "GRAPH 1",
            "entity_type": "EVENT",
            "description": "A graph showing the accuracy percentage of four different datasets (D1, D2, D3, D4) as the percentage of document-level training examples increases from 0 to 1. The accuracy for all datasets generally increases with more training data."
        },
        {
            "entity_name": "GRAPH 2",
            "entity_type": "EVENT",
            "description": "A graph displaying the Macro-F1 score percentage of the same four datasets (D1, D2, D3, D4) as the percentage of document-level training examples increases from 0 to 1. The Macro-F1 scores also show an upward trend with more training data."
        }
    ]
}