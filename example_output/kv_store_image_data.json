{
  "image_1": {
    "image_id": 1,
    "image_path": "./example_output/images/image_1.jpg",
    "caption": [],
    "footnote": [],
    "context": "Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is 1 Introduction Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks. deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP. ",
    "chunk_order_index": 0,
    "chunk_id": "chunk-08d62444b992fc41f1eb900a71e0b520",
    "description": "The image is a table labeled 'Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.' The table is structured with four main columns: Task, # neg, # pos, and ratio. Each row represents a different NLP task and contains the following values: \\n- For CoNLL03 NER, the number of negative examples (# neg) is 170K, the number of positive examples (# pos) is 34K, and the ratio is 4.98. \\n- For OntoNotes5.0 NER, the number of negative examples (# neg) is 1.96M, the number of positive examples (# pos) is 239K, and the ratio is 8.18. \\n- For SQuAD 1.1 (Rajpurkar et al., 2016), the number of negative examples (# neg) is 10.3M, the number of positive examples (# pos) is 175K, and the ratio is 55.9. \\n- For SQuAD 2.0 (Rajpurkar et al., 2018), the number of negative examples (# neg) is 15.4M, the number of positive examples (# pos) is 188K, and the ratio is 82.0. \\n- For QUOREF (Dasigi et al., 2019), the number of negative examples (# neg) is 6.52M, the number of positive examples (# pos) is 38.6K, and the ratio is 169. \\nThe table highlights the significant data imbalance in these NLP tasks, with the number of negative examples far exceeding the number of positive examples.",
    "segmentation": false
  },
  "image_2": {
    "image_id": 2,
    "image_path": "./example_output/images/image_2.jpg",
    "caption": [
      "Figure 1: An illustration of derivatives of the four losses. The derivative of DSC approaches zero right after $p$ exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. "
    ],
    "footnote": [],
    "context": "To address this issue, we propose to multiply the soft probability Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F1$ , using a continuous $p$ rather than the binary $\\mathbb{I}(p_{i1}>0.5)$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance. cient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows: $$ \\mathrm{TI}=\\frac{|A\\cap B|}{|A\\cap B|+\\alpha|A\\backslash B|+\\beta|B\\backslash A|} $$ Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\\alpha=\\beta=0.5$ The Tversky loss (TL) is thus given as follows: $$ \\mathrm{TL}=\\frac{1}{N}\\sum_{i}\\left[1-\\frac{p_{i1}y_{i1}+\\gamma}{p_{i1}y_{i1}+\\alpha\\,p_{i1}y_{i0}+\\beta\\,p_{i0}y_{i1}+\\gamma}\\right] $$ 3.4 Self-adjusting Dice Loss Consider a simple case where the dataset consists of only one example $x_{i}$ , which is classified as positive as long as $p_{i1}$ is larger than 0.5. The computation of $F1$ score is actually as follows: $$ \\mathrm{F}1(x_{i})=2\\;{\\frac{\\mathbb{I}(p_{i1}>0.5)y_{i1}}{\\mathbb{I}(p_{i1}>0.5)+y_{i1}}} $$ ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-7796c957ecdc053bf1b6da735b8ebe5f",
    "description": "The image is a line graph illustrating the derivatives of four different loss functions with respect to the probability of the ground-truth label, denoted as $\\\\bar{p}_i$. The x-axis represents the probability of the ground-truth label, ranging from 0 to 1. The y-axis represents the derivatives of the losses, ranging from -2 to 2. Four curves are plotted: \\\\nabla FL($\\\\gamma$=1) in blue, \\\\nabla DL($\\\\gamma$=1) in orange, \\\\nabla TL($\\\\beta$=0.5) in yellow, and \\\\nabla DSC in purple. The curve for \\\\nabla DSC approaches zero right after $\\\\bar{p}$ exceeds 0.5, while the other curves reach 0 only if the probability is exactly 1. This indicates that the derivative of DSC becomes negligible once the probability exceeds 0.5, whereas the other losses continue to push the probability towards 1.",
    "segmentation": false
  },
  "image_3": {
    "image_id": 3,
    "image_path": "./example_output/images/image_3.jpg",
    "caption": [],
    "footnote": [
      "Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4. "
    ],
    "context": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development 4 Experiments In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them. A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\gamma}$ factor, leading the final loss to be $-(1-p)^{\\gamma}\\log p$ . ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-7796c957ecdc053bf1b6da735b8ebe5f",
    "description": "The image is a table labeled 'Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.' The table compares the performance of different models on part-of-speech tagging tasks across three datasets: CTB5, CTB6, and UD1.4. The columns represent the datasets, while the rows represent various models and their corresponding metrics: Precision (Prec.), Recall (Rec.), and F1 score. The models listed are Joint-POS(Sig), Joint-POS(Ens), Lattice-LSTM, BERT-Tagger, BERT+FL, BERT+DL, and BERT+DSC. For example, on the CTB5 dataset, the BERT-Tagger model achieves a Precision of 95.86, Recall of 96.26, and an F1 score of 96.06. The BERT+DSC model performs the best with a Precision of 97.10, Recall of 98.75, and an F1 score of 97.92. The table also includes improvements over the baseline in parentheses, such as (+1.86) for the BERT+DSC model on the CTB5 dataset.",
    "segmentation": false
  },
  "image_4": {
    "image_id": 4,
    "image_path": "./example_output/images/image_4.jpg",
    "caption": [],
    "footnote": [
      "Table 4: Experimental results for English POS datasets. "
    ],
    "context": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development 4 Experiments In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them. A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\gamma}$ factor, leading the final loss to be $-(1-p)^{\\gamma}\\log p$ .  ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-7796c957ecdc053bf1b6da735b8ebe5f",
    "description": "The image is a table labeled 'Table 4: Experimental results for English POS datasets.' The table is divided into two main sections, each representing different datasets: English WSJ and English Tweets. Each section contains rows corresponding to different models and their performance metrics. For the English WSJ dataset, the columns are 'Model,' 'Prec.', 'Rec.', and 'F1.' The models listed include Meta BiLSTM, BERT-Tagger, BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The precision (Prec.), recall (Rec.), and F1 scores are provided for each model. For example, BERT-Tagger has a precision of 99.21, recall of 98.36, and an F1 score of 98.86. The best-performing model in this dataset is BERT-Tagger+DSC with a precision of 99.41, recall of 98.93, and an F1 score of 99.38. For the English Tweets dataset, the same structure is followed. The models listed are FastText+CNN+CRF, BERT-Tagger, BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The precision, recall, and F1 scores are provided for each model. For example, BERT-Tagger has a precision of 92.33, recall of 91.98, and an F1 score of 92.34. The best-performing model in this dataset is BERT-Tagger+DSC with a precision of 92.87, recall of 93.54, and an F1 score of 92.58.",
    "segmentation": false
  },
  "image_5": {
    "image_id": 5,
    "image_path": "./example_output/images/image_5.jpg",
    "caption": [],
    "footnote": [],
    "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition   datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-16c9562e4fc9927ad1d945506d8bd2d2",
    "description": "The image is a table labeled 'English CoNLL 2003' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores are provided for each model. Notably, BERT-MRC+DSC achieves the highest F1 score of 93.33, which is an improvement of +0.29 over BERT-MRC. The table highlights the superior performance of the DSC loss in enhancing the F1 score compared to other losses.",
    "segmentation": false
  },
  "image_6": {
    "image_id": 6,
    "image_path": "./example_output/images/image_6.jpg",
    "caption": [],
    "footnote": [],
    "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition  datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets.  ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-16c9562e4fc9927ad1d945506d8bd2d2",
    "description": "The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision values range from 90.01 to 92.70, the recall values range from 88.35 to 92.56, and the F1 scores range from 88.8 to 92.07. Notably, BERT-MRC+DSC achieves the highest F1 score of 92.07, which is an improvement of +0.96 over BERT-MRC.",
    "segmentation": false
  },
  "image_7": {
    "image_id": 7,
    "image_path": "./example_output/images/image_7.jpg",
    "caption": [],
    "footnote": [
      "Table 5: Experimental results for NER task. "
    ],
    "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets.   ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-16c9562e4fc9927ad1d945506d8bd2d2",
    "description": "The image is a table labeled 'Table 5: Experimental results for NER task.' The table is divided into two main sections, each representing different datasets: Chinese MSRA and Chinese OntoNotes 4.0. Each section contains rows listing various models used for Named Entity Recognition (NER) tasks and their corresponding performance metrics: Precision (Prec.), Recall (Rec.), and F1 score. For the Chinese MSRA dataset, the models listed are Lattice-LSTM, BERT-Tagger, Glyce-BERT, BERT-MRC, BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The highest F1 score is achieved by BERT-MRC+DSC with a value of 96.72. For the Chinese OntoNotes 4.0 dataset, the same models are listed, and the highest F1 score is also achieved by BERT-MRC+DSC with a value of 84.47. The table highlights the performance improvements of the DSC loss over other models, with specific gains indicated in parentheses next to the F1 scores.",
    "segmentation": false
  },
  "image_8": {
    "image_id": 8,
    "image_path": "./example_output/images/image_8.jpg",
    "caption": [],
    "footnote": [],
    "context": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning 4.4 Paraphrase Identification Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1. enables learning bidirectional contexts.  We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019). Baselines We used the following baselines: QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table that presents the experimental results for the machine reading comprehension (MRC) task using different models and datasets. The table is divided into rows and columns. The rows represent different models: QANet, BERT, BERT+FL, BERT+DL, BERT+DSC, XLNet, XLNet+FL, XLNet+DL, and XLNet+DSC. The columns are labeled with the datasets SQuAD v1.1, SQuAD v2.0, and QuoRef, each further divided into EM (Exact Match) and F1 scores. For example, QANet scores 73.6 on EM and 82.7 on F1 for SQuAD v1.1, while BERT scores 84.1 on EM and 90.9 on F1 for the same dataset. The table also shows the performance improvements of the proposed methods over the baseline models. For instance, BERT+DSC achieves 85.34 on EM and 91.97 on F1 for SQuAD v1.1, which is an improvement of +1.24 and +1.07 respectively over BERT. Similarly, XLNet+DSC achieves 89.79 on EM and 95.77 on F1 for SQuAD v1.1, which is an improvement of +0.84 and +1.25 respectively over XLNet. The table highlights the significant performance boost obtained by the proposed DSC loss method on both EM and F1 scores across all datasets.",
    "segmentation": false
  },
  "image_9": {
    "image_id": 9,
    "image_path": "./example_output/images/image_9.jpg",
    "caption": [
      "Table 6: Experimental results for MRC task. "
    ],
    "footnote": [
      "Table 7: Experimental results for PI task. "
    ],
    "context": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning 4.4 Paraphrase Identification Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1. enables learning bidirectional contexts. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019). Baselines We used the following baselines: QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that  ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table labeled 'Table 6: Experimental results for MRC task.' The table compares the performance of different models on two datasets, MRPC and QQP, using the F1 score metric. The rows represent different models and their variations, while the columns show the F1 scores for each dataset. The models listed are BERT (Devlin et al., 2018), BERT+FL, BERT+DL, BERT+DSC, XLNet (Yang et al., 2019), XLNet+FL, XLNet+DL, and XLNet+DSC. For the MRPC dataset, the F1 scores are as follows: BERT has an F1 score of 88.0, BERT+FL has 88.43 (+0.43), BERT+DL has 88.71 (+0.71), and BERT+DSC has 88.92 (+0.92). For the QQP dataset, the F1 scores are: BERT has an F1 score of 91.3, BERT+FL has 91.86 (+0.56), BERT+DL has 91.92 (+0.62), and BERT+DSC has 92.11 (+0.81). For XLNet, the F1 scores are: XLNet has an F1 score of 89.2 for MRPC and 91.8 for QQP, XLNet+FL has 89.25 (+0.05) for MRPC and 92.31 (+0.51) for QQP, XLNet+DL has 89.33 (+0.13) for MRPC and 92.39 (+0.59) for QQP, and XLNet+DSC has 89.78 (+0.58) for MRPC and 92.60 (+0.79) for QQP. The table highlights the performance improvements when additional techniques (FL, DL, DSC) are applied to both BERT and XLNet.",
    "segmentation": false
  },
  "image_10": {
    "image_id": 10,
    "image_path": "./example_output/images/image_10.jpg",
    "caption": [],
    "footnote": [],
    "context": "Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that  We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\\%$ being positive and $50\\%$ being negative. Positive and negative augmentation ( $\\mathbf{\\Psi}+$ positive $\\pmb{\\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. Negative downsampling (- negative)  training examples in the original training set as templates. Then we used Spacy to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia. The augmented set contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. Negative augmentation ( $\\mp$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\\%$ being positive and $79\\%$ being negative. ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table that presents the performance of different models on a dataset with various data augmentation strategies. The table has six columns: 'original', '+ positive', '+ negative', '- negative', and '+ positive & negative'. Each row represents a different model or model configuration. The first row shows the results for the BERT model, with the original accuracy being 91.3%. The subsequent rows show the results for BERT combined with different techniques: FL (Focal Loss), DL (Dynamic Loss), and DSC (Dynamic Sample Contribution). For each model, the table provides the accuracy for the original dataset and the datasets augmented with positive examples, negative examples, down-sampled negative examples, and both positive and negative examples. The values are presented in percentages, with the changes in accuracy relative to the original model shown in parentheses. For example, BERT+FL has an accuracy of 91.86% (+0.56) on the original dataset, 92.64% (+0.37) on the + positive dataset, 90.61% (+0.53) on the + negative dataset, 90.79% (+1.06) on the - negative dataset, and 93.45% (+0.31) on the + positive & negative dataset.",
    "segmentation": false
  },
  "image_11": {
    "image_id": 11,
    "image_path": "./example_output/images/image_11.jpg",
    "caption": [
      "Table 8: The effect of different data augmentation ways for QQP in terms of F1-score. "
    ],
    "footnote": [],
    "context": "DSC achieves the highest F1 score across all datasets. Specially, for $^+$ positive, DSC achieves minor improvements $\\left(+0.05\\,\\mathrm{F}1\\right)$ ) over DL. In contrast, it significantly outperforms DL Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that $^+$ positive outperforms original, and +negative underperforms original. This is in line with our expectation since $^+$ positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances. +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\\%$ being positive and $79\\%$ being negative.   Negative downsampling (- negative) We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\\%$ being positive and $50\\%$ being negative. Positive and negative augmentation ( $\\mathbf{\\Psi}+$ positive $\\pmb{\\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' The table is structured with two main columns: SST-2 and SST-5, each representing different datasets. Under each dataset, there are two sub-columns labeled 'Acc' for accuracy. The rows represent different models used for data augmentation: BERT+CE, BERT+DL, and BERT+DSC. The values in the table are as follows: for SST-2, BERT+CE has an accuracy of 94.90%, BERT+DL has 94.37%, and BERT+DSC has 94.84%. For SST-5, BERT+CE has an accuracy of 55.57%, BERT+DL has 54.63%, and BERT+DSC has 55.19%. The table highlights the performance of different models across the two datasets, with BERT+CE achieving the highest accuracy on both SST-2 and SST-5.",
    "segmentation": false
  },
  "image_12": {
    "image_id": 12,
    "image_path": "./example_output/images/image_12.jpg",
    "caption": [],
    "footnote": [],
    "context": "Bernd Bohnet, Ryan T. McDonald, Gon¸calo Simo˜es, Daniel Andor, Emily Pitler, and Joshua Maynez. 2018. Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2642–2652. References We thank all anonymous reviewers, as well as Qinghong Han, Wei Wu and Jiawei Wu for their comments and suggestions. The work is supported by the National Natural Science Foundation of China (NSFC No. 61625107 and 61751209). Acknowledgement to achieve significant performance boost without changing model architectures.  $\\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha$ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha$ changes in distinct datasets, which shows that the hyperparameters $\\alpha,\\beta$ acturally play an important role in TI. 6 Conclusion In this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (F1 score). Experimental results show that the proposed loss function help Table 10: The effect of hyperparameters in Tversky Index. We set $\\beta=1-\\alpha$ and thus we only list $\\alpha$ here. ",
    "chunk_order_index": 6,
    "chunk_id": "chunk-fbdf9d64766ea51ff2a2526d92ed5d67",
    "description": "The image is a table that presents the effect of hyperparameters in the Tversky Index. The table has three columns: 'α', 'Chinese Onto4.0', and 'English QuoRef'. The first column lists values of α ranging from 0.1 to 0.9 in increments of 0.1. The second column shows corresponding F1 scores for the Chinese Onto4.0 dataset, with values ranging from 80.13 to 84.67. The third column displays F1 scores for the English QuoRef dataset, ranging from 63.23 to 68.44. Notably, the highest F1 score for Chinese Onto4.0 is 84.67 at α = 0.6, while for English QuoRef, the highest F1 score is 68.44 at α = 0.4.",
    "segmentation": false
  }
}