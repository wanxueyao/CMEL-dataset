{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1034/images/image_1.jpg",
        "caption": [
            "Figure 1: Example of the result of our approach on Spanish-English words not included in the dictionary (embeddings are reduced to 2 dimensions for visual clarity). We first project word embeddings into a shared space, and then use the nearest neighbors for word translation. Notice that the word pairs are not perfectly aligned in the shared embedding space, but after word translation we obtain correct alignments. "
        ],
        "footnote": [],
        "context": "Given text in the source and target language, we first independently learn word embedding matrices $X$ and $Y$ in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages 2.2.1 Learning Monolingual Embeddings We consider each in detail. 3. For each word in the source language training data, translate it by finding its nearest neighbor in the shared embedding space ( 2.2.3). 4. Train an NER model using the translated words along with the named entity tags from the English corpus ( 2.2.4). the word embedding alignment using the given dictionary ( 2.2.2). al., 2017). In this work, we limit ourselves to a setting where we have the following resources, making us comparable to other methods such as Mayhew et al. (2017) and Ni et al. (2017): Labeled training data in the source language. Monolingual corpora in both source and target languages. A dictionary, either a small pre-existing one, or one induced by unsupervised methods. 2.2 Method Our method follows the process below: 1. Train separate word embeddings using monolingual corpora using standard embedding training methods ( 2.2.1). 2. Project word embeddings in the two languages into a shared embedding space by optimizing ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-3c526155921c2b169f8ecd91e96d88c6",
        "description": "The image consists of two scatter plots side by side, illustrating the projection of word embeddings from Spanish to English. The left plot shows the initial distribution of word embeddings in a 2-dimensional space, with words labeled in both Spanish (red dots) and English (blue dots). Words such as 'mezclas', 'habilidades', 'trastorno', 'angry', 'encounter', and 'disorder' are scattered across the plot. The right plot depicts the result after projecting these embeddings into a shared space. Here, the words are more closely aligned, indicating improved alignment for translation purposes. A red box highlights the nearest neighbor relationship between 'trastorno' (Spanish) and 'disorder' (English), demonstrating the effectiveness of the projection method. To the right of the plots, a table lists the translations of selected words, including 'disorder' and 'recognize' in English and their Spanish counterparts 'trastorno' and 'reconocer'. The overall layout and content suggest a focus on cross-lingual word embedding alignment for translation tasks.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1034/images/image_2.jpg",
        "caption": [
            "Figure 2: Self-attentive Bi-LSTM-CRF Model "
        ],
        "footnote": [],
        "context": "We describe the model we use to perform NER. We will first describe the basic hierarchical neural CRF tagging model (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2016), 3 NER Model Architecture Instead of directly modeling the shared embedding space (Guo et al., 2015; Zhang et al., 2016; Fang and Cohn, 2017; Ni et al., 2017), we leverage the shared embedding space for word translation. As shown in Figure 1, unaligned word pairs can still be translated correctly with our method, as the embeddings are still closer to the correct translations than the closest incorrect one.  1 shows an example of the embeddings and translations learned with our approach trained on Spanish and English data from the experiments (see $\\S4$ for more details). As shown in the figure, there is usually a noticeable difference between the word embeddings of a word pair in different languages, which is inevitable because different languages have distinct traits and different monolingual data, and as a result it is intrinsically hard to learn a perfect alignment. This indicates that models trained directly on data using the source embeddings may not generalize well to the slightly different embeddings of the target language. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-1946702dacf619e91d3f9408900d55e7",
        "description": "The image is a detailed diagram of a Self-attentive Bi-LSTM-CRF Model used for Named Entity Recognition (NER). The diagram is structured in a hierarchical manner, starting from the bottom with character-level inputs and progressing to the CRF layer at the top. At the bottom, there are characters 'S', 'a', 'n' representing parts of the word 'San'. These characters are fed into a Char Bi-LSTM layer, which processes them sequentially. Above this, there is a Word Embedding layer that takes the word 'flights' as input. This word embedding is then passed through a Word Bi-LSTM layer, which captures the sequential information of the words. The output of the Word Bi-LSTM layer is then fed into a Masked Self-attention mechanism, which allows the model to focus on relevant parts of the input sequence. The output of the self-attention mechanism is combined with global context information and fed into an LSTM Output layer. Finally, the output of this layer is passed to the CRF Layer, which assigns labels to each word. In this case, the labels are 'O', 'O', 'B-LOC', and 'I-LOC', indicating the start and continuation of a location entity. The diagram uses circles and rectangles to represent different layers and arrows to indicate the flow of information. The colors used are primarily orange for the embeddings and outputs, and gray for the layers and labels.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1034/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Our supervised results $86.26\\pm0.40$ $\\overline{{86.40\\pm0.17}}$ $78.16\\pm0.45$ annotated corpus "
        ],
        "context": "Table 1: NER $F_{1}$ scores. ∗Approaches that use more resources than ours (“Wikipedia” means Wikipedia is used not as a monolingual corpus, but to provide external knowledge). †Approaches that use multiple languages for transfer. “Only Eng. data” is the model used in Mayhew et al. (2017) trained on their data translated from English without using Wikipedia and other languages. The “data from Mayhew et al. (2017)” is the same data translated from only English they used. “Id.c.” indicates using identical character strings between the two languages as the seed dictionary. “Adv.” indicates using adversarial training and mutual nearest neighbors to German indicates the same problem, as it is about 8 $F_{1}$ points worse than Spanish and Dutch. Second, these difficulties become more pronounced in the cross-lingual setting, leading to a noisier embedding space alignment, which lowers the quality of BWE-based translation. We believe that this is a problem with all methods using word embeddings. In such cases, more resource-intensive methods may be necessary. 4.2.1 Comparison with Dictionary-Based Translation Table 1 also presents results of a comparison between our proposed BWE translation method and the “cheap translation” baseline of (Mayhew et al., 2017). The size of the dictionaries used by both ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-e482f56d4c89128c81d9ac8c4175a9ea",
        "description": "The image is a table labeled 'Table 1: NER $F_{1}$ scores' that provides comparative results for Named Entity Recognition (NER) across different models and languages. The table is structured with the following columns: Model, Spanish, Dutch, German, and Extra Resources. Each row represents a different model or method used for NER. The rows include various models such as Täckström et al. (2012), Nothman et al. (2013), Tsai et al. (2016), Ni et al. (2017), Mayhew et al. (2017), and our methods. The values in the Spanish, Dutch, and German columns represent the $F_{1}$ scores for each model in these respective languages. For example, Täckström et al. (2012) has an $F_{1}$ score of 59.30 for Spanish, 58.40 for Dutch, and 40.40 for German. Our methods are listed at the bottom with their respective $F_{1}$ scores and standard deviations. The footnote indicates that approaches marked with an asterisk (*) use more resources than ours, and those marked with a dagger (†) use multiple languages for transfer. The context discusses the challenges in cross-lingual settings and the comparison with dictionary-based translation.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1034/images/image_4.jpg",
        "caption": [],
        "footnote": [],
        "context": "In this section, we directly apply our approach to Uyghur, a truly low-resource language with very limited monolingual and parallel resources. We test our model on 199 annotated evaluation documents from the DARPA LORELEI program (the “unsequestered set”) and compare with previously reported results in the cross-lingual setting by Mayhew et al. (2017). Similar to our previous experiments, 4.3 Case Study: Uyghur Table 3: NER $F_{1}$ scores on Uyghur. ∗Approaches using language-specific features and resources (“Wikipedia” means Wikipedia is used not as a monolingual corpus, but to provide external knowledge). †Approaches that transfer from multiple languages and use language-specific techniques.  target language character sequences. The three variants are compared in Table 2. The “common space” variant performs the worst by a large margin, confirming our hypothesis that discrepancy between the two embedding spaces harms the model’s ability to generalize. From the comparison between the “replace” and “translation,” we observe that having access to the target language’s character sequence helps performance, especially for German, perhaps due in part to its capitalization patterns, which differ from English. In this case, we have to lower-case all the words for character inputs in order to prevent the model from overfitting the English capitalization pattern. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-e482f56d4c89128c81d9ac8c4175a9ea",
        "description": "The image is a table that presents NER (Named Entity Recognition) F1 scores for different models applied to Spanish, Dutch, and German languages. The table has four rows and four columns. The first row serves as the header, listing the languages: 'Spanish', 'Dutch', and 'German'. The subsequent rows are labeled 'Common space', 'Replace', and 'Translation'. Each cell contains a numerical value representing the F1 score along with its standard deviation. Specifically, for Spanish, the scores are 65.40 ± 1.22 for 'Common space', 68.21 ± 1.22 for 'Replace', and 69.21 ± 0.95 for 'Translation'. For Dutch, the scores are 66.15 ± 1.62 for 'Common space', 69.37 ± 1.33 for 'Replace', and 69.39 ± 1.21 for 'Translation'. For German, the scores are 43.73 ± 0.94 for 'Common space', 48.59 ± 1.21 for 'Replace', and 53.94 ± 0.66 for 'Translation'. The highest scores for each language are highlighted in bold.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1034/images/image_5.jpg",
        "caption": [
            "Table 2: Comparison of different ways of using bilingual word embeddings, within our method (NER $F_{1}$ ). ",
            "Uyghur Unsequestered Set Extra Resources "
        ],
        "footnote": [],
        "context": "In this section, we directly apply our approach to Uyghur, a truly low-resource language with very limited monolingual and parallel resources. We test our model on 199 annotated evaluation documents from the DARPA LORELEI program (the “unsequestered set”) and compare with previously reported results in the cross-lingual setting by Mayhew et al. (2017). Similar to our previous experiments, 4.3 Case Study: Uyghur Table 3: NER $F_{1}$ scores on Uyghur. ∗Approaches using language-specific features and resources (“Wikipedia” means Wikipedia is used not as a monolingual corpus, but to provide external knowledge). †Approaches that transfer from multiple languages and use language-specific techniques. target language character sequences. The three variants are compared in Table 2. The “common space” variant performs the worst by a large margin, confirming our hypothesis that discrepancy between the two embedding spaces harms the model’s ability to generalize. From the comparison between the “replace” and “translation,” we observe that having access to the target language’s character sequence helps performance, especially for German, perhaps due in part to its capitalization patterns, which differ from English. In this case, we have to lower-case all the words for character inputs in order to prevent the model from overfitting the English capitalization pattern.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-e482f56d4c89128c81d9ac8c4175a9ea",
        "description": "The image is a table labeled 'Table 2: Comparison of different ways of using bilingual word embeddings, within our method (NER $F_{1}$ )'. The table compares various models and their performance on the Uyghur Unsequestered Set. The columns are as follows: Model, Uyghur Unsequestered Set, and Extra Resources. The rows list different models and their corresponding scores. The first row shows Mayhew et al. (2017) with a score of 51.32 and extra resources listed as Wikipedia, 100K dict. The second row shows Mayhew et al. (2017) (only Eng. data) with a score of 27.20 and the same extra resources. The third row lists BWET with a score of 25.73 ± 0.89 and extra resources listed as 5K dict. The fourth row lists BWET + self-att. with a score of 26.38 ± 0.34 and the same extra resources. The fifth row lists BWET on data from Mayhew et al. (2017) with a score of 30.20 ± 0.98 and extra resources listed as Wikipedia, 100K dict. The sixth row lists BWET + self-att. on data from Mayhew et al. (2017) with a score of 30.68 ± 0.45 and the same extra resources. The seventh row lists Combined (see text) with a score of 31.61 ± 0.46 and extra resources listed as Wikipedia, 100K dict., 5K dict. The eighth row lists Combined + self-att. with a score of 32.09 ± 0.61 and the same extra resources.",
        "segmentation": false
    }
}