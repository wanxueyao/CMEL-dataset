{
    "chunk-04cdbdac133e6bf24d195a81fabc28a2": [
        {
            "entity_name": "Linked WikiText2",
            "entity_type": "DATASET",
            "description": "Linked WikiText2 is a dataset of annotated text aligned to the Wikidata knowledge graph, used for training and evaluating the knowledge graph language model (KGLM). It is a corpus whose contents closely match the popular WikiText-2 benchmark, allowing comparisons against existing language models.",
            "source_entities": [
                "LINKED WIKITEXT2",
                "WIKITEXT-2"
            ]
        },
        {
            "entity_name": "KGLM",
            "entity_type": "EVENT",
            "description": "KGLM (Knowledge Graph Language Model) is a neural language model introduced in the paper that has mechanisms for selecting and copying facts from a knowledge graph relevant to the context. It enables the model to render information it has never seen before and generate out-of-vocabulary tokens, outperforming other language models in generating facts.",
            "source_entities": [
                "KGLM",
                "AWD-LSTM"
            ]
        }
    ],
    "chunk-3ded7fc44093a7d42cdcd18d1e6016b3": null,
    "chunk-302c5b0c3b9bde5d7d7b3867e7ce7577": [
        {
            "entity_name": "$\\kappa G_{<T}$ ($\\mathcal{K}\\mathcal{G}_{<T}$)",
            "entity_type": "CONCEPT",
            "description": "$\\kappa G_{<T}$ (or $\\mathcal{K}\\mathcal{G}_{<T}$) represents a subset of the knowledge graph up to time $t$, containing entities and relations used for reference.",
            "source_entities": [
                "$\\KAPPA G_{<T}$",
                "$\\MATHCAL{K}\\MATHCAL{G}_{<T}$"
            ]
        },
        {
            "entity_name": "$\\mathcal{E}$ ($\\MATHCAL{E}$)",
            "entity_type": "CONCEPT",
            "description": "$\\mathcal{E}$ is the set of all entities in the knowledge graph, from which upcoming entities are chosen.",
            "source_entities": [
                "$\\MATHCAL{E}$",
                "$\\MATHCAL{E}_{<T}$"
            ]
        },
        {
            "entity_name": "$\\mathcal{E}_{<t}$ ($\\MATHCAL{E}_{<T}$)",
            "entity_type": "CONCEPT",
            "description": "$\\mathcal{E}_{<t}$ represents the set of entities known up to time $t$, used for choosing parent entities and relations.",
            "source_entities": [
                "$\\MATHCAL{E}_{<T}",
                "$\\MATHCAL{E}_{<t}$"
            ]
        },
        {
            "entity_name": "$e_{t}$ ($E_{T}$)",
            "entity_type": "CONCEPT",
            "description": "$e_{t}$ represents the upcoming entity to be chosen from the set of all entities $\\mathcal{E}$, based on the model's conditions.",
            "source_entities": [
                "$E_{T}",
                "$\\MATHCAL{E}_{<(T+1)}$"
            ]
        },
        {
            "entity_name": "$p_{t}$ ($P_{T}$)",
            "entity_type": "CONCEPT",
            "description": "$p_{t}$ is the parent entity chosen from $\\mathcal{E}_{<t}$, used in the process of rendering factual relations.",
            "source_entities": [
                "$P_{T}",
                "$\\MATHCAL{E}_{<(T+1)}$"
            ]
        },
        {
            "entity_name": "$r_{t}$ ($R_{T}$)",
            "entity_type": "CONCEPT",
            "description": "$r_{t}$ represents the factual relation chosen from $\\mathcal{K}\\mathcal{G}_{<t}$, connecting a parent entity to a tail entity.",
            "source_entities": [
                "$R_{T}"
            ]
        },
        {
            "entity_name": "$x_{t}$ ($X_{T}$)",
            "entity_type": "CONCEPT",
            "description": "$x_{t}$ is the token generated by the model, conditioned on the chosen entity $e_{t}$, potentially using one of $e_{t}$'s aliases.",
            "source_entities": [
                "$X_{T}"
            ]
        }
    ],
    "chunk-6f4d1e0804a58ae782ff0bab0cf36bd8": [
        {
            "entity_name": "Linked WikiText-2",
            "entity_type": "DATASET",
            "description": "Linked WikiText-2 is a dataset that links articles from the WikiText-2 language modeling corpus to the Wikidata knowledge graph, providing a resource for training language models that incorporate factual knowledge from a knowledge graph.",
            "source_entities": [
                "LINKED WIKITEXT-2",
                "LINKED WIKITEXT-2 DATASET"
            ]
        },
        {
            "entity_name": "WikiText-2",
            "entity_type": "DATASET",
            "description": "WikiText-2 is a language modeling corpus that forms the basis of the Linked WikiText-2 dataset, consisting of articles that are also linked to the Wikidata knowledge graph.",
            "source_entities": [
                "WIKITEXT-2"
            ]
        },
        {
            "entity_name": "Wikidata",
            "entity_type": "ORGANIZATION",
            "description": "Wikidata is a knowledge base that provides structured data for entities, used in the Linked WikiText-2 dataset to link text to factual knowledge and as the source of many facts in Wikipedia.",
            "source_entities": [
                "WIKIDATA"
            ]
        },
        {
            "entity_name": "Vrandeˇci´c and Krötzsch",
            "entity_type": "PERSON",
            "description": "Vrandeˇci´c and Krötzsch are associated with the development of Wikidata, as referenced in the text, and their work contributes to the knowledge base used in the Linked WikiText-2 dataset.",
            "source_entities": [
                "VRANDEˇCI´C",
                "KRÖTZSCH"
            ]
        },
        {
            "entity_name": "Stanford CoreNLP",
            "entity_type": "TECHNOLOGY",
            "description": "Stanford CoreNLP is used for identifying coreferences in the text, as mentioned in the process of entity linking within the Linked WikiText-2 dataset, helping to cover pronouns, nominals, and other tokens missed by the linker.",
            "source_entities": [
                "STANFORD CORENLP"
            ]
        },
        {
            "entity_name": "neural-el",
            "entity_type": "TECHNOLOGY",
            "description": "neural-el is an entity linker used to identify additional links to Wikidata in the Linked WikiText-2 dataset, aiding in the process of entity linking by finding more mentions of entities in the text.",
            "source_entities": [
                "NEURAL-EL"
            ]
        },
        {
            "entity_name": "Game Boy",
            "entity_type": "GEO",
            "description": "Game Boy is mentioned as a platform for Super Mario Land and as manufactured by Nintendo, with Wikidata identifiers, indicating its role in the Linked WikiText-2 dataset as an entity with multiple parent and relation annotations.",
            "source_entities": [
                "GAME BOY"
            ]
        },
        {
            "entity_name": "Entity Linking",
            "entity_type": "EVENT",
            "description": "Entity linking is a process used in the Linked WikiText-2 dataset to identify entity mentions within the text, starting with human-provided links and using tools like neural-el and Stanford CoreNLP to capture all mentions.",
            "source_entities": [
                "ENTITY LINKING",
                "HUMAN-PROVIDED LINKS"
            ]
        },
        {
            "entity_name": "Relation Annotations",
            "entity_type": "EVENT",
            "description": "Relation annotations are part of the process in the Linked WikiText-2 dataset to create a generative story for entities by identifying related entities in Wikidata and linking them to the text as facts.",
            "source_entities": [
                "RELATION ANNOTATIONS",
                "LOCAL KNOWLEDGE GRAPH"
            ]
        },
        {
            "entity_name": "Copy Mechanism",
            "entity_type": "TECHNOLOGY",
            "description": "The copy mechanism is used in the Linked WikiText-2 dataset to obtain probabilities for words in the alias vocabulary, employing LSTM to encode token sequences comprising each alias.",
            "source_entities": [
                "COPY MECHANISM",
                "LSTM"
            ]
        }
    ],
    "chunk-2f06c69af9c4d4b92c28cab88c6fe605": null,
    "chunk-cb506e0b6203718188dec65c9d6b31fc": null,
    "chunk-b7ebeb4e41946712146a875868639d42": [
        {
            "entity_name": "KGLM",
            "entity_type": "ORGANIZATION",
            "description": "KGLM, or Knowledge Graph-based Language Model, is a model that leverages knowledge graphs for accurate language modeling and outperforms other models in perplexity. It is directly controllable via modifications to the KG and is competitive with models trained on much larger data sets, producing factual completions that require specific knowledge. KGLM variants include Oracle KGLM, which is given the correct entity annotation for $X$, and NEL KGLM, which uses a discriminative model combined with an NEL entity linker to produce entity annotations.",
            "source_entities": [
                "KGLM",
                "ORACLE KGLM",
                "NEL KGLM"
            ]
        },
        {
            "entity_name": "GPT-2",
            "entity_type": "ORGANIZATION",
            "description": "GPT-2 is a state-of-the-art language model trained on the WebText corpus, which contains over 8 million documents. It is used for comparison with KGLM in terms of fact completion. GPT-2 tends to produce very common and general tokens, such as popular cities for birthplaces, and is not as effective at completing facts for more-popular entities while using more-generic tokens.",
            "source_entities": [
                "GPT-2",
                "RADFORD ET AL., 2019"
            ]
        },
        {
            "entity_name": "ENTITYNLM",
            "entity_type": "ORGANIZATION",
            "description": "ENTITYNLM, or Entity-based Language Model, is a model that improves a language model's ability to track entities by jointly modeling named entity recognition and coreference. It is one of the existing knowledge-based language models that inspired the development of KGLM.",
            "source_entities": [
                "ENTITYNLM",
                "JI ET AL., 2017"
            ]
        },
        {
            "entity_name": "NKLM",
            "entity_type": "ORGANIZATION",
            "description": "NKLM, or Neural Knowledge Language Model, is an existing knowledge-based language model that inspired the development of KGLM. It is one of the models that draws inspiration for the work on KGLM.",
            "source_entities": [
                "NKLM",
                "AHN ET AL., 2016"
            ]
        }
    ],
    "chunk-95e7fe1a0d66b2b7adcd1f58eda63e07": [
        {
            "entity_name": "KGLM",
            "entity_type": "ORGANIZATION",
            "description": "KGLM, or the knowledge graph language model, is a neural language model proposed in this work that can access an external source of facts encoded as a knowledge graph to generate text. It is designed to handle rare tokens and facts on a broad domain of topics, with an emphasis on improving perplexity. The implementation of KGLM is available on GitHub under the username rloganiv.",
            "source_entities": [
                "KGLM",
                "rloganiv"
            ]
        },
        {
            "entity_name": "Linked WikiText2",
            "entity_type": "EVENT",
            "description": "Linked WikiText2 is a dataset where text has been aligned to facts in the knowledge graph, allowing for efficient training of the KGLM model. It is freely available for download and was created to support the training and evaluation of the KGLM model.",
            "source_entities": [
                "LINKED WIKITEXT2",
                "WIKITEXT-2"
            ]
        },
        {
            "entity_name": "AWDLSTM",
            "entity_type": "ORGANIZATION",
            "description": "AWDLSTM is a model that has been improved upon to enhance performance on the WikiText2 dataset. It is used as a benchmark in this work, and the authors propose that combining KGLM with these improvements could yield further enhancements.",
            "source_entities": [
                "AWDLSTM",
                "GONG ET AL., 2018",
                "YANG ET AL., 2018",
                "KRAUSE ET AL., 2018"
            ]
        }
    ]
}