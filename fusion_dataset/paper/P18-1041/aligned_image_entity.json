{
    "image_1": {
        "entity_name": "Table 1: Comparisons of CNN, LSTM and SWEM architectures.",
        "entity_type": "DOCUMENT",
        "description": "The table provides a comparison of the CNN, LSTM, and SWEM models in terms of parameters, computational complexity, and sequential operations.",
        "reason": "The image is a table labeled 'Table 1: Comparisons of CNN, LSTM and SWEM architectures.' which matches the description provided in the text. The table contains information about the parameters, complexity, and sequential operations for each model, aligning with the content described in the text.",
        "matched_chunk_entity_name": "no match"
    },
    "image_2": {
        "entity_name": "Table 2",
        "entity_type": "CONCEPT",
        "description": "SWEM-concat is a variant of the SWEM model that concatenates the features from SWEM-aver and SWEM-max to form sentence embeddings.",
        "reason": "The image presents a table comparing various models on different datasets for document categorization tasks. The SWEM-concat model achieves the highest scores in most categories, indicating its superior performance compared to other architectures. This aligns with the text information which highlights the strong performance of the SWEM-concat model.",
        "matched_chunk_entity_name": "Table 2"
    },
    "image_3": {
        "entity_name": "Table 3",
        "entity_type": "DOCUMENT",
        "description": "Table 3 shows the top five words with the largest values in a given word-embedding dimension.",
        "reason": "The image is a table labeled 'Table 3' which matches the description provided in the text. The table contains columns for different topics and lists words associated with each topic, which aligns with the content of Table 3 mentioned in the text.",
        "matched_chunk_entity_name": "Table 3"
    },
    "image_4": {
        "entity_name": "Table 4",
        "entity_type": "DOCUMENT",
        "description": "Table 4 contains information on speed and parameters on the Yahoo! Answer dataset.",
        "reason": "The image is a table labeled 'Table 4: Speed & Parameters on Yahoo! Answer dataset.' which directly corresponds to the entity 'Table 4' in the text. The table compares three different models: CNN, LSTM, and SWEM, in terms of their parameters and speed, which aligns with the content of Table 4 mentioned in the text.",
        "matched_chunk_entity_name": "Table 4"
    },
    "image_5":{
        "entity_name": "Figure 1",
        "entity_type": "DOCUMENT",
        "description": "Figure 1 is a reference to a figure in the text that shows histograms for learned word embeddings of SWEM-max and GloVe embeddings for the same vocabulary.",
        "reason": "Human check result.",
        "matched_chunk_entity_name": "Figure 1"
    },
    "image_6": {
        "entity_name": "Table 5",
        "entity_type": "DOCUMENT",
        "description": "Performance of different models on matching natural language sentences. Results with â‡¤are for Bidirectional LSTM, reported in Williams et al. (2017). Our reported results on MultiNLI are only trained MultiNLI training set (without training data from SNLI). For MSRP dataset, we follow the setup in Hu et al. (2014) and do not use any additional features.",
        "reason": "The image is a table that presents the performance of different models on matching natural language sentences across various datasets. This matches the description of Table 5 in the text information.",
        "matched_chunk_entity_name": "no match"
    },
    "image_7": {
        "entity_name": "Table 6",
        "entity_type": "EVENT",
        "description": "Table 6 presents the test accuracy for LSTM model trained on original and shuffled training sets, indicating the importance of word-order information.",
        "reason": "The image is a table labeled 'Table 6: Test accuracy for LSTM model trained on original/shuffled training set.' The content of the table matches the description provided in the text, showing the test accuracies for three distinct datasets (Yahoo, Yelp P., and SNLI) with both original and shuffled training sets. This directly corresponds to the entity 'Table 6' in the nearby entities list.",
        "matched_chunk_entity_name": "Table 6"
    },
    "image_8": {
        "entity_name": "Table 7",
        "entity_type": "GEO",
        "description": "Yelp Polarity is a dataset used to evaluate the performance of LSTM and SWEM models, particularly for sentiment analysis.",
        "reason": "The image contains a table with positive and negative reviews, which aligns with the Yelp Polarity dataset used for sentiment analysis in the text. The content of the reviews matches the description of the Yelp Polarity dataset.",
        "matched_chunk_entity_name": "Table 7"
    },
    "image_9": {
        "entity_name": "Table 8",
        "entity_type": "EVENT",
        "description": "Table 8 presents test accuracies with different compositional functions on short sentence classifications, comparing SWEM with CNN/LSTM.",
        "reason": "The image clearly shows a table labeled 'Table 8: Test accuracies with different compositional functions on (short) sentence classifications.' The content of the table matches the description provided in the text information.",
        "matched_chunk_entity_name": "TABLE 8"
    },
    "image_10": {
        "entity_name": "Figure 2: Performance of subspace training",
        "entity_type": "EVENT",
        "description": "Subspace training is a method used to measure model complexity in text classification problems, as discussed in Section 5.1.",
        "reason": "The image depicts the performance of two models, SWEM and CNN, under subspace training with varying dimensions. This directly aligns with the description of subspace training in the text, which measures model complexity by constraining the optimization of trainable parameters in a low-dimensional subspace.",
        "matched_chunk_entity_name": "no match/Subspace training"
    },
    "image_11": {
        "entity_name": "Figure 2: Performance of subspace training",
        "entity_type": "EVENT",
        "description": "Subspace training is a method used to measure model complexity in text classification problems, as discussed in Section 5.1.",
        "reason": "The image depicts the performance of two models, SWEM and CNN, under subspace training, which aligns with the description of subspace training in the text. The graphs show how the accuracy of these models changes with different subspace dimensions d, which is a key aspect of subspace training.",
        "matched_chunk_entity_name": "no match/SUBSPACE TRAINING"
    }
}