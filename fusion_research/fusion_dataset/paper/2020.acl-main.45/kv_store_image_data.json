{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is 1 Introduction Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks. deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-eca7c8b51cdc6605b35d4d74e1abbeca",
        "description": "The image is a table labeled 'Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.' The table is structured with four main columns: Task, # neg, # pos, and ratio. Each row represents a specific NLP task and contains the following values: 'CoNLL03 NER' has 170K negative examples and 34K positive examples, with a ratio of 4.98; 'OntoNotes5.0 NER' has 1.96M negative examples and 239K positive examples, with a ratio of 8.18; 'SQuAD 1.1 (Rajpurkar et al., 2016)' has 10.3M negative examples and 175K positive examples, with a ratio of 55.9; 'SQuAD 2.0 (Rajpurkar et al., 2018)' has 15.4M negative examples and 188K positive examples, with a ratio of 82.0; 'QUOREF (Dasigi et al., 2019)' has 6.52M negative examples and 38.6K positive examples, with a ratio of 169. The table highlights the significant data imbalance across various NLP tasks, with the number of negative examples far exceeding the number of positive examples in each case.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_2.jpg",
        "caption": [
            "Figure 1: An illustration of derivatives of the four losses. The derivative of DSC approaches zero right after $p$ exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. "
        ],
        "footnote": [],
        "context": "To address this issue, we propose to multiply the soft probability Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F1$ , using a continuous $p$ rather than the binary $\\mathbb{I}(p_{i1}>0.5)$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance. cient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows: $$ \\mathrm{TI}=\\frac{|A\\cap B|}{|A\\cap B|+\\alpha|A\\backslash B|+\\beta|B\\backslash A|} $$ Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\\alpha=\\beta=0.5$ The Tversky loss (TL) is thus given as follows: $$ \\mathrm{TL}=\\frac{1}{N}\\sum_{i}\\left[1-\\frac{p_{i1}y_{i1}+\\gamma}{p_{i1}y_{i1}+\\alpha\\,p_{i1}y_{i0}+\\beta\\,p_{i0}y_{i1}+\\gamma}\\right] $$ 3.4 Self-adjusting Dice Loss Consider a simple case where the dataset consists of only one example $x_{i}$ , which is classified as positive as long as $p_{i1}$ is larger than 0.5. The computation of $F1$ score is actually as follows: $$ \\mathrm{F}1(x_{i})=2\\;{\\frac{\\mathbb{I}(p_{i1}>0.5)y_{i1}}{\\mathbb{I}(p_{i1}>0.5)+y_{i1}}} $$ ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-9d638435649353ad7eb68149e9db8dcc",
        "description": "The image is a line graph illustrating the derivatives of four different loss functions with respect to the probability of the ground-truth label, denoted as $\\\\bar{p}_i$. The x-axis represents the probability of the ground-truth label, ranging from 0 to 1. The y-axis represents the derivatives of the losses, ranging from -2 to 2. Four curves are plotted: \\\\nabla FL($\\\\gamma$=1) in blue, \\\\nabla DL($\\\\gamma$=1) in orange, \\\\nabla TL($\\\\beta$=0.5) in yellow, and \\\\nabla DSC in purple. The curve for \\\\nabla DSC approaches zero right after $\\\\bar{p}$ exceeds 0.5, while the other curves reach zero only if the probability is exactly 1. This indicates that the derivative of DSC will push $\\\\bar{p}$ towards 0.5, whereas the other losses will push $\\\\bar{p}$ towards 1.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4. "
        ],
        "context": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development 4 Experiments In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them. A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\gamma}$ factor, leading the final loss to be $-(1-p)^{\\gamma}\\log p$ . ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-9d638435649353ad7eb68149e9db8dcc",
        "description": "The image is a table labeled 'Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.' The table compares the performance of different models on part-of-speech tagging tasks across three datasets: CTB5, CTB6, and UD1.4. The columns represent the datasets, while the rows represent various models and their corresponding metrics: Precision (Prec.), Recall (Rec.), and F1 score. The models listed are Joint-POS(Sig), Joint-POS(Ens), Lattice-LSTM, BERT-Tagger, BERT+FL, BERT+DL, and BERT+DSC. For example, on the CTB5 dataset, the BERT-Tagger model achieves a Precision of 95.86, Recall of 96.26, and an F1 score of 96.06. The BERT+DSC model performs the best with a Precision of 97.10, Recall of 98.75, and an F1 score of 97.92. The table also includes improvements over the baseline in parentheses, such as (+1.86) for the BERT+DSC model on the CTB5 dataset.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 4: Experimental results for English POS datasets. "
        ],
        "context": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development 4 Experiments In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them. A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\gamma}$ factor, leading the final loss to be $-(1-p)^{\\gamma}\\log p$ .  ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-9d638435649353ad7eb68149e9db8dcc",
        "description": "The image is a table labeled 'Table 4: Experimental results for English POS datasets.' The table is divided into two main sections, each representing different datasets: English WSJ and English Tweets. Each section contains rows corresponding to different models used for part-of-speech tagging. The columns in the table are 'Model,' 'Prec.' (Precision), 'Rec.' (Recall), and 'F1' (F1 Score). For the English WSJ dataset, the models listed are Meta BiLSTM, BERT-Tagger, BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The precision, recall, and F1 scores for these models are as follows: Meta BiLSTM has an F1 score of 98.23; BERT-Tagger has a precision of 99.21, recall of 98.36, and F1 score of 98.86; BERT-Tagger+FL has a precision of 98.36, recall of 98.97, and F1 score of 98.88 (+0.02); BERT-Tagger+DL has a precision of 99.34, recall of 98.22, and F1 score of 98.91 (+0.05); BERT-Tagger+DSC has a precision of 99.41, recall of 98.93, and F1 score of 99.38 (+0.52). For the English Tweets dataset, the models listed are FastText+CNN+CRF, BERT-Tagger, BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The precision, recall, and F1 scores for these models are as follows: FastText+CNN+CRF has an F1 score of 91.78; BERT-Tagger has a precision of 92.33, recall of 91.98, and F1 score of 92.34; BERT-Tagger+FL has a precision of 91.24, recall of 93.22, and F1 score of 92.47 (+0.13); BERT-Tagger+DL has a precision of 91.44, recall of 92.88, and F1 score of 92.52 (+0.18); BERT-Tagger+DSC has a precision of 92.87, recall of 93.54, and F1 score of 92.58 (+0.24).",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition   datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-6dd1be559a398018f09dad69b307eede",
        "description": "The image is a table labeled 'English CoNLL 2003' that provides performance metrics for various models on the English CoNLL 2003 dataset. The table is structured with four main columns: Model, Prec., Rec., and F1. Each row represents a different model and its corresponding performance metrics. The models listed are ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision (Prec.), recall (Rec.), and F1 scores are provided for each model. For example, ELMo has an F1 score of 92.22, CVT has an F1 score of 92.6, BERT-Tagger has an F1 score of 92.8, and BERT-MRC has a precision of 92.33, recall of 94.61, and F1 score of 93.04. The best-performing model is BERT-MRC+DSC with a precision of 93.41, recall of 93.25, and F1 score of 93.33, which is an improvement of +0.29 over BERT-MRC.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition  datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets.  ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-6dd1be559a398018f09dad69b307eede",
        "description": "The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the English OntoNotes 5.0 dataset. The table is structured with four main columns: Model, Prec., Rec., and F1. Each row represents a different model and its corresponding precision (Prec.), recall (Rec.), and F1 score. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores are as follows: CVT has an F1 score of 88.8; BERT-Tagger has a precision of 90.01, recall of 88.35, and F1 score of 89.16; BERT-MRC has a precision of 92.98, recall of 89.95, and F1 score of 91.11; BERT-MRC+FL has a precision of 90.13, recall of 92.34, and F1 score of 91.22 (+0.11); BERT-MRC+DL has a precision of 91.70, recall of 92.06, and F1 score of 91.88 (+0.77); BERT-MRC+DSC has a precision of 91.59, recall of 92.56, and F1 score of 92.07 (+0.96). The table highlights the performance improvements of the models with additional loss functions compared to the baseline.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_7.jpg",
        "caption": [],
        "footnote": [
            "Table 5: Experimental results for NER task. "
        ],
        "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets.   ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-6dd1be559a398018f09dad69b307eede",
        "description": "The image is a table labeled 'Table 5: Experimental results for NER task.' The table is divided into two main sections: Chinese MSRA and Chinese OntoNotes 4.0. Each section contains a list of models with their corresponding precision (Prec.), recall (Rec.), and F1 scores. For the Chinese MSRA dataset, the models listed are Lattice-LSTM (Zhang and Yang, 2018), BERT-Tagger (Devlin et al., 2018), Glyce-BERT (Wu et al., 2019), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores for these models range from 93.57 to 96.67, 92.79 to 96.77, and 93.18 to 96.72 respectively. For the Chinese OntoNotes 4.0 dataset, the models listed are the same as above. The precision, recall, and F1 scores for these models range from 76.35 to 84.22, 71.56 to 84.72, and 73.88 to 84.47 respectively. The table highlights the performance improvements of the DSC loss over other models, with significant gains in F1 scores.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_8.jpg",
        "caption": [],
        "footnote": [],
        "context": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning 4.4 Paraphrase Identification Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1. enables learning bidirectional contexts.  We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019). Baselines We used the following baselines: QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ba8731394946285e9c56ecf440d2094d",
        "description": "The image is a table that presents the experimental results for the machine reading comprehension (MRC) task using different models and datasets. The table is divided into rows and columns. The first column lists the models: QANet, BERT, BERT+FL, BERT+DL, BERT+DSC, XLNet, XLNet+FL, XLNet+DL, and XLNet+DSC. The subsequent columns are labeled with the datasets SQuAD v1.1, SQuAD v2.0, and QuoRef, each further divided into EM (Exact Match) and F1 score metrics. For example, QANet scores 73.6 on EM and 82.7 on F1 for SQuAD v1.1. BERT scores 84.1 on EM and 90.9 on F1 for SQuAD v1.1. BERT+DSC achieves 85.34 on EM and 91.97 on F1 for SQuAD v1.1. XLNet scores 88.95 on EM and 94.52 on F1 for SQuAD v1.1. XLNet+DSC achieves 89.79 on EM and 95.77 on F1 for SQuAD v1.1. The table highlights the performance boost obtained by the proposed DSC loss over BERT and XLNet across all datasets and metrics.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_9.jpg",
        "caption": [
            "Table 6: Experimental results for MRC task. "
        ],
        "footnote": [
            "Table 7: Experimental results for PI task. "
        ],
        "context": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning 4.4 Paraphrase Identification Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1. enables learning bidirectional contexts. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019). Baselines We used the following baselines: QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ba8731394946285e9c56ecf440d2094d",
        "description": "The image is a table labeled 'Table 6: Experimental results for MRC task.' The table compares the performance of different models on two tasks, MRPC and QQP, using the F1 score as the metric. The rows represent different models and their variations, while the columns show the F1 scores for each task. The models listed are BERT (Devlin et al., 2018), BERT+FL, BERT+DL, BERT+DSC, XLNet (Yang et al., 2019), XLNet+FL, XLNet+DL, and XLNet+DSC. For the MRPC task, the F1 scores are as follows: BERT has an F1 score of 88.0, BERT+FL has 88.43 (+0.43), BERT+DL has 88.71 (+0.71), and BERT+DSC has 88.92 (+0.92). For the QQP task, the F1 scores are: BERT has an F1 score of 91.3, BERT+FL has 91.86 (+0.56), BERT+DL has 91.92 (+0.62), and BERT+DSC has 92.11 (+0.81). For XLNet, the F1 scores are: XLNet has an F1 score of 89.2 for MRPC and 91.8 for QQP, XLNet+FL has 89.25 (+0.05) for MRPC and 92.31 (+0.51) for QQP, XLNet+DL has 89.33 (+0.13) for MRPC and 92.39 (+0.59) for QQP, and XLNet+DSC has 89.78 (+0.58) for MRPC and 92.60 (+0.79) for QQP. The table highlights the performance boost obtained by adding different loss functions to BERT and XLNet.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_10.jpg",
        "caption": [],
        "footnote": [],
        "context": "Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that  We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\\%$ being positive and $50\\%$ being negative. Positive and negative augmentation ( $\\mathbf{\\Psi}+$ positive $\\pmb{\\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. Negative downsampling (- negative)  training examples in the original training set as templates. Then we used Spacy to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia. The augmented set contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. Negative augmentation ( $\\mp$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\\%$ being positive and $79\\%$ being negative. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ba8731394946285e9c56ecf440d2094d",
        "description": "The image is a table that presents the performance of different models on a dataset with various data augmentation strategies. The table has six columns: 'original', '+ positive', '+ negative', '- negative', and '+ positive & negative'. Each row represents a different model configuration. The first row is labeled 'BERT' and shows the following values: 91.3 for 'original', 92.27 for '+ positive', 90.08 for '+ negative', 89.73 for '- negative', and 93.14 for '+ positive & negative'. The second row, 'BERT+FL', shows slight improvements over BERT, with values of 91.86 (+0.56), 92.64 (+0.37), 90.61 (+0.53), 90.79 (+1.06), and 93.45 (+0.31). The third row, 'BERT+DL', also shows improvements, with values of 91.92 (+0.62), 92.87 (+0.60), 90.22 (+0.14), 90.49 (+0.76), and 93.52 (+0.38). The last row, 'BERT+DSC', shows the highest improvements, with values of 92.11 (+0.81), 92.92 (+0.65), 90.78 (+0.70), 90.80 (+1.07), and 93.63 (+0.49).",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_11.jpg",
        "caption": [
            "Table 8: The effect of different data augmentation ways for QQP in terms of F1-score. "
        ],
        "footnote": [],
        "context": "DSC achieves the highest F1 score across all datasets. Specially, for $^+$ positive, DSC achieves minor improvements $\\left(+0.05\\,\\mathrm{F}1\\right)$ ) over DL. In contrast, it significantly outperforms DL Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that $^+$ positive outperforms original, and +negative underperforms original. This is in line with our expectation since $^+$ positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances. +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\\%$ being positive and $79\\%$ being negative.   Negative downsampling (- negative) We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\\%$ being positive and $50\\%$ being negative. Positive and negative augmentation ( $\\mathbf{\\Psi}+$ positive $\\pmb{\\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ba8731394946285e9c56ecf440d2094d",
        "description": "The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' The table is structured with two main columns: SST-2 and SST-5, each representing different datasets. Under each dataset column, there are two sub-columns labeled 'Acc' for accuracy. The rows represent different models used for data augmentation: BERT+CE, BERT+DL, and BERT+DSC. The values in the table are as follows: For SST-2, BERT+CE has an accuracy of 94.90%, BERT+DL has 94.37%, and BERT+DSC has 94.84%. For SST-5, BERT+CE has an accuracy of 55.57%, BERT+DL has 54.63%, and BERT+DSC has 55.19%. The table highlights the performance of these models across different datasets, with BERT+CE generally achieving higher accuracy scores.",
        "segmentation": false
    },
    "image_12": {
        "image_id": 12,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.45/images/image_12.jpg",
        "caption": [],
        "footnote": [],
        "context": "Bernd Bohnet, Ryan T. McDonald, Gon¸calo Simo˜es, Daniel Andor, Emily Pitler, and Joshua Maynez. 2018. Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2642–2652. References We thank all anonymous reviewers, as well as Qinghong Han, Wei Wu and Jiawei Wu for their comments and suggestions. The work is supported by the National Natural Science Foundation of China (NSFC No. 61625107 and 61751209). Acknowledgement to achieve significant performance boost without changing model architectures.  $\\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha$ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha$ changes in distinct datasets, which shows that the hyperparameters $\\alpha,\\beta$ acturally play an important role in TI. 6 Conclusion In this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (F1 score). Experimental results show that the proposed loss function help Table 10: The effect of hyperparameters in Tversky Index. We set $\\beta=1-\\alpha$ and thus we only list $\\alpha$ here. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-0a3be01b9d7564716773010993d3694a",
        "description": "The image is a table that presents the effect of hyperparameters in Tversky Index on two different datasets: Chinese Onto4.0 and English QuoRef. The table has three columns: the first column lists values of the hyperparameter α ranging from 0.1 to 0.9, the second column shows the corresponding F1 scores for the Chinese Onto4.0 dataset, and the third column shows the F1 scores for the English QuoRef dataset. For the Chinese Onto4.0 dataset, the F1 scores are as follows: 80.13 for α = 0.1, 81.17 for α = 0.2, 84.22 for α = 0.3, 84.52 for α = 0.4, 84.47 for α = 0.5, 84.67 for α = 0.6 (highlighted in bold), 81.81 for α = 0.7, 80.97 for α = 0.8, and 80.21 for α = 0.9. For the English QuoRef dataset, the F1 scores are: 63.23 for α = 0.1, 63.45 for α = 0.2, 65.88 for α = 0.3, 68.44 for α = 0.4 (highlighted in bold), 67.52 for α = 0.5, 66.35 for α = 0.6, 65.09 for α = 0.7, 64.13 for α = 0.8, and 64.84 for α = 0.9. The highest F1 score for the Chinese Onto4.0 dataset is 84.67 at α = 0.6, and for the English QuoRef dataset, it is 68.44 at α = 0.4.",
        "segmentation": false
    }
}