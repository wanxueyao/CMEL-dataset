{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "placed by a [MASK] Figure 1: SenseBERT includes a masked-word supersense prediction task, pre-trained jointly with BERT’s original masked-word prediction task (Devlin et al., 2019) (see section 3.2). As in the original BERT, the mapping from the Transformer dimension to the external dimension is the same both at input and at output ( $W$ for words and $S$ for supersenses), where $M$ denotes a fixed mapping between word-forms and their allowed WordNet supersenses (see section 3.3). The vectors $p^{(j)}$ denote positional embeddings. For clarity, we omit a reference to a sentence-level Next Sentence Prediction task trained jointly with the above. learning $S$ in parallel to $W$ , effectively implementing word-form and wordsense multi-task learning in the pre-training stage. Then, in section 3.3 we describe our methodology for adding supersense information in $S$ to the initial Transformer embedding, in parallel to word-level information added by $W$ . In section 3.4 we address the issue of supersense prediction for out-ofvocabulary words, and in section 3.5 we describe our modification of BERT’s masking strategy, prioritizing single-supersensed words which carry a clearer semantic signal. 3.1 Background The input to BERT is a sequence of words $\\{x^{(j)}\\in$ $\\{0,1\\}^{\\bar{D}_{W}}\\}_{j=1}^{N}$ where $15\\%$ of the words are re ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-0e1697e417dfb2436073447a327f08fe",
        "description": "The image is a diagram illustrating the architecture of SenseBERT, an extension of BERT (Bidirectional Encoder Representations from Transformers). The diagram is divided into two parts: (a) BERT and (b) SenseBERT. In part (a), the input sequence $x^{(1)}$ to $x^{(N)}$ is shown with a [MASK] token included. These inputs are mapped through a matrix $W$ to produce $Wx^{(j)}$ for each token. Positional embeddings $p^{(j)}$ are added to these transformed inputs before they are fed into a Transformer encoder. The output of the Transformer encoder is then used to predict words $y^{words}$ through a matrix $W^T$. In part (b), SenseBERT adds an additional mapping $S$ for supersenses. The input sequence is similarly mapped through $W$ and $S$, producing $Wx^{(j)}$ and $SMx^{(j)}$ respectively. Positional embeddings $p^{(j)}$ are added to these transformed inputs before they are fed into the same Transformer encoder. The outputs are used to predict both words $y^{words}$ through $W^T$ and supersenses $y^{senses}$ through $S^T$. The diagram uses color coding to differentiate between word-level and supersense-level information, with pink representing word-level information and blue representing supersense-level information.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_2.jpg",
        "caption": [
            "Figure 2: UMAP visualization of supersense vectors (rows of the classifier $S$ ) learned by SenseBERT at pre-training. (a) Clustering by the supersense’s part-of speech. (b) Within noun supersenses, semantically similar supersenses are clustered together (see more details in the supplementary materials). ",
            "(b)  Noun Supersenses "
        ],
        "footnote": [],
        "context": "The above strategy for constructing vinput allows for the semantic level vectors in $S$ to come into play and shape the input embeddings where $p^{(j)}$ are the regular positional embeddings as used in BERT, and $M\\in\\mathbb{R}^{D_{S}\\times D_{W}}$ is a static $0/1$ matrix converting between words and their allowed WordNet supersenses $A(w)$ (see construction details above). $$ v_{\\mathrm{input}}^{(j)}=(W+S M)x^{(j)}+p^{(j)}, $$ We follow this approach, and insert our newly proposed semantic-level language model matrix $S$ in the input in addition to $W$ [as depicted in figure 1(b)], such that the input vector to the Transformer encoder (eq. 1) is modified to obey: level language model with a combined loss of the form: $$ \\mathcal{L}_{\\mathrm{SLM}}=\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{allowed}}+\\mathcal{L}_{\\mathrm{SLM}}^{\\mathrm{reg}}\\quad. $$ 3.3 Supersense Aware Input Embeddings Though in principle two different matrices could have been used for converting in and out of the Tranformer encoder, the BERT architecture employs the same mapping $W$ . This approach, referred to as weight tying, was shown to yield theoretical and pracrical benefits (Inan et al., 2017; Press and Wolf, 2017). Intuitively, constructing the Transformer encoder’s input embeddings from the same mapping with which the scores are computed improves their quality as it makes the input more sensitive to the training signal. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-5e483e86c3da6b91e674acee8d582695",
        "description": "The image is a UMAP visualization of supersense vectors learned by SenseBERT at pre-training. The visualization is divided into two parts: (a) Clustering by the supersense’s part-of-speech and (b) Clustering within noun supersenses. In part (a), different colors represent different parts of speech, with Verb Supersenses in dark gray, Noun Supersenses in yellow, and Other (adv./adj.) in light blue. The points are scattered across the plot, forming distinct clusters for each part of speech. In part (b), various noun supersenses are labeled and clustered together based on semantic similarity. The labels include 'noun.artifact', 'noun.group', 'noun.location', 'noun.animal', 'noun.object', 'noun.food', 'noun.substance', 'noun.attribute', 'noun.person', 'noun.shape', 'noun.feeling', 'noun.body', 'noun.plant', and 'noun.concrete'. Each label is associated with a specific color, and the points are distributed across the plot, forming clusters that reflect their semantic relationships.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_3.jpg",
        "caption": [],
        "footnote": [],
        "context": "vocabulary and employ a whole-word-masking strategy. Accordingly, all of the tokens of a tokenized OOV word are masked together. In this case, we train the supersense prediction task to Figure 3: (a) A demonstration of supersense probabilities assigned to a masked position within context, as given by SenseBERT’s word-supersense level semantic language model (capped at $5\\%$ ). Example words corresponding to each supersense are presented in parentheses. (b) Examples of SenseBERT’s prediction on raw text, when the unmasked input sentence is given to the model. This beyond word-form abstraction ability facilitates a more natural elicitation of semantic content at pre-training.  regular word level loss and do not train the supersense prediction task. The above addition to the vocabulary results in an increase of approximately 23M parameters over the 110M parameters of $\\mathrm{BERT_{BASE}}$ and an increase of approximately 30M parameters over the 340M parameters of BERTLARGE (due to different embedding dimensions $d=768$ and $d=1024$ , respectively). It is worth noting that similar vocabulary sizes in leading models have not resulted in increased sense awareness, as reflected for example in the WiC task results (Liu et al., 2019). As a second alternative, referred to as average embedding, we employ BERT’s regular 30K-token ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-44a0f7927ed315ea256f3cbcec154b51",
        "description": "The image consists of two text blocks with masked words and their possible interpretations. The first block reads: 'The [MASK] fell to the floor.' Below it, there is a list of possible interpretations for the masked word, categorized by their probability and part-of-speech (POS) tag. The top interpretation is 'noun.artifact' with a 52% probability, followed by examples such as 'sword,' 'chair,' etc. The second most probable interpretation is 'noun.person' with a 17% probability, followed by examples like 'man,' 'girl,' etc. The second block reads: 'Gill [MASK] the bread.' Similarly, it lists possible interpretations for the masked word, categorized by their probability and POS tag. The top interpretation is 'verb.contact' with a 33% probability, followed by examples such as 'cut,' 'buttered,' etc. The second most probable interpretation is 'verb.consumption' with a 20% probability, followed by examples like 'ate,' 'chewed,' etc. The third interpretation is 'verb.change' with an 11% probability, followed by examples such as 'heated,' 'baked,' etc. The fourth interpretation is 'verb.possession' with a 6% probability, followed by examples such as 'took,' 'bought,' etc.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_4.jpg",
        "caption": [],
        "footnote": [],
        "context": "vocabulary and employ a whole-word-masking strategy. Accordingly, all of the tokens of a tokenized OOV word are masked together. In this case, we train the supersense prediction task to Figure 3: (a) A demonstration of supersense probabilities assigned to a masked position within context, as given by SenseBERT’s word-supersense level semantic language model (capped at $5\\%$ ). Example words corresponding to each supersense are presented in parentheses. (b) Examples of SenseBERT’s prediction on raw text, when the unmasked input sentence is given to the model. This beyond word-form abstraction ability facilitates a more natural elicitation of semantic content at pre-training. regular word level loss and do not train the supersense prediction task. The above addition to the vocabulary results in an increase of approximately 23M parameters over the 110M parameters of $\\mathrm{BERT_{BASE}}$ and an increase of approximately 30M parameters over the 340M parameters of BERTLARGE (due to different embedding dimensions $d=768$ and $d=1024$ , respectively). It is worth noting that similar vocabulary sizes in leading models have not resulted in increased sense awareness, as reflected for example in the WiC task results (Liu et al., 2019). As a second alternative, referred to as average embedding, we employ BERT’s regular 30K-token  ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-44a0f7927ed315ea256f3cbcec154b51",
        "description": "The image is a diagram illustrating the syntactic structure of two sentences. The first sentence, 'Dan cooked a bass on the grill,' is broken down into its constituent parts: 'noun.person' (Dan), 'noun.food' (bass), 'verb.creation' (cooked), and 'noun.artifact' (grill). The second sentence, 'The bass player was exceptional,' is similarly analyzed with 'noun.artifact' (player), 'adj.all' (exceptional), and 'noun.person' (bass player). Each word or phrase is labeled according to its grammatical function in the sentence.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "SenseBERT’s semantic language model allows predicting a distribution over supersenses rather than over words in a masked position. Figure 3(a) shows We illustrate the resultant mapping in figure 2, showing a UMAP dimensionality reduction (McInnes et al., 2018) of the rows of $S$ ,which corresponds to the different supersenses. A clear clustering according to the supersense partof-speech is apparent in figure 2(a). We further identify finer-grained semantic clusters, as shown for example in figure 2(b) and given in more detail in the supplementary materials. guage model that predicts the missing word’s meaning jointly with the standard word-form level language model.  (with training hyperparameters as in Devlin et al. (2019)), has an immediate non-trivial bi-product. The pre-trained mapping to the supersenses space, denoted $S$ , acts as an additional head predicting a word’s supersense given context [see figure 1(b)]. We thereby effectively attain a semantic-level lanTable 1: Testing variants for predicting supersenses of rare words during SenseBERT’s pretraining, as described in section 5.1. Results are reported on the SemEval-SS task (see section 5.2). 30K/60K stand for vocabulary size, and no/average OOV stand for not predicting senses for OOV words or predicting senses from the average of the sub-word token embeddings, respectively. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-44a0f7927ed315ea256f3cbcec154b51",
        "description": "The image is a table labeled 'Table 1: Testing variants for predicting supersenses of rare words during SenseBERT’s pretraining, as described in section 5.1.' The table contains two columns and three rows. The first column is labeled 'SenseBERT_BASE' and the second column is labeled 'SemEval-SS Fine-tuned'. The rows are labeled as follows: '30K no OOV' with a value of 81.9, '30K average OOV' with a value of 82.7, and '60K no OOV' with a value of 83. The table presents the results of different testing variants on the SemEval-SS task, with the values representing the performance metrics.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "an ability to view raw text at a lexical semantic level. Figure 3(b) shows example sentences and their supersense prediction Figure 4: Example entries of (a) the SemEval-SS task, where a model is to predict the supersense of the marked word, and (b) the Word in Context (WiC) task where a model must determine whether the underlined word is used in the same/different supersense within sentences A and B. In all displayed examples, taken from the corresponding development sets, SenseBERT predicted the correct label while BERT failed to do so. A quantitative comparison between models is presented in table 2. reduction (McInnes et al., 2018) of the rows of $S$ ,which corresponds to the different supersenses. A clear clustering according to the supersense partof-speech is apparent in figure 2(a). We further identify finer-grained semantic clusters, as shown for example in figure 2(b) and given in more detail in the supplementary materials. SenseBERT’s semantic language model allows predicting a distribution over supersenses rather than over words in a masked position. Figure 3(a) shows the supersense probabilities assigned by SenseBERT in several contexts, demonstrating the model’s ability to assign semantically meaningful categories to the masked position. Finally, we demonstrate that SenseBERT enjoys ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-44a0f7927ed315ea256f3cbcec154b51",
        "description": "The image is a table divided into two main sections labeled (a) and (b). Section (a) is titled 'SemEval-SS' and contains two sentences with marked words. The first sentence reads, 'The team used a battery of the newly developed “gene probes”', where 'battery' is marked. The supersense predictions for this word are 'noun.artifact' by BERT and 'noun.group' by SenseBERT. The second sentence reads, 'Ten shirt-sleeved ringers stand in a circle, one foot ahead of the other in a prize-fighter's stance', where 'foot' is marked. The supersense predictions for this word are 'noun.quantity' by BERT and 'noun.body' by SenseBERT. Section (b) is titled 'WiC' and contains pairs of sentences with underlined words. The first pair consists of Sentence A: 'The kick must be synchronized with the arm movements.' and Sentence B: 'A sidecar is a smooth drink but it has a powerful kick.', where 'kick' is underlined. The supersense prediction for 'kick' is 'Same' by BERT and 'Different' by SenseBERT. The second pair consists of Sentence A: 'Plant bugs in the dissident’s apartment.' and Sentence B: 'Plant a spy in Moscow.', where 'plant' is underlined. The supersense prediction for 'plant' is 'Different' by BERT and 'Same' by SenseBERT.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 2: Results on a supersense variant of the SemEval WSD test set standardized in Raganato et al. (2017), which we denote SemEval-SS, and on the Word in Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) included in the recently introduced SuperGLUE benchmark (Wang et al., 2019). These tasks require a high level of lexical semantic understanding, as can be seen in the examples in figure 4. For both tasks, SenseBERT demonstrates a clear improvement over BERT in the regular fine-tuning setup, where network weights are modified during training on the task. Notably, SenseBERTLARGE achieves state of the art performance on the common WSD task. We use SemCor (Miller et al., 1993) as our training dataset (226, 036 annotated examples), and the SenseEval (Edmonds and Cotton, 2001; Snyder and Palmer, 2004) / SemEval (Pradhan et al., 2007; Navigli et al., 2013; Moro and Navigli, 2015) suite for evaluation (overall 7253 annotated examples), following Raganato et al. (2017). For each word in both training and test sets, we change its fine-grained sense label to its corresponding WordNet supersense, and therefore train the network to predict a given word’s supersense. We name this Supersense disambiguation task SemEval-SS. See figure 4(a) for an example ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-a24ba6c84e31348058a7fdc460010cff",
        "description": "The image is a table that presents the results of different models on two tasks: SemEval-SS Frozen, SemEval-SS Fine-tuned, and Word in Context. The table has four rows and three columns. The first row contains the names of the models: BERT_BASE, BERT_LARGE, SenseBERT_BASE, and SenseBERT_LARGE. The second row shows the scores for the SemEval-SS Frozen task, with values of 65.1 for BERT_BASE, 67.3 for BERT_LARGE, 75.6 for SenseBERT_BASE, and 79.5 for SenseBERT_LARGE. The third row displays the scores for the SemEval-SS Fine-tuned task, with values of 79.2 for BERT_BASE, 81.1 for BERT_LARGE, 83.0 for SenseBERT_BASE, and 83.7 for SenseBERT_LARGE. The fourth row provides the scores for the Word in Context task, with values of - for BERT_BASE, 69.6 for BERT_LARGE, 70.3 for SenseBERT_BASE, and 72.1 for SenseBERT_LARGE. The table highlights the performance improvements of SenseBERT over BERT in both fine-tuning setups.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_8.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Test set results for the WiC dataset. †Pilehvar and Camacho-Collados (2019) ††Loureiro and Jorge (2019) ‡Wang et al. (2019) ‡‡Liu et al. (2019) ⋄Peters et al. (2019) "
        ],
        "context": "In the second training scheme we fine-tuned the examined model on the task, We show results on the SemEval-SS task for two different training schemes. In the first, we trained a linear classifier over the ‘frozen’ output embeddings of the examined model – we do not change the the trained SenseBERT’s parameters in this scheme. This Frozen setting is a test for the amount of basic lexical semantics readily present in the pre-trained model, easily extricable by further downstream tasks (reminiscent of the semantic probes employed in Hewitt and Manning (2019); Reif et al. (2019). from this modified data set. In the SemEval-SS Frozen setting, we train a linear classifier over pretrained embeddings, without changing the network weights. The results show that SenseBERT introduces a dramatic improvement in this setting, implying that its word-sense aware pre-training (section 3) yields embeddings that carries lexical semantic information which is easily extractable for the benefits of downstream tasks. Results for BERT on the SemEval-SS task are attained by employing the published pre-trained BERT models, and the $\\mathbf{BERT}_{\\mathrm{LARGE}}$ result on WiC is taken from the baseline scores published on the SuperGLUE benchmark (Wang et al., 2019) (no result has been published for $\\mathrm{BERT}_{\\mathrm{BASE}}.$ ). ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-dcf192e88a7847643419addcb213b70c",
        "description": "The image is a table labeled 'Word in Context' that presents test set results for the WiC dataset. The table contains six rows, each representing a different model or embedding type, and their corresponding performance scores. The models listed are: ELMo† with a score of 57.7, BERT sense embeddings †† with a score of 67.7, BERT_LARGE‡ with a score of 69.6, RoBERTa‡‡ with a score of 69.9, KnowBERT-W+W⋄ with a score of 70.9, and SenseBERT with the highest score of 72.1. The footnote provides references for each model: ELMo by Pilehvar and Camacho-Collados (2019), BERT sense embeddings by Loureiro and Jorge (2019), BERT_LARGE by Wang et al. (2019), RoBERTa by Liu et al. (2019), and KnowBERT-W+W by Peters et al. (2019).",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_9.jpg",
        "caption": [
            "Table 4: Results on the GLUE benchmark test set. "
        ],
        "footnote": [],
        "context": "The General Language Understanding Evaluation (GLUE; Wang et al. (2018)) benchmark is a popular testbed for language understanding models. It consists of 9 different NLP tasks, covering different linguistic phenomena. We evaluate our model on GLUE, in order to verify that SenseBERT gains its lexical semantic knowledge without compromising performance on other 5.4 GLUE Results on the WiC task comparing SenseBERT to vanilla BERT are shown in table 2. SenseBERTBASE surpasses a larger vanilla model, BERTLARGE. As shown in table 3, a single SenseBERTLARGE model achieves the state of the art score in this task, demonstrating unprecedented lexical semantic awareness. binary classification task. Each instance in WiC has a target word $w$ for which two contexts are provided, each invoking a specific meaning of $w$ .The task is to determine whether the occurrences of $w$ in the two contexts share the same meaning or not, clearly requiring an ability to identify the word’s semantic category. The WiC task is defined over supersenses (Pilehvar and Camacho-Collados, 2019) – the negative examples include a word used in two different supersenses and the positive ones include a word used in the same supersense. See figure 4(b) for an example from this data set. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-dcf192e88a7847643419addcb213b70c",
        "description": "The image is a table labeled 'Table 4: Results on the GLUE benchmark test set.' The table compares the performance of two models, BERTBASE (OURS) and SenseBERTBASE, across various NLP tasks. The columns represent different tasks in the GLUE benchmark: Score, CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, and RTE. Each row corresponds to a model. For BERTBASE (OURS), the scores are as follows: Score = 77.5, CoLA = 50.1, SST-2 = 92.6, MRPC = 88.7/84.3, STS-B = 85.7/84.6, QQP = 71.0/88.9, MNLI = 83.6, QNLI = 89.4, and RTE = 67.9. For SenseBERTBASE, the scores are: Score = 77.9, CoLA = 54.6, SST-2 = 92.2, MRPC = 89.2/85.2, STS-B = 83.5/82.3, QQP = 70.3/88.8, MNLI = 83.6, QNLI = 90.6, and RTE = 67.5. The table highlights the performance metrics for each task, with some tasks having two values separated by a slash, indicating possibly different evaluation metrics or subsets.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_10.jpg",
        "caption": [
            "Table 5: A list of supersense categories from WordNet lexicographer. "
        ],
        "footnote": [],
        "context": " a Dendrogram of an Agglomerative hierarchical clustering over the supersense embedding vectors learned by SenseBERT in pre-training. The clustering shows a clear separation between Noun senses and Verb senses. Furthermore, we can observe that semantically related supersenses are clustered together (i.e, noun.animal and noun.plant). BTraining Details As hyperparameters for the fine-tuning, we used max seq lengt $h=128$ , chose learning rates from $\\{5e-6,1e-5,2e-5,3e-5,5e-5\\}$ , batch sizes from $\\{16,32\\}$ , and fine-tuned up to 10 epochs for all the datasets. Figure 5: Dendrogram visualization of an Agglomerative hierarchical clustering over the supersense vectors (rows of the classifier S) learned by SenseBERT. ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-41383b4585bbcacfa20edac3e89a064e",
        "description": "The image is a dendrogram visualization of an Agglomerative hierarchical clustering over the supersense embedding vectors learned by SenseBERT in pre-training. The dendrogram is divided into two main branches: Nouns and Verbs. Under the Nouns branch, there are several subcategories including noun.attribute, noun.state, noun.person, noun.group, noun.location, noun.act, noun.artifact, noun.communication, noun.time, noun.quantity, noun.substance, noun.plant, noun.animal, noun.body, noun.object, noun.food, noun.motive, noun.process, noun.shape, noun.feeling, noun.possession, and noun.phenomenon. Under the Verbs branch, there are subcategories such as verb.contact, verb.competition, verb.motion, verb.social, verb.possession, verb.creation, verb.cognition, verb.change, verb.stative, verb.emotion, verb.weather, and verb.consumption. The categories are color-coded with nouns in black and verbs in red. The dendrogram shows a clear separation between noun senses and verb senses, with semantically related supersenses clustered together, such as noun.animal and noun.plant.",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.423/images/image_11.jpg",
        "caption": [],
        "footnote": [],
        "context": "a Dendrogram of an Agglomerative hierarchical clustering over the supersense embedding vectors learned by SenseBERT in pre-training. The clustering shows a clear separation between Noun senses and Verb senses. Furthermore, we can observe that semantically related supersenses are clustered together (i.e, noun.animal and noun.plant). BTraining Details As hyperparameters for the fine-tuning, we used max seq lengt $h=128$ , chose learning rates from $\\{5e-6,1e-5,2e-5,3e-5,5e-5\\}$ , batch sizes from $\\{16,32\\}$ , and fine-tuned up to 10 epochs for all the datasets. Figure 5: Dendrogram visualization of an Agglomerative hierarchical clustering over the supersense vectors (rows of the classifier S) learned by SenseBERT.  ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-41383b4585bbcacfa20edac3e89a064e",
        "description": "The image is a table that categorizes different types of nouns and verbs into specific groups based on their semantic content. The table is divided into two main sections: 'Name' and 'Content'. Under the 'Name' column, there are various categories such as 'adj.all', 'noun.quantity', 'adv.all', etc. Each category is further detailed under the 'Content' column. For example, 'adj.all' includes all adjective clusters, 'noun.quantity' includes nouns denoting quantities and units of measure, and 'verb.body' includes verbs of grooming, dressing, and bodily care. The table provides a comprehensive list of these categories, detailing their specific contents. Notable entries include 'noun.animal' for nouns denoting animals, 'noun.artifact' for nouns denoting man-made objects, and 'verb.change' for verbs of size, temperature change, intensifying, etc. The table is structured in a clear and organized manner, with each row representing a distinct category and its corresponding content.",
        "segmentation": false
    }
}