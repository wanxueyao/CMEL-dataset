{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "We use bidirectional LSTMs in place of standard LSTMs. Bidirectional LSTMs capture both the previous timesteps (past features) and the future timesteps (future features) via forward and backward states respectively. Correspondingly, there are two hidden states that capture past and future information that Figure 1: Framework for credibility assessment. Upper part of the pipeline combines the article and claim embeddings to get the claim specific attention weights. Lower part of the pipeline captures the article representation through biLSTM. Attention focused article representation along with the source embeddings are passed through dense layers to predict the credibility score of the claim. capture task-specific features such as whether it contains objective language, we use a bidirectional Long Short-Term Memory (LSTM) network as proposed by Graves et al. (2005). A basic LSTM cell consists of various gates to control the flow of information through timesteps in a sequence, making LSTMs suitable for capturing long and short range dependencies in text that may be difficult to capture with standard recurrent neural networks (RNNs). Given an input word embedding of tokens $\\left<a_{k}\\right>$ , an LSTM cell performs various nonlinear transformations to generate a hidden vector state $h_{k}$ for each token at each timestep $k$ . ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-b2d3ca77559605515b9380d661fe5f71",
        "description": "The image is a detailed flowchart illustrating a framework for credibility assessment. The chart is divided into two main parts: the upper part and the lower part. The upper part of the pipeline combines the article and claim embeddings to get the claim-specific attention weights. This process involves concatenating the claim word embeddings (represented by blue circles) and passing them through a bidirectional LSTM (depicted as a series of interconnected boxes with arrows indicating the flow of information). The output from the bidirectional LSTM is then fed into a dense layer, which generates attention weights. These attention weights are used to weight the inner product of the article source embedding (red circles) and the claim source embedding (blue circles). The weighted inner product is then averaged and concatenated with the claim source embedding. The concatenated features are passed through two dense layers, which ultimately predict the credibility score of the claim. The lower part of the pipeline captures the article representation through the bidirectional LSTM. The attention-focused article representation, along with the source embeddings, is passed through dense layers to predict the credibility score of the claim.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_2.jpg",
        "caption": [],
        "footnote": [
            "Table 1: Data statistics (SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE: SemEval). "
        ],
        "context": "As the fourth dataset, we consider the benchmark dataset released by SemEval-2017 for the task of determining credibility and stance of social media content (Twitter) (Derczynski et al., 2017). The objective of this task is to predict the credibility of a questionable tweet (true, false or unverified) along with a confidence score from the model. It has two sub-tasks: (i) a closed variant in which models only consider the questionable tweet, and (ii) an open variant in which models consider both the questionable tweet and additional context consisting of snapshots of relevant sources retrieved immediately before 3.4 SemEval-2017 Task 8 source, and a set of reviews and ratings by community members. NewsTrust aggregates these ratings and assigns an overall credibility score (on a scale of 1 to 5) to the posted article. We map the attributes in this data to the inputs expected by DeClarE as follows: the title and the web source of the posted (news) article are mapped to the input claim and claim source, respectively. Reviews and their corresponding user identities are mapped to reporting articles and article sources, respectively. We use this dataset for the regression task of predicting the credibility score of the posted article. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-20dde6ce8918177c443c210520290855",
        "description": "The image is a table labeled 'Table 1: Data statistics (SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE: SemEval)' that provides statistical information about four datasets used for evaluating the credibility and stance of social media content. The table is structured with five main columns: Dataset, SN, PF, NT, and SE. Each column represents a different dataset and contains the following rows: 'Total claims', 'True claims', 'False claims', 'Unverified claims', 'Claim sources', 'Articles', and 'Article sources'. The 'Total claims' row shows the total number of claims in each dataset: SN has 4341 claims, PF has 3568 claims, NT has 5344 claims, and SE has 272 claims. The 'True claims' row indicates the number of true claims: SN has 1164 true claims, PF has 1867 true claims, NT has no true claims listed, and SE has 127 true claims. The 'False claims' row shows the number of false claims: SN has 3177 false claims, PF has 1701 false claims, NT has no false claims listed, and SE has 50 false claims. The 'Unverified claims' row indicates the number of unverified claims: SN has no unverified claims listed, PF has no unverified claims listed, NT has no unverified claims listed, and SE has 95 unverified claims. The 'Claim sources' row shows the number of claim sources: SN has no claim sources listed, PF has 95 claim sources, NT has 161 claim sources, and SE has 10 claim sources. The 'Articles' row indicates the number of articles: SN has 29242 articles, PF has 29556 articles, NT has 25128 articles, and SE has 3717 articles. The 'Article sources' row shows the number of article sources: SN has 336 article sources, PF has 336 article sources, NT has 251 article sources, and SE has 89 article sources. The table highlights the significant differences in the size and composition of the datasets.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Model parameters used for each dataset (SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE: SemEval). "
        ],
        "context": "When using the Snopes, PolitiFact and NewsTrust datasets, we reserve $10\\%$ of the data as validation data for parameter tuning. We report 10-fold cross validation results on the remaining $90\\%$ of the data; the model is trained on 9-folds and the remaining fold is used as test data. When using the SemEval dataset, we use the data splits provided by the task’s organizers. The objective for 4.1 Experimental Setup We evaluate our approach by conducting experiments on four datasets, as described in the previous section. We describe our experimental setup and report our results in the following sections. 4 Experiments  the fraction of claim words that are present in the snippet, and simsemantic represents the cosine similarity between the average of claim word embeddings and snippet word embeddings. We also enforce a constraint that the sim score is at least $\\delta$ . We varied $\\delta$ from 0.2 to 0.8 and found 0.5 to give the optimal performance on a withheld dataset. We discard all articles related to Snopes and PolitiFact websites from our datasets to have an unbiased model. Statistics of the datasets after pre-processing is provided in Table 1. All the datasets are made publicly available at https://www.mpi-inf. mpg.de/dl-cred-analysis/. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-20dde6ce8918177c443c210520290855",
        "description": "The image is a table labeled 'Table 2: Model parameters used for each dataset (SN: Snopes, PF: PolitiFact, NT: NewsTrust, SE: SemEval)'. The table is structured with the following columns: Parameter, SN, PF, NT, and SE. Each row represents a different parameter used in the model. The parameters listed are Word embedding length, Claim source embedding length, Article source embedding length, LSTM size (for each pass), Size of fully connected layers, and Dropout. The values for these parameters are as follows: Word embedding length is 100 for SN, PF, and SE, and 300 for NT. Claim source embedding length is not applicable for SN, 4 for PF and SE, and 8 for NT. Article source embedding length is 8 for SN and NT, and 4 for PF and SE. LSTM size (for each pass) is 64 for SN, PF, and NT, and 16 for SE. Size of fully connected layers is 32 for SN and PF, 64 for NT, and 8 for SE. Dropout is 0.5 for SN and PF, and 0.3 for NT and SE.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Comparison of various approaches for credibility classification on Snopes and PolitiFact datasets. "
        ],
        "context": "We compare our approach with the following state-of-the-art models: (i) LSTM-text, a recent approach proposed by Rashkin et al. (2017). (ii) CNN-text: a CNN based approach proposed by Wang (2017). (iii) Distant Supervision: stateof-the-art distant 4.2 Results: Snopes and Politifact Credibility Classification (Snopes, PolitiFact and SemEval): accuracy of the models in classifying true and false claims separately, macro F1-score and Area-Under-Curve (AUC) for the ROC (Receiver Operating Characteristic) curve. Credibility Regression (NewsTrust): Mean Square Error (MSE) between the predicted and true credibility scores. To evaluate and compare the performance of DeClarE with other state-of-the-art methods, we report the following measures:  are not very large, we do not tune the word embeddings during training. The remaining model parameters are tuned on the validation data; the parameters chosen are reported in Table 2. We use Keras with a Tensorflow backend to implement our system. All the models are trained using Adam optimizer (Kingma and Ba, 2014) (learning rate: 0.002) with categorical cross-entropy loss for classification and mean squared error loss for regression task. We use L2-regularizers with the fully connected layers as well as dropout. For all the datasets, the model is trained using each claimarticle pair as a separate training instance. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-5c10727c37b415aa074a7d364148dfb3",
        "description": "The image is a table labeled 'Table 3: Comparison of various approaches for credibility classification on Snopes and PolitiFact datasets.' The table is structured with the following columns: Dataset, Configuration, True Claims Accuracy (%), False Claims Accuracy (%), Macro F1-Score, and AUC. For the Snopes dataset, the configurations listed are LSTM-text, CNN-text, Distant Supervision, DeClarE (Plain), DeClarE (Plain+Attn), DeClarE (Plain+SrEmb), and DeClarE (Full). The corresponding values for True Claims Accuracy (%) are 64.65, 67.15, 83.21, 74.37, 78.34, 77.43, and 78.96 respectively. For False Claims Accuracy (%), the values are 64.21, 63.14, 80.78, 78.57, 78.91, 79.80, and 78.32 respectively. The Macro F1-Score values are 0.66, 0.66, 0.82, 0.78, 0.79, 0.79, and 0.79 respectively. The AUC values are 0.70, 0.72, 0.88, 0.83, 0.85, 0.85, and 0.86 respectively. For the PolitiFact dataset, the configurations listed are LSTM-text, CNN-text, Distant Supervision, DeClarE (Plain), DeClarE (Plain+Attn), DeClarE (Plain+SrEmb), and DeClarE (Full). The corresponding values for True Claims Accuracy (%) are 63.19, 63.67, 62.53, 62.67, 65.53, 66.71, and 67.32 respectively. For False Claims Accuracy (%), the values are 61.96, 63.31, 62.08, 69.05, 68.49, 69.28, and 69.62 respectively. The Macro F1-Score values are 0.63, 0.64, 0.62, 0.66, 0.66, 0.67, and 0.68 respectively. The AUC values are 0.66, 0.67, 0.68, 0.70, 0.72, 0.74, and 0.75 respectively.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "first three models described in Section 4.2 as baselines. For CNN-text and LSTM-text, we add a linear fully connected layer as the final layer of the model to support regression. Additionally, we also consider the state-of-the-art $\\mathrm{CCRF+SVR}$ model based on Continuous Conditional Random Field (CCRF) and Support Vector Regression (SVR) proposed by Mukherjee and Weikum (2015). The results are shown in Table 4. We observe that DeClarE (Full) outperforms all four baselines, with a $17\\%$ decrease in MSE compared to the bestperforming baselines (i.e., LSTM-text and Distant Supervision). The DeClarE (Plain) model performs substantially worse than the full model, illustrating  terms of Macro F1. A performance comparison of DeClarE’s various configurations indicates the contribution of each component of our model, i.e, biLSTM capturing article representations, attention mechanism and source embeddings. The additions of both the attention mechanism and source embeddings improve performance over the plain configuration in all cases when measured by Macro F1 or AUC. 4.3 Results: NewsTrust When performing credibility regression on the NewsTrust dataset, we evaluate the models in terms of mean squared error (MSE; lower is better) for credibility rating prediction. We use the Table 4: Comparison of various approaches for credibility regression on NewsTrust dataset. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-5c10727c37b415aa074a7d364148dfb3",
        "description": "The image is a table that compares the performance of different configurations for credibility regression on the NewsTrust dataset, measured by Mean Squared Error (MSE). The table has two columns: 'Configuration' and 'MSE'. The rows under 'Configuration' list various models and their respective MSE values. The configurations listed are: CNN-text with an MSE of 0.53, CCRF+SVR with an MSE of 0.36, LSTM-text with an MSE of 0.35, DistantSup with an MSE of 0.35, DeClarE (Plain) with an MSE of 0.34, and DeClarE (Full) with an MSE of 0.29. The lowest MSE value, indicating the best performance, is achieved by DeClarE (Full).",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_6.jpg",
        "caption": [],
        "footnote": [
            "Table 5: Comparison of various approaches for credibility classification on SemEval dataset. "
        ],
        "context": "Similar to the treatment of article representations, we perform an analysis with the claim and article source embeddings by employing PCA and plotting the 5.2 Analyzing Source Embeddings In order to assess how our model separates articles reporting false claims from those reporting true ones, we employ dimensionality reduction using Principal Component Analysis (PCA) to project the article representations $\\mathit{\\Delta}_{g}$ in Equation 2) from a high dimensional space to a 2d plane. The projections are shown in Figure 2a. We observe that DeClarE obtains clear separability between credible versus non-credible articles in Snopes dataset. 5.1 Analyzing Article Representations 5 Discussion best performing approach for the close variant of the task, (ii) IITP (Singh et al., 2017): the best performing approach for the open variant of the task, (iii) DeClare (Plain): our approach with only biLSTM (no attention and source embeddings), and (iv) DeClarE (Full): our end-to-end system with biLSTM, attention and source embeddings. We use the evaluation measure proposed by the task’s organizers: macro F1-score for overall classification and Root-Mean-Square Error (RMSE) over confidence scores. Results are shown in Table 5. We observe that DeClarE (Full) outperforms all the other approaches — thereby, re-affirming its power in harnessing external evidence. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ffaeb6179029526d2930f24a0dfecfe0",
        "description": "The image is a table labeled 'Table 5: Comparison of various approaches for credibility classification on SemEval dataset.' The table is structured with three main columns: Configuration, Macro Accuracy, and RMSE. Each row represents a different configuration and contains the following values: 'IITP (Open)' with Macro Accuracy of 0.39 and RMSE of 0.746, 'NileTMRG (Close)' with Macro Accuracy of 0.54 and RMSE of 0.673, 'DeClarE (Plain)' with Macro Accuracy of 0.46 and RMSE of 0.687, and 'DeClarE (Full)' with Macro Accuracy of 0.57 and RMSE of 0.604. The table highlights the performance of different configurations in terms of Macro Accuracy and RMSE, with DeClarE (Full) showing the best performance.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "Our work is closely related to the following areas: Credibility analysis of Web claims: Our work builds upon 6 Related Work play a major role in deciding the corresponding claim’s credibility. In the first example on Table 6, highlighted words such as “..barely true...” and “..sketchy evidence...” help our system to identify the claim as not credible. On the other hand, highlighted words in the last example, like, “..reveal...” and “..documenting reports...” help our system to assess the claim as credible. Table 6: Interpretation via attention (weights) $([T r u e]/[F a l s e]$ indicates the verdict from DeClarE).   to each other in the embedding space. 5.3 Analyzing Attention Weights Attention weights help understand what DeClarE focuses on during learning and how it affects its decisions – thereby, making our model transparent to the end-users. Table 6 illustrates some interesting claims and salient words (highlighted) that DeClarE focused on during learning. Darker shades indicate higher weights given to the corresponding words. As illustrated in the table, DeClarE gives more attention to important words in the reporting article that are relevant to the claim and also Figure 2: Dissecting the article, article source and claim source representations learned by DeClarE. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ffaeb6179029526d2930f24a0dfecfe0",
        "description": "The image consists of three subfigures labeled (a), (b), and (c). Subfigure (a) shows a scatter plot with projections of article representations using PCA. The points are colored in red and green, representing non-credible and credible articles, respectively. DeClarE obtains clear separation between these two categories. Subfigure (b) is another scatter plot showing projections of article source representations using PCA. Various news sources are labeled, including CNN, NYTimes, WashingtonPost, DailyCurrant, Huzlers, EmpireNews, NationalReport, WorldNewsDailyReport, FoxNews, BBC, USA Today, and WSJ. DeClarE clearly separates fake news sources from authentic ones. Subfigure (c) is a third scatter plot showing projections of claim source representations using PCA. Politicians such as Bernie Sanders, Barack Obama, Ted Cruz, Rudy Giuliani, Hillary Clinton, Mike Pence, Paul Ryan, Donald Trump, and Mitch McConnell are labeled. DeClarE clusters politicians of similar ideologies close to each other in the embedding space.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1003/images/image_8.jpg",
        "caption": [],
        "footnote": [],
        "context": "Our work is closely related to the following areas: Credibility analysis of Web claims: Our work builds upon 6 Related Work play a major role in deciding the corresponding claim’s credibility. In the first example on Table 6, highlighted words such as “..barely true...” and “..sketchy evidence...” help our system to identify the claim as not credible. On the other hand, highlighted words in the last example, like, “..reveal...” and “..documenting reports...” help our system to assess the claim as credible. Table 6: Interpretation via attention (weights) $([T r u e]/[F a l s e]$ indicates the verdict from DeClarE). to each other in the embedding space. 5.3 Analyzing Attention Weights Attention weights help understand what DeClarE focuses on during learning and how it affects its decisions – thereby, making our model transparent to the end-users. Table 6 illustrates some interesting claims and salient words (highlighted) that DeClarE focused on during learning. Darker shades indicate higher weights given to the corresponding words. As illustrated in the table, DeClarE gives more attention to important words in the reporting article that are relevant to the claim and also Figure 2: Dissecting the article, article source and claim source representations learned by DeClarE.   ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-ffaeb6179029526d2930f24a0dfecfe0",
        "description": "The image is a table labeled 'Table 6: Interpretation via attention (weights)' that provides examples of claims and their credibility analysis. The table is structured with four main rows, each containing a claim, its verdict (True or False), and highlighted words that indicate the basis for the credibility assessment. The first row contains a false claim by Barbara Boxer about Fiorina's plan to slash Social Security and Medicare, with highlighted words such as 'barely true' and 'sketchy evidence.' The second row contains a true claim by Hillary Clinton about the gun epidemic being the leading cause of death for young African-American men, with highlighted words like 'leading cause of death' and 'gun epidemic.' The third row contains a false claim about Coca-Cola's original diet cola drink, TaB, taking its name from an acronym, with highlighted words such as 'untrue' and 'completely untrue.' The fourth row contains a true claim about household paper shredders posing a danger to children and pets, with highlighted words like 'reveals' and 'documenting reports.' The table uses darker shades to indicate higher weights given to the corresponding words, which are relevant to the claim and help in assessing its credibility.",
        "segmentation": false
    }
}