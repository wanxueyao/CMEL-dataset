{
    "chunk-c474bd136004e0c87233db96a8e12b49": [
        {
            "entity_name": "College of Information and Computer Sciences (University of Massachusetts Amherst)",
            "entity_type": "ORGANIZATION",
            "description": "College of Information and Computer Sciences is an academic institution affiliated with University of Massachusetts Amherst, where the authors of the paper are associated.",
            "source_entities": [
                "COLLEGE OF INFORMATION AND COMPUTER SCIENCES",
                "UNIVERSITY OF MASSACHUSETTS AMHERST"
            ]
        },
        {
            "entity_name": "NLP",
            "entity_type": "FIELD_OF_STUDY",
            "description": "NLP, or Natural Language Processing, is the field of focus for the paper discussing energy and policy considerations for deep learning, with various publications and models contributing to its advancements.",
            "source_entities": [
                "NLP"
            ]
        },
        {
            "entity_name": "LISA",
            "entity_type": "MODEL",
            "description": "LISA is a state-of-the-art NLP model from EMNLP 2018, used as a case study in the paper to analyze the computational and environmental costs of training deep neural network models.",
            "source_entities": [
                "LISA"
            ]
        },
        {
            "entity_name": "GPU (Graphics Processing Units)",
            "entity_type": "TECHNOLOGY",
            "description": "GPU, or Graphics Processing Units, are specialized hardware required for training the most computationally-hungry deep neural network models in NLP, with specific models like NVIDIA Titan X and NVIDIA GTX 1080 Ti being used.",
            "source_entities": [
                "GPU",
                "NVIDIA TITAN X",
                "NVIDIA GTX 1080 TI"
            ]
        },
        {
            "entity_name": "TPU (Tensor Processing Units)",
            "entity_type": "TECHNOLOGY",
            "description": "TPU, or Tensor Processing Units, are specialized hardware required for training the most computationally-hungry deep neural network models in NLP.",
            "source_entities": [
                "TPU"
            ]
        },
        {
            "entity_name": "NVIDIA System Management Interface",
            "entity_type": "TECHNOLOGY",
            "description": "NVIDIA System Management Interface is a tool used to sample GPU power consumption during the training of deep neural network models.",
            "source_entities": [
                "NVIDIA SYSTEM MANAGEMENT INTERFACE"
            ]
        },
        {
            "entity_name": "Intel’s Running Average Power Limit Interface",
            "entity_type": "TECHNOLOGY",
            "description": "Intel’s Running Average Power Limit interface is a tool used to sample CPU power consumption during the training of deep neural network models.",
            "source_entities": [
                "INTEL’S RUNNING AVERAGE POWER LIMIT INTERFACE"
            ]
        },
        {
            "entity_name": "CO2 (Carbon Dioxide Emissions)",
            "entity_type": "ENVIRONMENTAL_CONCERN",
            "description": "CO2, or carbon dioxide emissions, are a key environmental concern associated with the energy consumption of training deep neural network models.",
            "source_entities": [
                "CO2"
            ]
        },
        {
            "entity_name": "Renewable Energy Sources",
            "entity_type": "ENERGY_SOURCE",
            "description": "Renewable refers to renewable energy sources like hydro, solar, and wind, which are compared to non-renewable sources in terms of their contribution to energy consumption for cloud compute providers.",
            "source_entities": [
                "RENEWABLE"
            ]
        }
    ],
    "chunk-f96a801f228339f43372f3cdb35b0f77": [
        {
            "entity_name": "Transformer (T2T) Model",
            "entity_type": "Organization",
            "description": "The Transformer (T2T) model, also known as T2T, is an encoder-decoder architecture recognized for efficient and accurate machine translation, with code available online. It was reported on by Vaswani et al., 2017, and serves as the basis for recent work on neural architecture search (NAS) for machine translation and language modeling.",
            "source_entities": [
                "TRANSFORMER (T2T) MODEL",
                "VASWANI ET AL., 2017"
            ]
        },
        {
            "entity_name": "ELMo Model",
            "entity_type": "Organization",
            "description": "The ELMo model is based on stacked LSTMs and provides rich word representations in context by pre-training on a large amount of data using a language modeling objective. It has code freely available online and was reported on by Peters et al., 2018.",
            "source_entities": [
                "ELMO MODEL",
                "PETERS ET AL., 2018"
            ]
        },
        {
            "entity_name": "BERT Model",
            "entity_type": "Organization",
            "description": "The BERT model provides a Transformer-based architecture for building contextual representations, similar to ELMo, but trained with a different language modeling objective. It substantially improves accuracy on tasks requiring sentence-level representations and has code available online. The model was reported on by Devlin et al., 2019, and training details were also provided by Forster et al., 2019 using NVIDIA's DGX-2H servers.",
            "source_entities": [
                "BERT MODEL",
                "DEVLIN ET AL., 2019",
                "FORSTER ET AL., 2019"
            ]
        },
        {
            "entity_name": "GPT-2 Model",
            "entity_type": "Organization",
            "description": "The GPT-2 model is OpenAI’s latest edition of GPT general-purpose token encoder, also based on Transformer-style self-attention and trained with a language modeling objective. It is trained on massive data to show improvements in language modeling tasks and was reported on by Radford et al., 2019.",
            "source_entities": [
                "GPT-2 MODEL",
                "RADFORD ET AL., 2019",
                "TRANSFORMER-STYLE SELF-ATTENTION"
            ]
        }
    ],
    "chunk-3eec394eafb261cc01db2d32f0050eb0": [
        {
            "entity_name": "GPT-2",
            "entity_type": "ORGANIZATION",
            "description": "GPT-2 is a model developed by OpenAI, based on Transformer-style self-attention and trained with a language modeling objective. It has demonstrated high zero-shot performance on question answering and language modeling benchmarks.",
            "source_entities": [
                "GPT-2",
                "\"OPENAI\"",
                "\"RADFORD ET AL.\"",
                "\"TRANSFORMER-STYLE SELF-ATTENTION\"",
                "\"LANGUAGE MODELING OBJECTIVE\"",
                "\"ZERO-SHOT PERFORMANCE\"",
                "\"QUESTION ANSWERING\"",
                "\"LANGUAGE MODELING BENCHMARKS\""
            ]
        },
        {
            "entity_name": "LinguisticallyInformed Self-Attention (LISA)",
            "entity_type": "PERSON",
            "description": "LinguisticallyInformed Self-Attention (LISA) is a multi-task model developed by Strubell et al. for part-of-speech tagging, labeled dependency parsing, predicate detection, and semantic role labeling. It was recognized as a Best Long Paper at EMNLP.",
            "source_entities": [
                "\"STRUBELL ET AL.\"",
                "\"EMNLP\""
            ]
        },
        {
            "entity_name": "NVIDIA Titan X and M40",
            "entity_type": "ORGANIZATION",
            "description": "NVIDIA Titan X and M40 are types of GPUs used in the training of models, specifically mentioned as being used in 72% and 28% of the training for the LinguisticallyInformed Self-Attention model, respectively.",
            "source_entities": [
                "\"NVIDIA TITAN X\"",
                "\"NVIDIA M40\""
            ]
        }
    ],
    "chunk-1826475fb0a73465b6f130778848bc2f": [
        {
            "entity_name": "National Science Foundation (U.S. National Science Foundation)",
            "entity_type": "ORGANIZATION",
            "description": "The National Science Foundation is an organization that provided support through grant number IIS-1514053 and is suggested to fund shared compute centers for academic researchers.",
            "source_entities": [
                "NATIONAL SCIENCE FOUNDATION",
                "U.S. NATIONAL SCIENCE FOUNDATION"
            ]
        }
    ]
}