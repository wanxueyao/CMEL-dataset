{
    "image_1": [
        {
            "entity_name": "IMAGE_1",
            "entity_type": "ORI_IMG",
            "description": "The image is a flowchart illustrating the proposed end-to-end task-oriented dialogue system architecture. The process begins with a user input, 'Movie for the day after tomorrow, please', which is fed into a Bi-LSTM Utterance Encoder. This encoder processes the user's utterance and generates an encoding at turn k. Simultaneously, the system dialogue act embedding from the previous turn (k-1) is considered. The encoded user utterance and the system dialogue act are then passed to an LSTM Dialogue State module, which updates the dialogue state tracking. The updated dialogue state is used to query the Knowledge Base, which returns results based on the date (Thursday) and time (none). The query results are encoded and fed into the Policy Network, which selects the next system action. In this case, the system responds with 'Ok, what time do you prefer?', which is generated by the Natural Language Generator. The overall architecture integrates multiple neural network components and a knowledge base to manage dialogue state tracking and policy decisions in a conversational system."
        },
        {
            "entity_name": "USER",
            "entity_type": "PERSON",
            "description": "The user is interacting with the system by requesting a movie for the day after tomorrow."
        },
        {
            "entity_name": "SYSTEM",
            "entity_type": "ORGANIZATION",
            "description": "The system processes the user's request and asks for a preferred time."
        },
        {
            "entity_name": "BI-LSTM UTTERANCE ENCODER",
            "entity_type": "OBJECT",
            "description": "A component that encodes the user's utterance at turn k using a Bi-LSTM model."
        },
        {
            "entity_name": "LSTM DIALOGUE STATE",
            "entity_type": "OBJECT",
            "description": "A component that tracks the dialogue state using an LSTM model."
        },
        {
            "entity_name": "DIALOGUE STATE TRACKING",
            "entity_type": "OBJECT",
            "description": "A process that tracks the dialogue state and updates it based on the user's input and system's response."
        },
        {
            "entity_name": "KNOWLEDGE BASE",
            "entity_type": "OBJECT",
            "description": "A database containing information about movies, including their availability on specific dates."
        },
        {
            "entity_name": "POLICY NETWORK",
            "entity_type": "OBJECT",
            "description": "A component that decides the next action to take in the dialogue, such as asking for more information or providing an answer."
        },
        {
            "entity_name": "NATURAL LANGUAGE GENERATOR",
            "entity_type": "OBJECT",
            "description": "A component that generates natural language responses based on the policy network's decisions."
        }
    ],
    "image_2": [
        {
            "entity_name": "IMAGE_2",
            "entity_type": "ORI_IMG",
            "description": "The image is a flowchart illustrating the dialogue state and policy network in a conversational system. The diagram consists of several interconnected components. At the bottom, there is a rectangular box labeled 'LSTM Dialogue State, $S_{k}$'. Above this box, there are two other boxes: one on the left labeled 'Slot value logits' with a dashed outline containing multiple smaller rectangles representing different slot values, and one on the right labeled 'Query results encoding, $E_{k}$'. These three components feed into a larger box labeled 'Policy Network', which is depicted as a stack of three horizontal rectangles. The output of the Policy Network is a single rectangle labeled 'System action at turn k, $a_{k}$'. The arrows indicate the flow of information from the LSTM Dialogue State, through the Slot value logits and Query results encoding, to the Policy Network, and finally to the System action. The overall structure suggests a sequential process where the current dialogue state, along with the slot value logits and query results encoding, are used to determine the next system action."
        },
        {
            "entity_name": "SYSTEM ACTION AT TURN K",
            "entity_type": "EVENT",
            "description": "The system action at a specific turn in the dialogue process, denoted as \\(a_k\\)."
        },
        {
            "entity_name": "POLICY NETWORK",
            "entity_type": "ORGANIZATION",
            "description": "A network responsible for determining the system action based on the current state of the dialogue."
        },
        {
            "entity_name": "SLOT VALUE LOGITS",
            "entity_type": "OBJECT",
            "description": "The predicted values for slot values, represented as \\(v_k\\), which are used to inform the policy network."
        },
        {
            "entity_name": "QUERY RESULTS ENCODING",
            "entity_type": "OBJECT",
            "description": "The encoded representation of query results, denoted as \\(\\bar{E}_k\\), which provides context for the dialogue state."
        },
        {
            "entity_name": "LSTM DIALOGUE STATE",
            "entity_type": "OBJECT",
            "description": "The internal state of the dialogue maintained by an LSTM network, represented as \\(S_k\\), which captures the history and context of the conversation."
        }
    ],
    "image_3": [
        {
            "entity_name": "IMAGE_3",
            "entity_type": "ORI_IMG",
            "description": "The image is a table comparing the performance of different models on various metrics. The table has four rows and five columns. The first column lists the models: RNN, RNN+sem. dict, NBT, and Our SL model. The subsequent columns are labeled 'Area', 'Food', 'Price', and 'Joint'. The values in the 'Area' column are 92 for RNN, 92 for RNN+sem. dict, 90 for NBT, and 90 for Our SL model. The 'Food' column shows 86 for RNN, 86 for RNN+sem. dict, 84 for NBT, and 84 for Our SL model. The 'Price' column has 86 for RNN, 92 for RNN+sem. dict, 94 for NBT, and 92 for Our SL model. The 'Joint' column displays 69 for RNN, 71 for RNN+sem. dict, 72 for NBT, and 72 for Our SL model. The table highlights the performance differences among the models across different metrics."
        },
        {
            "entity_name": "MODEL",
            "entity_type": "ORGANIZATION",
            "description": "A table listing different models used for evaluation, including RNN, RNN+sem. dict, NBT, and Our SL model."
        },
        {
            "entity_name": "AREA",
            "entity_type": "EVENT",
            "description": "A category in the table representing a specific area of evaluation."
        },
        {
            "entity_name": "FOOD",
            "entity_type": "EVENT",
            "description": "A category in the table representing food-related evaluations."
        },
        {
            "entity_name": "PRICE",
            "entity_type": "EVENT",
            "description": "A category in the table representing price-related evaluations."
        },
        {
            "entity_name": "JOINT",
            "entity_type": "EVENT",
            "description": "A category in the table representing joint evaluations across multiple aspects."
        }
    ],
    "image_4": [
        {
            "entity_name": "IMAGE_4",
            "entity_type": "ORI_IMG",
            "description": "The image is a table labeled 'Table 1: Dialogue state tracking results on DSTC2'. The table is structured with two main columns: 'Goal slot' and 'Accuracy'. Under the 'Goal slot' column, there are six rows listing different parameters: 'Num of Tickets', 'Movie', 'Theater Name', 'Date', 'Time', and 'Joint'. Correspondingly, under the 'Accuracy' column, the values for each parameter are as follows: 'Num of Tickets' has an accuracy of 98.22%, 'Movie' has 91.86%, 'Theater Name' has 97.33%, 'Date' has 99.31%, 'Time' has 97.71%, and 'Joint' has 84.57%. The table provides detailed accuracy metrics for various goal slots in dialogue state tracking tasks."
        },
        {
            "entity_name": "GOAL SLOT",
            "entity_type": "ORGANIZATION",
            "description": "A table listing the accuracy of different goal slots in a ticket booking system."
        },
        {
            "entity_name": "NUM OF TICKETS",
            "entity_type": "PERSON",
            "description": "A goal slot with an accuracy of 98.22% indicating the system's success rate in determining the number of tickets requested."
        },
        {
            "entity_name": "MOVIE",
            "entity_type": "GEO",
            "description": "A goal slot with an accuracy of 91.86% indicating the system's success rate in identifying the correct movie."
        },
        {
            "entity_name": "THEATER NAME",
            "entity_type": "EVENT",
            "description": "A goal slot with an accuracy of 97.33% indicating the system's success rate in identifying the correct theater name."
        },
        {
            "entity_name": "DATE",
            "entity_type": "PERSON",
            "description": "A goal slot with an accuracy of 99.31% indicating the system's success rate in identifying the correct date."
        },
        {
            "entity_name": "TIME",
            "entity_type": "GEO",
            "description": "A goal slot with an accuracy of 97.71% indicating the system's success rate in identifying the correct time."
        },
        {
            "entity_name": "JOINT",
            "entity_type": "EVENT",
            "description": "A goal slot with an accuracy of 84.57% indicating the system's success rate in jointly identifying all parameters correctly."
        }
    ],
    "image_5": [
        {
            "entity_name": "IMAGE_5",
            "entity_type": "ORI_IMG",
            "description": "The image is a line graph titled 'Task Success Rate over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10000. The y-axis represents the task success rate, ranging from 0.3 to 0.7. Four different curves are plotted on the graph, each representing a different model configuration: SL Baseline (dotted cyan line), SL + RL (solid red line with pentagon markers), SL + IL 500 + RL (solid yellow line with triangle markers), and SL + IL 1000 + RL (solid blue line with star markers). The SL Baseline curve remains relatively flat near the bottom of the graph, indicating a low and stable task success rate. The SL + RL curve starts lower but gradually increases, reaching a plateau around 0.55. The SL + IL 500 + RL curve starts higher than the SL + RL curve and also increases, reaching a plateau around 0.62. The SL + IL 1000 + RL curve starts the highest and increases further, reaching a plateau around 0.65. Two specific points are highlighted: one on the SL + IL 1000 + RL curve at around 2000 sessions with a task success rate of approximately 0.6, and another on the SL + IL 500 + RL curve at around 2000 sessions with a task success rate of approximately 0.58."
        },
        {
            "entity_name": "SL BASELINE",
            "entity_type": "ORGANIZATION",
            "description": "Represents the baseline performance of supervised learning without reinforcement learning enhancements. The task success rate remains relatively constant around 0.35 throughout the interactive dialogue learning sessions."
        },
        {
            "entity_name": "SL + RL",
            "entity_type": "ORGANIZATION",
            "description": "Indicates the performance when supervised learning is combined with reinforcement learning. The task success rate starts low but increases over time, reaching a plateau around 0.55 after approximately 6000 sessions."
        },
        {
            "entity_name": "SL + IL 500 + RL",
            "entity_type": "ORGANIZATION",
            "description": "Shows the performance when supervised learning is augmented with imitation learning for the first 500 sessions and then reinforced with reinforcement learning. The task success rate improves more rapidly initially and stabilizes around 0.62."
        },
        {
            "entity_name": "SL + IL 1000 + RL",
            "entity_type": "ORGANIZATION",
            "description": "Demonstrates the performance when supervised learning is enhanced with imitation learning for the first 1000 sessions before reinforcement learning is applied. This approach leads to the highest task success rate, which plateaus around 0.67."
        }
    ],
    "image_6": [
        {
            "entity_name": "IMAGE_6",
            "entity_type": "ORI_IMG",
            "description": "The image is a line graph titled 'Average Turn Size over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis represents the average turn size, ranging from 6.0 to 9.0. There are four lines representing different models: SL Baseline (dashed cyan line with star markers), SL + RL (solid red line with square markers), SL + IL 500 + RL (solid yellow line with triangle markers), and SL + IL 1000 + RL (solid blue line with pentagon markers). The SL Baseline line remains relatively constant at around 8.5 throughout the sessions. The SL + RL line starts at around 8.5 and gradually decreases to about 7.0 by 10,000 sessions. The SL + IL 500 + RL line starts at around 8.5 and decreases more slowly to about 7.5 by 10,000 sessions. The SL + IL 1000 + RL line starts at around 8.5 and decreases more gradually to about 7.5 by 10,000 sessions. The graph shows that the addition of imitation learning (IL) and reinforcement learning (RL) helps in reducing the average turn size over time, with the SL + IL 1000 + RL model showing the most gradual decrease."
        },
        {
            "entity_name": "GRAPH",
            "entity_type": "EVENT",
            "description": "A graph showing the average turn size over time for different learning methods in interactive dialogue learning sessions."
        },
        {
            "entity_name": "SL BASELINE",
            "entity_type": "ORGANIZATION",
            "description": "The baseline method using supervised learning."
        },
        {
            "entity_name": "SL + RL",
            "entity_type": "ORGANIZATION",
            "description": "The method combining supervised learning with reinforcement learning."
        },
        {
            "entity_name": "SL + IL 500 + RL",
            "entity_type": "ORGANIZATION",
            "description": "The method combining supervised learning, imitation learning with 500 sessions, and reinforcement learning."
        },
        {
            "entity_name": "SL + IL 1000 + RL",
            "entity_type": "ORGANIZATION",
            "description": "The method combining supervised learning, imitation learning with 1000 sessions, and reinforcement learning."
        }
    ],
    "image_7": [
        {
            "entity_name": "IMAGE_7",
            "entity_type": "ORI_IMG",
            "description": "The image is a line graph titled 'Average DST Accuracy over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis represents the average DST accuracy, ranging from 0.50 to 0.80. Four different lines are plotted on the graph, each representing a different training method: SL Baseline (dotted cyan line with star markers), SL + RL (solid red line with square markers), SL + IL 500 + RL (solid yellow line with triangle markers), and SL + IL 1000 + RL (solid blue line with circle markers). The SL Baseline line remains constant at approximately 0.50 throughout the sessions. The SL + RL line starts at around 0.55 and gradually increases to about 0.65 by 10,000 sessions. The SL + IL 500 + RL line starts at around 0.70 and increases to about 0.75 by 10,000 sessions. The SL + IL 1000 + RL line starts at around 0.75 and maintains this level throughout the sessions. There are two circles highlighting specific points on the SL + IL 500 + RL and SL + IL 1000 + RL lines, indicating significant data points."
        },
        {
            "entity_name": "GRAPH",
            "entity_type": "EVENT",
            "description": "A graph displaying the average DST accuracy over time for different training methods in interactive dialogue learning sessions."
        },
        {
            "entity_name": "SL BASELINE",
            "entity_type": "ORGANIZATION",
            "description": "The baseline method using supervised learning (SL) without reinforcement learning (RL)."
        },
        {
            "entity_name": "SL + RL",
            "entity_type": "ORGANIZATION",
            "description": "The method combining supervised learning (SL) with reinforcement learning (RL)."
        },
        {
            "entity_name": "SL + IL 500 + RL",
            "entity_type": "ORGANIZATION",
            "description": "The method combining supervised learning (SL), initial learning (IL) with 500 sessions, and reinforcement learning (RL)."
        },
        {
            "entity_name": "SL + IL 1000 + RL",
            "entity_type": "ORGANIZATION",
            "description": "The method combining supervised learning (SL), initial learning (IL) with 1000 sessions, and reinforcement learning (RL)."
        }
    ],
    "image_8": [
        {
            "entity_name": "IMAGE_8",
            "entity_type": "ORI_IMG",
            "description": "The image is a line graph titled 'Task Success Rate over Time (smoothed)'. The x-axis represents the number of interactive dialogue learning sessions, ranging from 0 to 10,000. The y-axis represents the task success rate, ranging from 0.3 to 0.7. There are five different lines representing different RL training settings: SL Baseline (dotted cyan line with crosses), SL + policy-only RL (solid red line with pentagons), SL + end-to-end RL (solid blue line with stars), SL + IL 1000 + policy-only RL (dashed blue line with stars), and SL + IL 1000 + end-to-end RL (solid blue line with stars). The SL Baseline starts at around 0.32 and remains relatively flat throughout the sessions. The SL + policy-only RL starts at around 0.35 and shows a gradual increase, reaching around 0.55 by 10,000 sessions. The SL + end-to-end RL starts at around 0.35 and shows a more significant increase, reaching around 0.65 by 10,000 sessions. The SL + IL 1000 + policy-only RL starts at around 0.45 and shows a steady increase, reaching around 0.6 by 10,000 sessions. The SL + IL 1000 + end-to-end RL starts at around 0.45 and shows the highest increase, reaching around 0.68 by 10,000 sessions."
        },
        {
            "entity_name": "GRAPH",
            "entity_type": "EVENT",
            "description": "A graph showing the task success rate over time for different learning methods in interactive dialogue learning sessions."
        },
        {
            "entity_name": "SL BASELINE",
            "entity_type": "UNKNOWN",
            "description": "The SL Baseline is a data point on the graph representing the supervised learning baseline performance."
        },
        {
            "entity_name": "SL + POLICY-ONLY RL",
            "entity_type": "UNKNOWN",
            "description": "The SL + policy-only RL is a data line on the graph representing the performance of combining supervised learning with policy-only reinforcement learning."
        },
        {
            "entity_name": "SL + END-TO-END RL",
            "entity_type": "UNKNOWN",
            "description": "The SL + end-to-end RL is a data line on the graph representing the performance of combining supervised learning with end-to-end reinforcement learning."
        },
        {
            "entity_name": "SL + IL 1000 + POLICY-ONLY RL",
            "entity_type": "UNKNOWN",
            "description": "The SL + IL 1000 + policy-only RL is a data line on the graph representing the performance of combining supervised learning with imitation learning and policy-only reinforcement learning."
        },
        {
            "entity_name": "SL + IL 1000 + END-TO-END RL",
            "entity_type": "UNKNOWN",
            "description": "The SL + IL 1000 + end-to-end RL is a data line on the graph representing the performance of combining supervised learning with imitation learning and end-to-end reinforcement learning."
        }
    ],
    "image_9": [
        {
            "entity_name": "IMAGE_9",
            "entity_type": "ORI_IMG",
            "description": "The image is a table labeled 'Table 3: Human evaluation results.' It presents the mean and standard deviation of crowd worker scores for different models used in training a dialogue system. The table has two columns: 'Model' and 'Score.' The rows under 'Model' are as follows: 'SL' with a score of 3.987 ± 0.086, 'SL + IL 1000' with a score of 4.378 ± 0.082, and 'SL + IL 1000 + RL' with a score of 4.603 ± 0.067. The scores range from 1 to 5, where 1 indicates a frustrating interaction and 5 indicates an optimal way to help the user. Each turn is rated by 3 different judges, and the data is collected from 100 dialogues for each model. The table highlights that the addition of interactive learning (IL) and reinforcement learning (RL) improves the quality of the model according to human judges."
        },
        {
            "entity_name": "TABLE",
            "entity_type": "EVENT",
            "description": "A table displaying the scores of different models in a machine learning context. The table includes three rows for models labeled SL, SL + IL 1000, and SL + IL 1000 + RL, with corresponding scores and standard deviations."
        },
        {
            "entity_name": "SL",
            "entity_type": "UNKNOWN",
            "description": "The table contains the score for the SL model."
        },
        {
            "entity_name": "SL + IL 1000",
            "entity_type": "UNKNOWN",
            "description": "The table contains the score for the SL + IL 1000 model."
        },
        {
            "entity_name": "SL + IL 1000 + RL",
            "entity_type": "UNKNOWN",
            "description": "The table contains the score for the SL + IL 1000 + RL model."
        }
    ]
}