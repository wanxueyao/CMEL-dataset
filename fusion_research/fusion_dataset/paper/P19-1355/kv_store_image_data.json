{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1355/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Even when these expensive computational resources are available, model training also incurs a substantial cost to the environment due to the energy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset resources, the high energy demands of these models are still a concern since (1) energy NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.   2015; Dozat and Manning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now requires substantial computational resources which demand considerable energy, along with the associated financial and environmental costs. Research and development of new models multiplies these costs by thousands of times by requiring retraining to experiment with model architectures and hyperparameters. Whereas a decade ago most Table 1: Estimated $\\mathrm{CO_{2}}$ emissions from training common NLP models, compared to familiar consumption. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-c474bd136004e0c87233db96a8e12b49",
        "description": "The image is a table that provides estimated CO2 emissions in pounds (lbs) for various types of consumption. The table has two columns: 'Consumption' and 'CO2e (lbs)'. The rows under 'Consumption' include 'Air travel, 1 person, NY↔SF', 'Human life, avg, 1 year', 'American life, avg, 1 year', and 'Car, avg incl. fuel, 1 lifetime'. The corresponding values in 'CO2e (lbs)' are 1984 lbs for air travel between New York and San Francisco, 11,023 lbs for the average human life per year, 36,156 lbs for the average American life per year, and 126,000 lbs for the average car including fuel over its lifetime. The table highlights the significant differences in CO2 emissions across different forms of consumption, with air travel having relatively low emissions compared to the lifetime emissions of a car.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1355/images/image_2.jpg",
        "caption": [
            "Training one model (GPU) "
        ],
        "footnote": [],
        "context": "Even when these expensive computational resources are available, model training also incurs a substantial cost to the environment due to the energy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset resources, the high energy demands of these models are still a concern since (1) energy NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances.  2015; Dozat and Manning, 2017; Vaswani et al., 2017), with the most computationally-hungry models obtaining the highest scores (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; So et al., 2019). As a result, training a state-of-the-art model now requires substantial computational resources which demand considerable energy, along with the associated financial and environmental costs. Research and development of new models multiplies these costs by thousands of times by requiring retraining to experiment with model architectures and hyperparameters. Whereas a decade ago most Table 1: Estimated $\\mathrm{CO_{2}}$ emissions from training common NLP models, compared to familiar consumption.  ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-c474bd136004e0c87233db96a8e12b49",
        "description": "The image is a table that provides the estimated CO2 emissions from training common NLP models. The table has two main rows: 'NLP pipeline (parsing, SRL)' and 'Transformer (big)'. Each row has two sub-rows indicating different conditions or configurations. For the 'NLP pipeline (parsing, SRL)', the first sub-row shows a value of 39, and the second sub-row, which includes tuning and experiments, shows a significantly higher value of 78,468. For the 'Transformer (big)', the first sub-row shows a value of 192, and the second sub-row, which includes neural architecture search, shows an even higher value of 626,155. The values likely represent the CO2 emissions in some unit, possibly kilograms or metric tons, associated with the training process under these conditions.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1355/images/image_3.jpg",
        "caption": [
            "Consumer Renew. Gas Coal Nuc. "
        ],
        "footnote": [
            "Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States,4 China5 and Germany (Burger, 2019). "
        ],
        "context": "We estimate the total time expected for models to train to completion using training times and hardware reported in the original papers. We then calculate the power consumption in kilowatt-hours $\\mathrm{(kWh)}$ as follows. Let $p_{c}$ be the average power draw (in watts) from all CPU sockets during training, let $p_{r}$ be the average power draw from all DRAM (main memory) sockets, let $p_{g}$ be the average power draw of a GPU during training, and let $g$ be the number of GPUs used to train. We estimate total power consumption as combined GPU, CPU and DRAM consumption, then multiply this by experimentation. We measure energy use as follows. We train the models described in $\\S2.1$ using the default settings provided, and sample GPU and CPU power consumption during training. Each model was trained for a maximum of 1 day. We train all models on a single NVIDIA Titan X GPU, with the exception of ELMo which was trained on 3 NVIDIA GTX 1080 Ti GPUs. While training, we repeatedly query the NVIDIA System Management Interface to sample the GPU power consumption and report the average over all samples. To sample CPU power consumption, we use Intel’s Running Average Power Limit interface. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-c474bd136004e0c87233db96a8e12b49",
        "description": "The image is a table labeled 'Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers (Cook et al., 2017), compared to the United States, China, and Germany (Burger, 2019)'. The table is structured with four main columns: Renew., Gas, Coal, and Nuc., representing the percentage of energy sourced from renewable sources, natural gas, coal, and nuclear power, respectively. Each row represents a different entity or country. The rows are as follows: China (Renew.: 22%, Gas: 3%, Coal: 65%, Nuc.: 4%), Germany (Renew.: 40%, Gas: 7%, Coal: 38%, Nuc.: 13%), United States (Renew.: 17%, Gas: 35%, Coal: 27%, Nuc.: 19%), Amazon-AWS (Renew.: 17%, Gas: 24%, Coal: 30%, Nuc.: 26%), Google (Renew.: 56%, Gas: 14%, Coal: 15%, Nuc.: 10%), and Microsoft (Renew.: 32%, Gas: 23%, Coal: 31%, Nuc.: 10%). The table highlights the significant differences in energy sourcing among these entities and countries, with notable data points such as Google's high reliance on renewable energy (56%) and the United States' heavy use of natural gas (35%).",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1355/images/image_4.jpg",
        "caption": [
            "Model Hardware Power (W) Hours kWhPUE CO2e Cloud compute cost "
        ],
        "footnote": [
            "Table 3: Estimated cost of training a model in terms of $\\mathrm{CO_{2}}$ emissions (lbs) and cloud compute cost (USD).7 Power and carbon footprint are omitted for TPUs due to lack of public information on power draw for this hardware. "
        ],
        "context": "Table 3 lists $\\mathrm{{CO}_{2}}$ emissions and estimated cost of training the models described in $\\S2.1$ . Of note is that TPUs are more cost-efficient than GPUs on workloads that make sense for that hardware (e.g. BERT). We also see that models emit substantial carbon emissions; training BERT on GPU is roughly equivalent to a trans-American flight. So et al. (2019) report that NAS achieves a new stateof-the-art BLEU score of 29.7 for English to German machine translation, an increase of just 0.1 BLEU at the cost of at least $\\mathbb{S}150\\mathbf{k}$ in on-demand compute 4.1 Cost of training 4 Experimental results required during inference. They also measure average power draw required during inference on GPUs as a function of batch size. Neither work analyzes the recurrent and self-attention models that have become commonplace in NLP, nor do they extrapolate power to estimates of carbon and dollar cost of training. Analysis of hyperparameter tuning has been performed in the context of improved algorithms for hyperparameter search (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). To our knowledge there exists to date no analysis of the computation required for R&D and hyperparameter tuning of neural network models in NLP. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-3eec394eafb261cc01db2d32f0050eb0",
        "description": "The image is a table labeled 'Table 3: Estimated cost of training a model in terms of CO2 emissions (lbs) and cloud compute cost (USD).' The table is structured with the following columns: Model, Hardware, Power (W), Hours, kWhPUE, CO2e, and Cloud compute cost. Each row represents a different model and its corresponding hardware specifications and costs. The models listed are T2T_base, T2T_big, ELMo, BERT_base (with two different hardware configurations), NAS (with two different hardware configurations), and GPT-2. The power consumption for TPUs is omitted due to lack of public information. Significant data points include: T2T_base on P100x8 hardware consumes 1415.78 W over 12 hours, resulting in 27 kWhPUE and 26 lbs of CO2e, with a cloud compute cost of $41-$140. T2T_big on the same hardware consumes 1515.43 W over 84 hours, resulting in 201 kWhPUE and 192 lbs of CO2e, with a cloud compute cost of $289-$981. ELMo on P100x3 hardware consumes 517.66 W over 336 hours, resulting in 275 kWhPUE and 262 lbs of CO2e, with a cloud compute cost of $433-$1472. BERT_base on V100x64 hardware consumes 12,041.51 W over 79 hours, resulting in 1507 kWhPUE and 1438 lbs of CO2e, with a cloud compute cost of $3751-$12,571. BERT_base on TPUv2x16 hardware has no power consumption listed but takes 96 hours, with a cloud compute cost of $2074-$6912. NAS on P100x8 hardware consumes 1515.43 W over 274,120 hours, resulting in 656,347 kWhPUE and 626,155 lbs of CO2e, with a cloud compute cost of $942,973-$3,201,722. NAS on TPUv2x1 hardware has no power consumption listed but takes 32,623 hours, with a cloud compute cost of $44,055-$146,848. GPT-2 on TPUv3x32 hardware has no power consumption listed but takes 168 hours, with a cloud compute cost of $12,902-$43,008.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1355/images/image_5.jpg",
        "caption": [
            "Estimated cost (USD) "
        ],
        "footnote": [
            "Table 4: Estimated cost in terms of cloud compute and electricity for training: (1) a single model (2) a single tune and (3) all models trained during R&D. "
        ],
        "context": "Our experiments suggest that it would be benefi- cial to directly compare different models to perform a cost-benefit (accuracy) analysis. To address this, when proposing a model that is meant to be re-trained for downstream use, such as retraining on a new domain or fine-tuning on a new task, authors should report training time and computational resources required, as well as model sensitivity to hyperparameters. This will enable direct comparison across models, allowing subsequent consumers of these models to accurately assess whether the required computational resources are compatible with Authors should report training time and sensitivity to hyperparameters. 5 Conclusions  GPUs. The sum GPU time required for the project totaled 9998 days (27 years). This averages to about 60 GPUs running constantly throughout the 6 month duration of the project. Table 4 lists upper and lower bounds of the estimated cost in terms of Google Cloud compute and raw electricity required to develop and deploy this model. We see that while training a single model is relatively inexpensive, the cost of tuning a model for a new dataset, which we estimate here to require 24 jobs, or performing the full R&D required to develop this model, quickly becomes extremely expensive. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-3eec394eafb261cc01db2d32f0050eb0",
        "description": "The image is a table labeled 'Estimated cost (USD)' which provides the estimated costs in terms of cloud compute and electricity for training different models. The table has four columns: Models, Hours, Cloud, and Electric. The rows represent different scenarios: (1) a single model, (2) a single tune, and (3) all models trained during R&D. The first row shows that for a single model, it takes 120 hours with an estimated cloud cost ranging from $52 to $175 and an electric cost of $5. The second row indicates that for 24 models, it takes 2880 hours with an estimated cloud cost ranging from $1238 to $4205 and an electric cost of $118. The third row shows that for 4789 models, it takes 239,942 hours with an estimated cloud cost ranging from $103k to $350k and an electric cost of $9870. The table highlights the significant increase in cost as the number of models increases, especially in terms of cloud computing.",
        "segmentation": false
    }
}