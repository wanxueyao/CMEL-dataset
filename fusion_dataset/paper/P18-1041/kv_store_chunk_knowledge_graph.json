{
    "1": {
        "chunk_key": "chunk-e0ebb8ded3819f66510c1caedfb42c07",
        "entities": [
            {
                "entity_name": "\"LI ET AL., 2018\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Li et al., 2018 is a group of researchers who developed the subspace training method to constrain trainable parameters in text classification tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"PARIKH ET AL., 2016\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Parikh et al., 2016 conducted studies suggesting that simpler word-embedding-based architectures can exhibit comparable or superior performance in certain NLP applications.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"VASWANI ET AL., 2017\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Vaswani et al., 2017 is a group of researchers whose work is cited in the context of comparing the performance of simpler word-embedding-based architectures with more sophisticated models.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"MITCHELL AND LAPATA, 2010\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Mitchell and Lapata, 2010 conducted studies showing that the advantages of distinct compositional functions are highly dependent on the specific task in NLP.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"IYYER ET AL., 2015\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Iyyer et al., 2015 is a group of researchers whose work is cited in relation to the advantages of distinct compositional functions and their dependence on specific NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"ZHANG ET AL., 2015A\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Zhang et al., 2015a conducted studies showing that the advantages of distinct compositional functions are highly dependent on the specific task in NLP.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"WIETING ET AL., 2015\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Wieting et al., 2015 is a group of researchers whose work is cited in the context of the advantages of distinct compositional functions and their dependence on specific NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"ARORA ET AL., 2016\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Arora et al., 2016 is a group of researchers whose work is cited in relation to the advantages of distinct compositional functions and their dependence on specific NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"JOULIN ET AL., 2016\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Joulin et al., 2016 is a group of researchers who developed fastText, a model that uses average pooling to achieve promising results on certain NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"HOCHREITER AND SCHMIDHUBER, 1997\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Hochreiter and Schmidhuber, 1997 are the researchers who introduced Long Short-Term Memory (LSTM), a model used to address the issue of learning long-term dependencies in text sequences.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"GRAVES ET AL., 2013\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Graves et al., 2013 is a group of researchers whose work provides further explanation on Long Short-Term Memory (LSTM), a model used in recurrent sequence encoders.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"KIM, 2014\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Kim, 2014 is a researcher whose work on Convolutional Neural Network (CNN) architecture is cited in the context of encoding text sequences.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"COLLOBERT ET AL., 2011\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Collobert et al., 2011 is a group of researchers whose work on Convolutional Neural Network (CNN) architecture is cited in the context of encoding text sequences.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"GAN ET AL., 2017\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Gan et al., 2017 is a group of researchers whose work on Convolutional Neural Network (CNN) architecture is cited in the context of encoding text sequences.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"ZHANG ET AL.,(\"ENTITY\"",
                "entity_type": "\"SHEN ET AL., 2018\"",
                "description": "\"organization\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"CONNEAU ET AL\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Conneau et al is a group of researchers who have developed Deep CNN text models, expanding on the single-layer CNN text models.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"OCCAM'S RAZOR\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Occam's razor is a principle that suggests simple models are preferred when they can perform as well as more complex ones, as mentioned in the context of model selection in NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"SWEMS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEMs (Simple Word Embedding Models) are compared to other models like DAN and fastText, and are discussed in terms of their performance on various NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"DEEP AVERAGING NETWORK (DAN)\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Deep Averaging Network (DAN) is a model that, along with fastText, is discussed for its use of average pooling in achieving promising results on certain NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"FASTTEXT\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"fastText is a model developed by Joulin et al., 2016, which uses average pooling and is compared with SWEMs in terms of performance on NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"RECURRENT SEQUENCE ENCODER\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Recurrent Sequence Encoder is a compositional function defined in a recurrent manner, often using LSTM to update the current hidden unit in the sequence.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"LONG SHORT-TERM MEMORY (LSTM)\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Long Short-Term Memory (LSTM) is a model that employs gates to control the flow of information abstracted from a sequence, used in Recurrent Sequence Encoders.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"CONVOLUTIONAL SEQUENCE ENCODER\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Convolutional Sequence Encoder is a strategy used to encode text sequences by applying filters to windows of words and aggregating feature maps to abstract semantic features.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"NATURAL LANGUAGE PROCESSING (NLP)\"",
                "entity_type": "\"FIELD\"",
                "description": "\"Natural Language Processing (NLP) is the field in which the discussed models and techniques are applied, focusing on developing expressive and computationally efficient functions to capture linguistic structures.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"COMPOSITIONAL FUNCTIONS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Compositional functions are discussed in the context of NLP as a means to combine word embeddings into a fixed-length sentence/document representation for making predictions.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"WORD EMBEDDINGS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Word Embeddings are discussed as a fundamental component in various models, where they represent words in a vector space and are combined through compositional functions.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "entity_name": "\"ATTENTION LAYERS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Attention layers are additional modules employed on top of the word embedding layer in some models to improve performance on NLP tasks.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            }
        ],
        "relationships": [
            {
                "src_id": "\"SWEMS\"",
                "tgt_id": "\"DEEP AVERAGING NETWORK (DAN)\"",
                "weight": 7.0,
                "description": "\"SWEMs bear close resemblance to Deep Averaging Network (DAN), indicating a similarity in approach between the two models.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "src_id": "\"SWEMS\"",
                "tgt_id": "\"FASTTEXT\"",
                "weight": 7.0,
                "description": "\"SWEMs are compared to fastText, showing that both use simple pooling operations but SWEMs explore a series of pooling operations beyond average-pooling.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            },
            {
                "src_id": "\"RECURRENT SEQUENCE ENCODER\"",
                "tgt_id": "\"LONG SHORT-TERM MEMORY (LSTM)\"",
                "weight": 8.0,
                "description": "\"Recurrent Sequence Encoder often uses Long Short-Term Memory (LSTM) as the transition function to address the issue of learning long-term dependencies.\"",
                "source_id": "chunk-e0ebb8ded3819f66510c1caedfb42c07"
            }
        ]
    },
    "4": {
        "chunk_key": "chunk-156a1e35714463aabe81efd03d2bb137",
        "entities": [
            {
                "entity_name": "\"SWEM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM is an organization or model variant mentioned in the context of sentiment analysis and text sequence matching.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"PANG ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Pang et al. are researchers who hypothesize that positional information of words in text sequences may be beneficial to predict sentiment.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"CNN\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"CNN refers to Convolutional Neural Networks, a type of model that can capture word-order information via convolutional filters.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"LSTM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"LSTM refers to Long Short-Term Memory models, which can capture word-order information via recurrent transition functions.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"SWEM-MAX\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM-max is a variant of the SWEM model that generally performs slightly worse than SWEM-aver but extracts complementary features.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"SWEM-AVER\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM-aver is a variant of the SWEM model that generally performs better than SWEM-max.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"SWEM-CONCAT\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM-concat is a variant of the SWEM model that combines features from SWEM-max and SWEM-aver, showing the best performance among SWEM variants.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"GLOVE\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"GloVe is a word embedding model that provides denser embeddings compared to those learned from SWEM-max.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"YAHOO\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Yahoo is the source of the datasets used for training the SWEM-max model and for comparison with GloVe embeddings.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"NIE AND BANSAL\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Nie and Bansal are researchers whose findings support the performance of max-pooling over BiLSTM hidden units on the SNLI dataset.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"CONNEAU ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Conneau et al. are researchers whose findings support the performance of max-pooling over BiLSTM hidden units on the SNLI dataset.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"PARIKH ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Parikh et al. are researchers who suggest that word-level alignments between two sequences are sufficient for sentence matching tasks.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"WIKIQA\"",
                "entity_type": "\"EVENT\"",
                "description": "\"WikiQA is a dataset where SWEM did not demonstrate the best results compared to CNN or LSTM models.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"SNLI\"",
                "entity_type": "\"EVENT\"",
                "description": "\"SNLI is a dataset where SWEM-max performed the best among all SWEM variants, consistent with findings by Nie and Bansal and Conneau et al.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"MULTINLI\"",
                "entity_type": "\"EVENT\"",
                "description": "\"MultiNLI is a dataset used for training, where results are reported without training data from SNLI.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"MSRP\"",
                "entity_type": "\"EVENT\"",
                "description": "\"MSRP is a dataset used for training, following the setup by Hu et al. without using any additional features.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"WILLIAMS ET AL.\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Williams et al. are researchers who reported results for Bidirectional LSTM on matching natural language sentences.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"POLITICS & GOVERNMENT\"",
                "entity_type": "\"GEO\"",
                "description": "\"Politics & Government is a topic inferred from words in the first column of Table 3, indicating a common theme among the selected words.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"SCIENCE\"",
                "entity_type": "\"GEO\"",
                "description": "\"Science is a topic that includes words related to Chemistry, which are learned by the model even without explicit label information.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"CHEMISTRY\"",
                "entity_type": "\"GEO\"",
                "description": "\"Chemistry is a field related to words in the fifth column of Table 3, indicating a common topic among them.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"TABLE 3\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Table 3 is a reference to a table in the text that shows words with the largest values among the entire vocabulary for each word embedding dimension.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"FIGURE 1\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Figure 1 is a reference to a figure in the text that shows histograms for learned word embeddings of SWEM-max and GloVe embeddings for the same vocabulary.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"YAHOO! ANSWER\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Yahoo! Answer is the source of the dataset used for training the SWEM-max model and for comparison with GloVe embeddings.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "entity_name": "\"BIDIRECTIONAL LSTM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Bidirectional LSTM is a model whose performance on matching natural language sentences is reported by Williams et al.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            }
        ],
        "relationships": [
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"PANG ET AL.\"",
                "weight": 6.0,
                "description": "\"SWEM is related to Pang et al. as their hypothesis about positional information of words in text sequences is consistent with SWEM's findings.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"CNN\"",
                "weight": 5.0,
                "description": "\"SWEM is compared with CNN models in terms of capturing word-order information.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"LSTM\"",
                "weight": 5.0,
                "description": "\"SWEM is compared with LSTM models in terms of capturing word-order information.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"SWEM-AVER\"",
                "weight": 7.0,
                "description": "\"SWEM-max extracts complementary features from SWEM-aver, and together they form SWEM-concat which shows the best performance.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"YAHOO! ANSWER\"",
                "weight": 7.0,
                "description": "\"SWEM-max model was trained on the Yahoo! Answer dataset.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"GLOVE\"",
                "tgt_id": "\"YAHOO! ANSWER\"",
                "weight": 7.0,
                "description": "\"GloVe word embeddings are compared with those learned from SWEM-max, which were also trained on the Yahoo! Answer dataset.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"TABLE 3\"",
                "weight": 7.0,
                "description": "\"SWEM-max model's learned word embeddings are used to select words for Table 3 based on their largest values among the entire vocabulary.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"FIGURE 1\"",
                "weight": 7.0,
                "description": "\"SWEM-max model's learned word embeddings are compared with GloVe embeddings in Figure 1.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"POLITICS & GOVERNMENT\"",
                "tgt_id": "\"TABLE 3\"",
                "weight": 8.0,
                "description": "\"Words in the first column of Table 3 are all political terms, which could be assigned to the Politics & Government topic.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"CHEMISTRY\"",
                "tgt_id": "\"TABLE 3\"",
                "weight": 8.0,
                "description": "\"All words in the fifth column of Table 3 are Chemistry-related, indicating a common topic.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            },
            {
                "src_id": "\"SCIENCE\"",
                "tgt_id": "\"CHEMISTRY\"",
                "weight": 7.0,
                "description": "\"Chemistry-related words in Table 3 should belong to the Science topic, even without an explicit chemistry label in the dataset.\"",
                "source_id": "chunk-156a1e35714463aabe81efd03d2bb137"
            }
        ]
    },
    "5": {
        "chunk_key": "chunk-f513c5de611803566998ac79f309def6",
        "entities": [
            {
                "entity_name": "\"SWEM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM is a model that is compared with CNN and LSTM models in terms of performance on various tasks, particularly in sentiment analysis.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"CNN\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"CNN is a model used for comparison with SWEM and LSTM, capturing word-order information within text sequences.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"LSTM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"LSTM is a model that captures word-order information and is used for comparison with SWEM on various tasks, including sentiment analysis.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"YAHOO\"",
                "entity_type": "\"GEO\"",
                "description": "\"Yahoo is one of the datasets used to evaluate the performance of LSTM and SWEM models, particularly for topic categorization and textual entailment.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SNLI\"",
                "entity_type": "\"GEO\"",
                "description": "\"SNLI is one of the datasets used to evaluate the performance of LSTM and SWEM models, particularly for topic categorization and textual entailment.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"YELP POLARITY\"",
                "entity_type": "\"GEO\"",
                "description": "\"Yelp Polarity is a dataset used to evaluate the performance of LSTM and SWEM models, particularly for sentiment analysis.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"MR\"",
                "entity_type": "\"GEO\"",
                "description": "\"MR is a sentiment classification dataset used to evaluate the performance of SWEM on short sentence processing tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SST-1\"",
                "entity_type": "\"GEO\"",
                "description": "\"SST-1 is a sentiment classification dataset used to evaluate the performance of SWEM on short sentence processing tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SST-2\"",
                "entity_type": "\"GEO\"",
                "description": "\"SST-2 is a sentiment classification dataset used to evaluate the performance of SWEM on short sentence processing tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SUBJ\"",
                "entity_type": "\"GEO\"",
                "description": "\"Subj is a subjectivity classification dataset used to evaluate the performance of SWEM on short sentence processing tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"TREC\"",
                "entity_type": "\"GEO\"",
                "description": "\"TREC is a question classification dataset used to evaluate the performance of SWEM on short sentence processing tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"CONLL2000\"",
                "entity_type": "\"GEO\"",
                "description": "\"CoNLL2000 is a chunking dataset used to evaluate the performance of SWEM on sequence tagging tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"CONLL2003\"",
                "entity_type": "\"GEO\"",
                "description": "\"CoNLL2003 is a NER dataset used to evaluate the performance of SWEM on sequence tagging tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SWEM-HIER\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM-hier is a variant of the SWEM model that incorporates hierarchical pooling operation and shows improved performance in sentiment analysis tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SECTION 4.2.1\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Section 4.2.1 is where the importance of word-order information for sentiment analysis tasks is demonstrated in the document.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SECTION 3.3\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Section 3.3 details the hierarchical pooling operation used in SWEM-hier, which is crucial for its performance in sentiment analysis.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SECTION 4.3\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Section 4.3 discusses the performance of SWEM-hier on sentiment analysis tasks and its comparison with other SWEM variants.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"TABLE 6\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Table 6 presents the test accuracy for LSTM model trained on original and shuffled training sets, indicating the importance of word-order information.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"TABLE 7\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Table 7 shows test samples from Yelp Polarity dataset where LSTM gives wrong predictions with shuffled training data but predicts correctly with the original training set.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"TABLE 8\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Table 8 presents test accuracies with different compositional functions on short sentence classifications, comparing SWEM with CNN/LSTM.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"SUBSPACE TRAINING\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Subspace training is a method used to measure model complexity in text classification problems, as discussed in Section 5.1.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "entity_name": "\"LI ET AL., 2018\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Li et al., 2018 is referenced for the subspace training method used to measure model complexity in text classification problems.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            }
        ],
        "relationships": [
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"CNN\"",
                "weight": 7.0,
                "description": "\"SWEM is compared with CNN in terms of performance on various tasks, with SWEM showing similar or superior performance in some cases.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"LSTM\"",
                "weight": 8.0,
                "description": "\"SWEM is compared with LSTM in terms of performance on various tasks, with SWEM showing similar or superior performance in some cases, but LSTM captures word-order information.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"LSTM\"",
                "tgt_id": "\"YAHOO\"",
                "weight": 6.0,
                "description": "\"LSTM is used to evaluate performance on the Yahoo dataset, showing comparable accuracies on original and shuffled training sets.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"LSTM\"",
                "tgt_id": "\"SNLI\"",
                "weight": 6.0,
                "description": "\"LSTM is used to evaluate performance on the SNLI dataset, showing comparable accuracies on original and shuffled training sets.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"LSTM\"",
                "tgt_id": "\"YELP POLARITY\"",
                "weight": 9.0,
                "description": "\"LSTM shows a noticeable drop in performance on the Yelp Polarity dataset when trained on shuffled data, indicating the importance of word-order information for sentiment analysis.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"YELP POLARITY\"",
                "weight": 7.0,
                "description": "\"SWEM's performance on the Yelp Polarity dataset is close to LSTM's performance on a shuffled training set, suggesting that word-order features are a key difference.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SWEM-HIER\"",
                "tgt_id": "\"SWEM\"",
                "weight": 9.0,
                "description": "\"SWEM-hier is a variant of the SWEM model, incorporating a hierarchical pooling operation to improve performance in sentiment analysis tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SWEM-HIER\"",
                "tgt_id": "\"SECTION 4.3\"",
                "weight": 8.0,
                "description": "\"SWEM-hier's performance is discussed in Section 4.3, where it is shown to outperform other SWEM variants in sentiment analysis tasks.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SECTION 4.2.1\"",
                "tgt_id": "\"SWEM\"",
                "weight": 7.0,
                "description": "\"Section 4.2.1 demonstrates the importance of word-order information for sentiment analysis tasks, relevant to the performance of the SWEM model.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SECTION 3.3\"",
                "tgt_id": "\"SWEM-HIER\"",
                "weight": 8.0,
                "description": "\"Section 3.3 details the hierarchical pooling operation used in SWEM-hier, which contributes to its improved performance in sentiment analysis.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"TABLE 6\"",
                "tgt_id": "\"LSTM\"",
                "weight": 7.0,
                "description": "\"Table 6 shows the test accuracy of LSTM model trained on original and shuffled training sets, indicating LSTM's reliance on word-order information.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"TABLE 7\"",
                "tgt_id": "\"LSTM\"",
                "weight": 8.0,
                "description": "\"Table 7 demonstrates the impact of word-order information on LSTM's performance in sentiment analysis by showing incorrect predictions with shuffled training data.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"TABLE 8\"",
                "tgt_id": "\"SWEM\"",
                "weight": 7.0,
                "description": "\"Table 8 compares the performance of SWEM with CNN/LSTM on short sentence classifications, showing SWEM's inferior accuracies on sentiment analysis datasets.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            },
            {
                "src_id": "\"SUBSPACE TRAINING\"",
                "tgt_id": "\"LI ET AL., 2018\"",
                "weight": 9.0,
                "description": "\"Subspace training, a method referenced from Li et al., 2018, is used to measure model complexity in text classification problems.\"",
                "source_id": "chunk-f513c5de611803566998ac79f309def6"
            }
        ]
    },
    "7": {
        "chunk_key": "chunk-a5877fac67633a73a05026418a3515b4",
        "entities": [
            {
                "entity_name": "\"LAVI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Lavi is an author of the paper 'Fine-grained analysis of sentence embeddings using auxiliary prediction tasks' presented at ICLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"YOAV GOLDBERG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yoav Goldberg is an author of the paper 'Fine-grained analysis of sentence embeddings using auxiliary prediction tasks' presented at ICLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"SANJEEV ARORA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Sanjeev Arora is an author of the paper 'A simple but tough-to-beat baseline for sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"YINGYU LIANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yingyu Liang is an author of the paper 'A simple but tough-to-beat baseline for sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"TENGYU MA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Tengyu Ma is an author of the paper 'A simple but tough-to-beat baseline for sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"YOSHUA BENGIO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yoshua Bengio is an author of the paper 'A neural probabilistic language model' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"RÉJEAN DUCHARME\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Réjean Ducharme is an author of the paper 'A neural probabilistic language model' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"PASCAL VINCENT\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Pascal Vincent is an author of the paper 'A neural probabilistic language model' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"CHRISTIAN JAUVIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Christian Jauvin is an author of the paper 'A neural probabilistic language model' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"RONAN COLLOBERT\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ronan Collobert is an author of the paper 'Natural language processing (almost) from scratch' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"JASON WESTON\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jason Weston is an author of the paper 'Natural language processing (almost) from scratch' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"LÉON BOTTOU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Léon Bottou is an author of the paper 'Natural language processing (almost) from scratch' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"MICHAEL KARLEN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Michael Karlen is an author of the paper 'Natural language processing (almost) from scratch' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"KORAY KAVUKCUOGLU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Koray Kavukcuoglu is an author of the paper 'Natural language processing (almost) from scratch' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"PAVEL KUKSA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Pavel Kuksa is an author of the paper 'Natural language processing (almost) from scratch' published in JMLR.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"ALEXIS CONNEAU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Alexis Conneau is an author of the paper 'Supervised learning of universal sentence representations from natural language inference data' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"DOUWE KIELA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Douwe Kiela is an author of the paper 'Supervised learning of universal sentence representations from natural language inference data' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"HOLGER SCHWENK\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Holger Schwenk is an author of the paper 'Supervised learning of universal sentence representations from natural language inference data' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"LOIC BARRAULT\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Loic Barrault is an author of the paper 'Supervised learning of universal sentence representations from natural language inference data' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"ANTOINE BORDES\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Antoine Bordes is an author of the paper 'Supervised learning of universal sentence representations from natural language inference data' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"YANN LECUN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yann LeCun is an author of the paper 'Very deep convolutional networks for natural language processing'.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"ZHE GAN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Zhe Gan is an author of the paper 'Learning generic sentence representations using convolutional neural networks' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"YUNCHEN PU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yunchen Pu is an author of the paper 'Learning generic sentence representations using convolutional neural networks' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"RICARDO HENAO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ricardo Henao is an author of the paper 'Learning generic sentence representations using convolutional neural networks' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"CHUNYUAN LI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Chunyuan Li is an author of the paper 'Learning generic sentence representations using convolutional neural networks' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"XIAODONG HE\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Xiaodong He is an author of the paper 'Learning generic sentence representations using convolutional neural networks' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"LAWRENCE CARIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Lawrence Carin is an author of the paper 'Learning generic sentence representations using convolutional neural networks' presented at EMNLP.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"ALEX GRAVES\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Alex Graves is an author of the paper 'Hybrid speech recognition with deep bidirectional lstm' presented at the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"NAVDEEP JAITLY\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Navdeep Jaitly is an author of the paper 'Hybrid speech recognition with deep bidirectional lstm' presented at the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"ABDEL-RAHMAN MOHAMED\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Abdel-rahman Mohamed is an author of the paper 'Hybrid speech recognition with deep bidirectional lstm' presented at the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"SEPP HOCHREITER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Sepp Hochreiter is an author of the paper 'Long short-term memory' published in Neural Computation.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"JÜRGEN SCHMIDHUBER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jürgen Schmidhuber is an author of the paper 'Long short-term memory' published in Neural Computation.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"BAOTIAN HU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Baotian Hu is an author of the paper 'Convolutional neural network architectures for matching natural language sentences' presented at NIPS.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"ZHENGDONG LU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Zhengdong Lu is an author of the paper 'Convolutional neural network architectures for matching natural language sentences' presented at NIPS.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"QINGCAI CHEN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Qingcai Chen is an author of the paper 'Convolutional neural network architectures for matching natural language sentences' presented at NIPS.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"MOHIT IYYER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Mohit Iyyer is an author of the paper 'Deep unordered composition rivals syntactic methods for text classification' presented at ACL.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            },
            {
                "entity_name": "\"VARUN MANJUNATHA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Varun Manjunatha is an author of the paper 'Deep unordered composition rivals syntactic methods for text classification' presented at ACL.\"",
                "source_id": "chunk-a5877fac67633a73a05026418a3515b4"
            }
        ],
        "relationships": []
    },
    "0": {
        "chunk_key": "chunk-c125139d59ca2aa28f4e2eb6fae21d67",
        "entities": [
            {
                "entity_name": "\"DUKE UNIVERSITY\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Duke University is an institution where some of the authors of the paper are affiliated.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"NEC LABORATORIES AMERICA\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"NEC Laboratories America is an organization affiliated with one of the authors of the paper.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"SUN YAT-SEN UNIVERSITY\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Sun Yat-sen University is an institution affiliated with one of the authors of the paper.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"MICROSOFT RESEARCH\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Microsoft Research is an organization affiliated with one of the authors of the paper.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"DINGHAN SHEN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Dinghan Shen is an author of the paper and is affiliated with Duke University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"GUOYIN WANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Guoyin Wang is an author of the paper and is affiliated with Duke University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"WENLIN WANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Wenlin Wang is an author of the paper and is affiliated with Duke University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"MARTIN RENQIANG MIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Martin Renqiang Min is an author of the paper and is affiliated with NEC Laboratories America.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"QINLI SU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Qinli Su is an author of the paper and is affiliated with Sun Yat-sen University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"YIZHE ZHANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yizhe Zhang is an author of the paper and is affiliated with Microsoft Research.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"CHUNYUAN LI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Chunyuan Li is an author of the paper and is affiliated with Duke University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"RICARDO HENAO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ricardo Henao is an author of the paper and is affiliated with Duke University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"LAWRENCE CARIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Lawrence Carin is an author of the paper and is affiliated with Duke University.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"SIMPLE WORD-EMBEDDING-BASED MODELS (SWEMS)\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEMs are models consisting of parameter-free pooling operations used for text sequence modeling in NLP.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"RECURRENT NEURAL NETWORKS (RNNS)\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"RNNs are a type of deep learning model used for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"CONVOLUTIONAL NEURAL NETWORKS (CNNS)\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"CNNs are a type of deep learning model used for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"RECURSIVE NEURAL NETWORKS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Recursive Neural Networks are a type of deep learning model used for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"WORD2VEC\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"word2vec is a method used to obtain word embeddings for text sequence modeling.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"GLOVE\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"GloVe is a method used to obtain word embeddings for text sequence modeling.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"DOCUMENT CLASSIFICATION\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Document classification is one of the NLP tasks considered in the paper, including datasets like Yahoo news and Yelp reviews.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"TEXT SEQUENCE MATCHING\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Text sequence matching is one of the NLP tasks considered in the paper, including datasets like SNLI and WikiQA.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"SHORT TEXT TASKS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Short text tasks are one of the NLP tasks considered in the paper, including classification and tagging.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"PENNINGTON ET AL., 2014\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Pennington et al., 2014 is a reference to a work on word embeddings, mentioned for implicitly encoding linguistic regularities and patterns.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"BENGIO ET AL., 2003\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Bengio et al., 2003 is a reference to a foundational work in the field of word embeddings.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"MITCHELL AND LAPATA, 2010\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Mitchell and Lapata, 2010 is a reference to a work that proposed simple operations like addition for text sequence modeling.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"IYYER ET AL., 2015\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Iyyer et al., 2015 is a reference to a work that contributed to simple operations for text sequence modeling.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"TAI ET AL., 2015\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Tai et al., 2015 is a reference to a work that used RNNs for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"SUTSKEVER ET AL., 2014\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Sutskever et al., 2014 is a reference to a work that used RNNs for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"KALCHBRENNER ET AL., 2014\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Kalchbrenner et al., 2014 is a reference to a work that used CNNs for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"KIM, 2014\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Kim, 2014 is a reference to a work that used CNNs for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"ZHANG ET AL., 2017A\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Zhang et al., 2017a is a reference to a work that used CNNs for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"SOCHER ET AL., 2011A\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Socher et al., 2011a is a reference to a work that used Recursive Neural Networks for compositionality in text sequences.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"PARIKH ET AL., 2016\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Parikh et al., 2016 is a reference to a work that discussed the computational expense of models with expressive compositional functions.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "entity_name": "\"LI ET AL.,\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Li et al., is a reference to a work cited in the context of subspace training for word-embedding-based text classification tasks.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            }
        ],
        "relationships": [
            {
                "src_id": "\"DINGHAN SHEN\"",
                "tgt_id": "\"DUKE UNIVERSITY\"",
                "weight": 1.0,
                "description": "\"Dinghan Shen is affiliated with Duke University(\"entity\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "src_id": "\"DINGHAN SHEN\"",
                "tgt_id": "\"SIMPLE WORD-EMBEDDING-BASED MODELS (SWEMS)\"",
                "weight": 7.0,
                "description": "\"Dinghan Shen is an author of the paper that conducts a comparative study on SWEMs.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "src_id": "\"GUOYIN WANG\"",
                "tgt_id": "\"SIMPLE WORD-EMBEDDING-BASED MODELS (SWEMS)\"",
                "weight": 7.0,
                "description": "\"Guoyin Wang is an author of the paper that conducts a comparative study on SWEMs.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "src_id": "\"WENLIN WANG\"",
                "tgt_id": "\"SIMPLE WORD-EMBEDDING-BASED MODELS (SWEMS)\"",
                "weight": 7.0,
                "description": "\"Wenlin Wang is an author of the paper that conducts a comparative study on SWEMs.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            },
            {
                "src_id": "\"MARTIN RENQIANG MIN\"",
                "tgt_id": "\"SIMPLE WORD-EMBEDDING-BASED MODELS (SWEMS)\"",
                "weight": 7.0,
                "description": "\"Martin Renqiang Min is an author of the paper that conducts a comparative study on SWEMs.\"",
                "source_id": "chunk-c125139d59ca2aa28f4e2eb6fae21d67"
            }
        ]
    },
    "8": {
        "chunk_key": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe",
        "entities": [
            {
                "entity_name": "\"ANDREW Y NG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Andrew Y Ng is an author of multiple research papers in the field of computational linguistics and neural networks.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"EMNLP\"",
                "entity_type": "\"EVENT\"",
                "description": "\"EMNLP is an event where research papers on computational linguistics are presented, with Andrew Y Ng presenting a paper on semantic compositionality.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ASSOCIATION FOR COMPUTATIONAL LINGUISTICS\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"The Association for Computational Linguistics is the organization that hosts the EMNLP event and publishes research papers in the field.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ICML\"",
                "entity_type": "\"EVENT\"",
                "description": "\"ICML is an event where research papers on machine learning are presented, including a paper co-authored by Andrew Y Ng.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"RICHARD SOCHER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Richard Socher is a co-author of multiple research papers in the field of natural language processing and computational linguistics.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"CLIFF C LIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Cliff C Lin is a co-author of a research paper on parsing natural scenes and natural language with recursive neural networks.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"CHRIS MANNING\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Chris Manning is a co-author of multiple research papers in the field of computational linguistics and natural language processing.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"JEFFREY PENNINGTON\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jeffrey Pennington is a co-author of a research paper on semi-supervised recursive autoencoders for predicting sentiment distributions.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ERIC H HUANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Eric H Huang is a co-author of a research paper on semi-supervised recursive autoencoders for predicting sentiment distributions.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"CHRISTOPHER D MANNING\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Christopher D Manning is a co-author of multiple research papers in the field of computational linguistics and natural language processing.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"NITISH SRIVASTAVA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Nitish Srivastava is a co-author of a research paper on Dropout, a method to prevent neural networks from overfitting.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"GEOFFREY E HINTON\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Geoffrey E Hinton is a co-author of a research paper on Dropout, a method to prevent neural networks from overfitting.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ALEX KRIZHEVSKY\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Alex Krizhevsky is a co-author of a research paper on Dropout, a method to prevent neural networks from overfitting.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ILYA SUTSKEVER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ilya Sutskever is a co-author of multiple research papers, including one on Dropout and another on sequence to sequence learning with neural networks.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"RUSLAN SALAKHUTDINOV\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ruslan Salakhutdinov is a co-author of a research paper on Dropout, a method to prevent neural networks from overfitting.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ORIOL VINYALS\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Oriol Vinyals is a co-author of a research paper on sequence to sequence learning with neural networks.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"QUOC V LE\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Quoc V Le is a co-author of a research paper on sequence to sequence learning with neural networks.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"KAI SHENG TAI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Kai Sheng Tai is a co-author of a research paper on improved semantic representations from tree-structured long short-term memory networks.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"JMLR\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"JMLR is the organization that published the research paper on Dropout by Nitish Srivastava et al.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"NIPS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"NIPS is an event where research papers on neural networks and machine learning are presented, including papers by Ilya Sutskever et al. and(\"entity\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"NOAM SHAZEER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Noam Shazeer is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"NIKI PARMAR\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Niki Parmar is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"JAKOB USZKOREIT\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Jakob Uszkoreit is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"LLION JONES\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Llion Jones is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"AIDAN N GOMEZ\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Aidan N Gomez is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"LUKASZ KAISER\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Lukasz Kaiser is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ILLIA POLOSUKHIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Illia Polosukhin is a co-author of the research paper 'Attention is all you need' presented at NIPS.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"JOHN WIETING\"",
                "entity_type": "\"PERSON\"",
                "description": "\"John Wieting is a co-author of the research paper 'Towards universal paraphrastic sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"MOHIT BANSAL\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Mohit Bansal is a co-author of the research paper 'Towards universal paraphrastic sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"KEVIN GIMPEL\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Kevin Gimpel is a co-author of the research paper 'Towards universal paraphrastic sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"KAREN LIVESCU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Karen Livescu is a co-author of the research paper 'Towards universal paraphrastic sentence embeddings' presented at ICLR.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"ADINA WILLIAMS\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Adina Williams is a co-author of the research paper 'A broad-coverage challenge corpus for sentence understanding through inference' presented as an arXiv preprint.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"NIKITA NANGIA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Nikita Nangia is a co-author of the research paper 'A broad-coverage challenge corpus for sentence understanding through inference' presented as an arXiv preprint.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"SAMUEL R BOWMAN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Samuel R Bowman is a co-author of the research paper 'A broad-coverage challenge corpus for sentence understanding through inference' presented as an arXiv preprint.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"SHILIANG ZHANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Shiliang Zhang is a co-author of the research paper 'The fixed-size ordinally-forgetting encoding method for neural network language models' presented at ACL-IJCNLP.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"HUI JIANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Hui Jiang is a co-author of the research paper 'The fixed-size ordinally-forgetting encoding method for neural network language models' presented at ACL-IJCNLP.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            },
            {
                "entity_name": "\"MINGBIN XU\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Mingbin Xu is a co-author of the research paper 'The fixed-size ordinally-forgetting encoding method for neural network language models' presented at ACL-IJCNLP.\"",
                "source_id": "chunk-b61d9c5f9ad7e3c4ba247256d0488bfe"
            }
        ],
        "relationships": []
    },
    "2": {
        "chunk_key": "chunk-b645d9c5da818cb28134c78c33a7c37d",
        "entities": [
            {
                "entity_name": "\"CONNEAU ET AL., 2016\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Conneau et al., 2016 refers to a group of researchers who have developed Deep CNN text models.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"WIETING ET AL., 2015\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Wieting et al., 2015 refers to a group of researchers who have contributed to the understanding of word embeddings through the SWEM model.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"ADI ET AL., 2016\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Adi et al., 2016 refers to a group of researchers who have worked on the SWEM model, specifically the average pooling operation.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"COLLOBERT ET AL., 2011\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Collobert et al., 2011 refers to a group of researchers who have contributed to the understanding of max-over-time pooling operation in convolutional neural networks.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"ZHANG ET AL., 2015B\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Zhang et al., 2015b refers to a group of researchers who have worked on the bag-of-$^{\\cdot n}$-grams method, which is related to the SWEM-hier model.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"VASWANI ET AL., 2017\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Vaswani et al., 2017 refers to a group of researchers who have considered the computational complexity of various models, including SWEM.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SWEMS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEMs stands for Simple Word-Embedding Models, a class of models that investigate the raw modeling capacity of word embeddings without additional compositional parameters.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SWEM-AVER\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEM-aver is a variant of the SWEM model that computes the element-wise average over word vectors for a given sequence.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SWEM-MAX\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEM-max is a variant of the SWEM model that extracts the most salient features from every word-embedding dimension by taking the maximum value along each dimension.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SWEM-CONCAT\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEM-concat is a variant of the SWEM model that concatenates the features from SWEM-aver and SWEM-max to form sentence embeddings.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SWEM-HIER\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"SWEM-hier is a variant of the SWEM model that uses a hierarchical pooling layer to preserve local spatial information of a text sequence.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"LSTM\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"LSTM stands for Long Short-Term Memory, a type of recurrent neural network (RNN) used in various sequence prediction problems.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"DEEP CNN TEXT MODELS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Deep CNN text models are advanced text models that utilize multiple layers of convolutional neural networks for text analysis.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SINGLELAYER CNN TEXT MODEL\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Singlelayer CNN text model refers to a simpler version of CNN text models that uses only one layer of convolution.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"MAX-POOLING\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Max-pooling is an aggregation operation used in neural networks to reduce the spatial size of the representation,提取 the most salient features.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"FEATURE MAPS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Feature maps are the output of a convolutional layer in a neural network, representing the features detected within the input data.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SEMANTIC FEATURES\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Semantic features refer to the meaningful aspects of data that are abstracted and used to represent the input in a higher-level form.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"WORD EMBEDDINGS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Word embeddings are the numerical representations of words in a vector space, which capture semantic meanings of the words.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"WORD VECTORS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Word vectors are the specific representations of words in the form of vectors within a high-dimensional space, used in the context of word embeddings.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"WORD-ORDER\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Word-order refers to the sequence or order of words in a sentence, which can be important for understanding the meaning of the sentence.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SPATIAL INFORMATION\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Spatial information in the context of text refers to the relative positions of words within a sentence or document, which can be important for certain NLP tasks.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"N-GRAMS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"n-grams are contiguous sequences of n items from a given sequence of text or speech, used in various natural language processing applications.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"BAG-OF-$^{\\CDOT N}$-GRAMS METHOD\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Bag-of-$^{\\cdot n}$-grams method is a technique that counts the frequency of n-grams in a text and is related to the SWEM-hier model in terms of capturing local spatial information.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"PARAMETERS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Parameters in the context of models refer to the weights and biases that are learned during the training process.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"COMPUTATIONAL COMPLEXITY\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Computational complexity refers to the amount of computational resources needed to perform a task, often analyzed in terms of time and space requirements.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "entity_name": "\"SEQUENTIAL OPERATIONS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Sequential operations are operations that need to be performed in a specific order, one after another, and are relevant when analyzing the efficiency of models like CNN, LSTM, and SWEM.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            }
        ],
        "relationships": [
            {
                "src_id": "\"CONNEAU ET AL., 2016\"",
                "tgt_id": "\"SWEMS\"",
                "weight": 6.0,
                "description": "\"Conneau et al., 2016 developed Deep CNN text models, which are related to the SWEMs in the context of text modeling.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "src_id": "\"WIETING ET AL., 2015\"",
                "tgt_id": "\"SWEM-AVER\"",
                "weight": 7.0,
                "description": "\"Wieting et al., 2015 contributed to the understanding of SWEM-aver, the average pooling operation in SWEM models.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "src_id": "\"ADI ET AL., 2016\"",
                "tgt_id": "\"SWEM-AVER\"",
                "weight": 7.0,
                "description": "\"Adi et al., 2016 worked on the SWEM-aver model, which is a simple strategy for computing the element-wise average over word vectors.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "src_id": "\"COLLOBERT ET AL., 2011\"",
                "tgt_id": "\"SWEM-MAX\"",
                "weight": 6.0,
                "description": "\"Collobert et al., 2011's work on max-over-time pooling operation is similar to the strategy used in SWEM-max for extracting salient features.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "src_id": "\"ZHANG ET AL., 2015B\"",
                "tgt_id": "\"SWEM-HIER\"",
                "weight": 6.0,
                "description": "\"Zhang et al., 2015b's work on the bag-of-$^{\\cdot n}$-grams method is related to the SWEM-hier model, which also considers word-order and spatial information.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            },
            {
                "src_id": "\"SWEM-HIER\"",
                "tgt_id": "\"N-GRAMS\"",
                "weight": 7.0,
                "description": "\"SWEM-hier learns fixed-length representations for n-grams that appear in the corpus, rather than just capturing their occurrences.\"",
                "source_id": "chunk-b645d9c5da818cb28134c78c33a7c37d"
            }
        ]
    },
    "6": {
        "chunk_key": "chunk-d762a89dc693820486c0957ced302e5e",
        "entities": [
            {
                "entity_name": "\"LI ET AL., 2018\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Li et al., 2018 are referenced for their work on subspace training in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"SWEM-MAX\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM-max is a variant of the SWEM model used in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"CNN\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"CNN refers to the Convolutional Neural Network model used in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"AG NEWS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"AG News is a dataset used for testing the performance of text classification models.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"YELP P.\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Yelp P. is a dataset used for testing the performance of text classification models.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"GLOVE\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"GloVe is a pre-trained word embedding model used in text classification.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"YAHOO! ANS.\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Yahoo! Ans. is a dataset used for testing the performance of text classification models.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"SOGOU NEWS CORPUS\"",
                "entity_type": "\"GEO\"",
                "description": "\"Sogou news corpus is a Chinese dataset used for text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"ZHANG ET AL., 2015B\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Zhang et al., 2015b are referenced for their work on text classification in Chinese.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"SWEM-CONCAT\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM-concat is a model variant used in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"SWEMHIER\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEMhier is a model variant used in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"LSTM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"LSTM refers to the Long Short-Term Memory model used in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"EINAT KERMANY\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Einat Kermany is an author referenced in the context of fine-grained analysis of sentence embeddings.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"YONATAN BELINKOV\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yonatan Belinkov is an author referenced in the context of fine-grained analysis of sentence embeddings.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"OFER LAVI\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Ofer Lavi is an author referenced in the context of fine-grained analysis of sentence embeddings.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"YOAV GOLDBERG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yoav Goldberg is an author referenced in the context of fine-grained analysis of sentence embeddings.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"SANJEEV ARORA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Sanjeev Arora is an author referenced for a baseline sentence embedding work.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"YINGYU LIANG\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yingyu Liang is an author referenced for a baseline sentence embedding work.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"TENGYU MA\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Tengyu Ma is an author referenced for a baseline sentence embedding work.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"YOSHUA BENGIO\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Yoshua Bengio is an author referenced for work on a neural probabilistic language model.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"RÉJEAN DUCHARME\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Réjean Ducharme is an author referenced for work on a neural probabilistic language model.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"PASCAL VINCENT\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Pascal Vincent is an author referenced for work on a neural probabilistic language model.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"CHRISTIAN JAUVIN\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Christian Jauvin is an author referenced for work on a neural probabilistic language model.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"ICLR\"",
                "entity_type": "\"EVENT\"",
                "description": "\"ICLR is a conference where several referenced papers were presented.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"J\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"J refers to the journal where the neural probabilistic language model paper was published.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"OCCAM’S RAZOR\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Occam’s razor is a principle that suggests simple models are preferred if all else is equal.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"FINE-GRAINED ANALYSIS OF SENTENCE EMBEDDINGS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Fine-grained analysis of sentence embeddings is a topic discussed in a referenced paper.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS\"",
                "entity_type": "\"EVENT\"",
                "description": "\"A simple but tough-to-beat baseline for sentence embeddings is a topic discussed in a referenced paper.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "entity_name": "\"A NEURAL PROBABILISTIC LANGUAGE MODEL\"",
                "entity_type": "\"EVENT\"",
                "description": "\"A neural probabilistic language model is a topic discussed in a referenced paper.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            }
        ],
        "relationships": [
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"CNN\"",
                "weight": 6.0,
                "description": "\"SWEM-max and CNN are both models studied for their performance in text classification problems.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"AG NEWS\"",
                "weight": 7.0,
                "description": "\"SWEM-max is evaluated on the AG News dataset for text classification performance.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"CNN\"",
                "tgt_id": "\"AG NEWS\"",
                "weight": 7.0,
                "description": "\"CNN is evaluated on the AG News dataset for text classification performance.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"SWEM-MAX\"",
                "tgt_id": "\"YELP P.\"",
                "weight": 7.0,
                "description": "\"SWEM-max is evaluated on the Yelp P. dataset for text classification performance.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"CNN\"",
                "tgt_id": "\"YELP P.\"",
                "weight": 7.0,
                "description": "\"CNN is evaluated on the Yelp P. dataset for text classification performance.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"GLOVE\"",
                "tgt_id": "\"SWEM-MAX\"",
                "weight": 7.0,
                "description": "\"GloVe word embeddings are used in conjunction with SWEM-max for text classification.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"GLOVE\"",
                "tgt_id": "\"CNN\"",
                "weight": 7.0,
                "description": "\"GloVe word embeddings are used in conjunction with CNN for text classification.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"SWEM-CONCAT\"",
                "tgt_id": "\"SOGOU NEWS CORPUS\"",
                "weight": 7.0,
                "description": "\"SWEM-concat is evaluated on the Sogou news corpus for text classification performance.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"SWEMHIER\"",
                "tgt_id": "\"SOGOU NEWS CORPUS\"",
                "weight": 7.0,
                "description": "\"SWEMhier is evaluated on the Sogou news corpus for text classification performance.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"SWEMHIER\"",
                "tgt_id": "\"CNN\"",
                "weight": 7.0,
                "description": "\"SWEMhier is compared to CNN for text classification performance on the Sogou news corpus.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"SWEMHIER\"",
                "tgt_id": "\"LSTM\"",
                "weight": 1.0,
                "description": "\"SWEMhier is compared to LSTM for text classification performance on the Sog(\"entity\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"EINAT KERMANY\"",
                "tgt_id": "\"FINE-GRAINED ANALYSIS OF SENTENCE EMBEDDINGS\"",
                "weight": 8.0,
                "description": "\"Einat Kermany is an author of a paper on fine-grained analysis of sentence embeddings.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            },
            {
                "src_id": "\"YONATAN BELINKOV\"",
                "tgt_id": "\"FINE-GRAINED ANALYSIS OF SENTENCE EMBEDDINGS\"",
                "weight": 8.0,
                "description": "\"Yonatan Belinkov is an author of a paper on fine-grained analysis of sentence embeddings.\"",
                "source_id": "chunk-d762a89dc693820486c0957ced302e5e"
            }
        ]
    },
    "3": {
        "chunk_key": "chunk-17ab6c2bb0d2cdb200916102c59e9778",
        "entities": [
            {
                "entity_name": "\"SWEM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"SWEM is a model that is compared to CNN and LSTM in terms of computational efficiency and performance on various tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"CNN\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"CNN is a model used for comparison with SWEM in terms of computational efficiency and performance on various tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"LSTM\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"LSTM is a model used for comparison with SWEM in terms of computational efficiency and performance on various tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"VASWANI ET AL. (2017)\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Vaswani et al. (2017) is referenced for comparison of computational complexity between models.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"PENNINGTON ET AL., 2014\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Pennington et al., 2014 is referenced for the use of GloVe word embeddings in initializing models.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"KINGMA AND BA, 2014\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Kingma and Ba, 2014 is referenced for the use of Adam optimization in model training.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"SRIVASTAVA ET AL., 2014\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Srivastava et al., 2014 is referenced for the use of Dropout regularization in model training.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"ZHANG ET AL. (2015B)\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Zhang et al. (2015b) is referenced for the data split used in document categorization tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"CONNEAU ET AL. (2016)\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Conneau et al. (2016) is referenced for comparison of results with SWEM on the DBpedia dataset.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"JOULIN ET AL. (2016)\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Joulin et al. (2016) is referenced for comparison of results with SWEM on document classification tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"PANG ET AL. (2002)\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Pang et al. (2002) is referenced for the hypothesis that positional information of words in text sequences is beneficial for sentiment analysis.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"AG NEWS\"",
                "entity_type": "\"GEO\"",
                "description": "\"AG news is a dataset used for topic categorization in document categorization tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"YELP POLARITY\"",
                "entity_type": "\"GEO\"",
                "description": "\"Yelp Polarity is a dataset used for sentiment analysis in document categorization tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"YELP FULL\"",
                "entity_type": "\"GEO\"",
                "description": "\"Yelp Full is a dataset used for sentiment analysis in document categorization tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"DBPEDIA\"",
                "entity_type": "\"GEO\"",
                "description": "\"DBpedia is a dataset used for ontology classification in document categorization tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"GLOVE WORD EMBEDDINGS\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"GloVe word embeddings are used for initializing all models in the experiments.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"OUT-OF-VOCABULARY (OOV) WORDS\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Out-Of-Vocabulary (OOV) words are words not found in the GloVe word embeddings and are initialized from a uniform distribution.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"MULTILAYER PERCEPTRON (MLP)\"",
                "entity_type": "\"ORGANIZATION\"",
                "description": "\"Multilayer Perceptron (MLP) is used as a layer in the models for learning refined word embeddings.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"RELU ACTIVATION\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"ReLU activation is used in the 300-dimensional MLP layer for learning refined word embeddings.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"SUPPLEMENTARY MATERIAL\"",
                "entity_type": "\"DOCUMENT\"",
                "description": "\"Supplementary Material contains summarized data statistics for the 17 datasets used in the experiments.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"ADAM\"",
                "entity_type": "\"PERSON\"",
                "description": "\"Adam is the optimization algorithm used to optimize all models, with learning rate selected from a given set.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"DROPOUT REGULARIZATION\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Dropout regularization is employed on the word embedding layer and final MLP layer with a selected dropout rate.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"TABLE 2\"",
                "entity_type": "\"DOCUMENT\"",
                "description": "\"Table 2 contains test accuracy on (long) document classification tasks in percentage.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"TABLE 3\"",
                "entity_type": "\"DOCUMENT\"",
                "description": "\"Table 3 shows the top five words with the largest values in a given word-embedding dimension.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"TABLE 4\"",
                "entity_type": "\"DOCUMENT\"",
                "description": "\"Table 4 contains information on speed and parameters on the Yahoo! Answer dataset.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"DOCUMENT CATEGORIZATION\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Document categorization is a task that involves categorizing documents based on their content.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"TEXT SEQUENCE MATCHING\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Text sequence matching is a task that involves predicting the relationship between a pair of sentences.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"SENTENCE CLASSIFICATION\"",
                "entity_type": "\"EVENT\"",
                "description": "\"Sentence classification is a task that involves classifying short sentences into categories.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "entity_name": "\"NATURAL LANGUAGE UNDERSTANDING\"",
                "entity_type": "\"CONCEPT\"",
                "description": "\"Natural language understanding is the overarching field concerning the 17 datasets used in the experiments.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            }
        ],
        "relationships": [
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"CNN\"",
                "weight": 7.0,
                "description": "\"SWEM is compared to CNN in terms of efficiency and performance on various tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"LSTM\"",
                "weight": 7.0,
                "description": "\"SWEM is compared to LSTM in terms of efficiency and performance on various tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"VASWANI ET AL. (2017)\"",
                "weight": 5.0,
                "description": "\"SWEM's efficiency is compared to models discussed by Vaswani et al. (2017).\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"PENNINGTON ET AL., 2014\"",
                "weight": 6.0,
                "description": "\"SWEM uses GloVe word embeddings from Pennington et al., 2014 for initialization.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"KINGMA AND BA, 2014\"",
                "weight": 6.0,
                "description": "\"SWEM uses Adam optimization from Kingma and Ba, 2014 for model training.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"SRIVASTAVA ET AL., 2014\"",
                "weight": 6.0,
                "description": "\"SWEM uses Dropout regularization from Srivastava et al., 2014 in model training.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"ZHANG ET AL. (2015B)\"",
                "weight": 5.0,
                "description": "\"SWEM follows the data split from Zhang et al. (2015b) for document categorization tasks.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"CONNEAU ET AL. (2016)\"",
                "weight": 6.0,
                "description": "\"SWEM's performance is compared to results from Conneau et al. (2016) on the DBpedia dataset.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"AG NEWS\"",
                "weight": 6.0,
                "description": "\"SWEM model is evaluated on the AG news dataset for document categorization.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"YELP POLARITY\"",
                "weight": 6.0,
                "description": "\"SWEM model is evaluated on the Yelp Polarity dataset for sentiment analysis.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            },
            {
                "src_id": "\"SWEM\"",
                "tgt_id": "\"YELP FULL\"",
                "weight": 6.0,
                "description": "\"SWEM model is evaluated on the Yelp Full dataset for sentiment analysis.\"",
                "source_id": "chunk-17ab6c2bb0d2cdb200916102c59e9778"
            }
        ]
    }
}