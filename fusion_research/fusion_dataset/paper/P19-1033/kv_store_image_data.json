{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_1.jpg",
        "caption": [
            "Figure 1: An illustrative example of long-term and short-term interests in news reading. "
        ],
        "footnote": [],
        "context": "Learning accurate user representations is critical for news recommendation (Okura et al., 2017). Existing news recommendation methods usually learn a single representation for each user (Okura et al., 2017; Lian et al., 2018; Wu et al., 2019). For example, Okura et al. (2017) proposed to learn representations of news using denoising autoencoder and learn representations of users from their browsed news using GRU network (Cho et al., 2014). However, it is very difficult for RNN networks such as GRU to capture the entire information of very long news browsing history. Wang et al. (2018) proposed to learn the representations of  effectively improve the performance of neural news recommendation. 1 Introduction Online news platforms such as MSN News1 and Google News2 which aggregate news from various sources and distribute them to users have gained huge popularity and attracted hundreds of millions of users (Das et al., 2007; Wang et al., 2018). However, massive news are generated everyday, making it impossible for users to read through all news (Lian et al., 2018). Thus, personalized news recommendation is very important for online news platforms to help users find their interested contents and alleviate information overload (Lavie et al., 2010; Zheng et al., 2018). ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-cfc4856ec35cb9a74b12e7ab5c8a14fc",
        "description": "The image is a timeline illustrating the long-term and short-term interests in news reading. The timeline is marked with specific events and their corresponding timestamps. Starting from the left, the first event is labeled '2017 NBA Championship Celebration From Warriors' at timestamp t1. Moving right, the next event is 'Bohemian Rhapsody Is Highest-Grossing Musician Biopic Ever' at timestamp ti. Following this, the event 'Rami Malek Wins the 2019 Oscar' is marked at timestamp ti+1. Finally, the last event on the timeline is 'Oklahoma City Thunder vs. Golden State Warriors' at timestamp tj. The timeline is depicted as a horizontal arrow with these events marked at different points along its length. Each event is enclosed in a box with a distinct color: blue for the NBA Championship, green for Bohemian Rhapsody, yellow for Rami Malek's Oscar win, and blue again for the basketball game. The background of the image is white, and the text is clearly legible.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_2.jpg",
        "caption": [
            "Figure 2: The framework of the news encoder. "
        ],
        "footnote": [],
        "context": "$$ \\begin{array}{l}{a_{i}=\\operatorname{tanh}(\\boldsymbol{v}\\times\\boldsymbol{c}_{i}+\\boldsymbol{v}_{b}),}\\\\ {\\alpha_{i}=\\frac{\\exp(a_{i})}{\\sum_{j=1}^{N}\\exp(a_{j})},}\\end{array} The third layer is an attention network (Bahdanau et al., 2015). Different words in the same news title may have different informativeness for representing news. For instance, in the news title “The best NBA moments in $2018^{\\circ}$ , the word “NBA” is very informative for representing this news since it is an important indication of sports news, while the word $^{\\bullet\\bullet}2018^{\\bullet}$ is less informative. Thus, we employ a word-level attention network to select important words in news titles to learn more informative news representations. The attention weight $\\alpha_{i}$ of the $i$ -th word is formulated as follows:  news title “Next season of super bowl games”, the local contexts of “bowl” such as “super” and “games” are very important for inferring that it belongs to a sports event name. Thus, we apply a CNN network to learn contextual word representations by capturing the local context information. Denote the contextual representation of $w_{i}$ as $c_{i}$ , which is computed as follows: $$ \\begin{array}{r}{\\pmb{c}_{i}=\\mathrm{ReLU}(\\pmb{C}\\times\\pmb{w}_{[i-M:i+M]}+\\pmb{b}),}\\end{array} $$ where ${\\pmb w}_{[i-M:i+M]}$ is the concatenation of the embeddings of words between position $i\\,-\\,M$ and $i+M$ .${\\cal C}$ and $^{b}$ are the parameters of the convolutional filters in CNN, and $M$ is the window size. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-40a50e43f2e71428b8dbff5c388e34ae",
        "description": "The image is a detailed diagram illustrating the framework of a news encoder. The diagram is structured into several components, each represented by boxes and arrows indicating the flow of information. At the bottom, there is a 'News Title' box connected to a 'Word Embedding' layer, which processes individual words (w1, w2, ..., wN) from the title. Each word embedding (w1, w2, ..., wN) is then passed through a CNN network to generate contextual representations (c1, c2, ..., cN). These contextual representations are further processed by an attention network, where each representation (ci) is weighted by an attention score (αi). The attention scores are calculated using a softmax function based on the dot product between the context vector (v) and the contextual representation (ci). The final output (e) is a weighted sum of the contextual representations, with weights determined by the attention scores. Additionally, there are two other inputs: 'Subtopic Embedding' and 'Topic Embedding', which are combined with the final output to form the overall news representation. The subtopic and topic embeddings are derived from the 'News Subtopic' and 'News Topic' respectively. The diagram uses various colors and shapes to distinguish different components and their relationships, with rectangles representing layers and arrows showing the flow of data.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_3.jpg",
        "caption": [
            "Figure 3: The two frameworks of our LSTUR approach. "
        ],
        "footnote": [],
        "context": "Online users may have dynamic short-term interests in reading news articles, which may be influenced by specific contexts or temporal information demands. For example, if a user just reads a news article about “Mission: Impossible 6 – Fallout”, and she may want 3.2.1 Short-Term User Representation The user encoder is used to learn representations of users from the history of their browsed news. It contains two modules, i.e., a short-term user representation model (STUR) to capture user’s temporal interests, and a long-term user representation model (LTUR) to capture user’s consistent preferences. Next, we introduce them in detail. 3.2 User Encoder  articles with the “Sports” topic category, then we can infer this user is probably interested in sports, and it may be effective to recommend candidate news in the “Sports” topic category to this user. To incorporate the topic and subtopic information into news representation, we propose to learn the representations of topics and subtopics from the embeddings of their IDs, as shown in Fig. 2. Denote $e_{v}$ and $e_{s v}$ as the representations of topic and subtopic. The final representation of a news article is the concatenation of the representations of its title, topic and subtopic, i.e., $\\boldsymbol{e}=[e_{t},e_{v},e_{s v}]$ . ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-40a50e43f2e71428b8dbff5c388e34ae",
        "description": "The image is a diagram illustrating the two frameworks of the LSTUR (Long-Short Term User Representation) approach for news recommendation systems. The diagram is divided into two parts: (a) LSTUR-ini and (b) LSTUR-con. Each part shows a flowchart of the user encoder process, which involves encoding user click history and candidate news to generate a score for news recommendation.\\n\\nIn both parts, the process starts with the user click history, represented by a series of news articles (c1, c2, ..., ck). Each news article is encoded using a News Encoder, which generates an embedding (e1, e2, ..., ek) for each article. These embeddings are then fed into a GRU (Gated Recurrent Unit) network, which processes the sequence of embeddings to generate a user representation (u).\\n\\nIn part (a) LSTUR-ini, the user representation (u) is directly used to compute the score for a candidate news article (cx) through a dot product operation. The score indicates the relevance of the candidate news to the user's interests.\\n\\nIn part (b) LSTUR-con, the user representation is further refined by concatenating short-term and long-term user representations (us and ul). This concatenated representation is then used in the dot product operation to compute the score for the candidate news article (cx).\\n\\nThe diagrams use color coding to distinguish different components: blue for user-related elements, green for news-related elements, and orange for the final score computation. The arrows indicate the flow of information through the system, from the user click history to the final score.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_4.jpg",
        "caption": [
            "Table 1: Statistics of the dataset in our experiments. "
        ],
        "footnote": [],
        "context": "In our experiments, we used the pretrained GloVe embedding (Pennington et al., 2014) as the initialization of word embeddings. The word embedding dimension is 200. The number of filters in CNN network is 300, and the window size of the filters in CNN network is set to 3. We applied dropout (Srivastava et al., 2014) to each layer in our approach to mitigate overfitting. The dropout rate is 0.2. The default value of long-term user representation masking probability $p$ for model training is 0.5. We used Adam (Kingma and Ba, 2014) to optimize the model, and the learning rate was approach. 4 Experiments 4.1 Dataset and Experimental Settings Since there is no off-the-shelf dataset for news recommendation, we built one by ourselves through collecting logs from MSN News in four weeks from December 23rd, 2018 to January 19th, 2019. We used the logs in the first three weeks for model training, and those in the last week for test. We also randomly sampled $10\\%$ of logs from the training set as the validation data. For each sample, we collected the browsing history in last 7 days to learn short-term user representations. The detailed dataset statistics are summarized in Table 1. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-72f78c27dad5114374bbbb67fbfc4777",
        "description": "The image is a table labeled 'Table 1: Statistics of the dataset in our experiments.' The table provides detailed statistics about a dataset used for news recommendation. It contains the following rows and their respective values: 'Number of users' is 25,000, with 22,938 users in the training set. 'Number of news' is 38,501, and the average number of words per title is 9.98. 'Number of impressions' is 393,191. The number of positive samples is 492,185, and the number of negative samples is 9,224,537. The NP ratio (negative to positive ratio) is 18.74. The table highlights the significant size of the dataset and the imbalance between positive and negative samples.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "From the results we find both LTUR and STUR are useful for news recommendation, and the STUR model can outperform the LTUR model. According to the statistics in Table 1, the longterm representations of many users in test data are unavailable, which leads to relative weak performance of LTUR on these users. In addition, combining STUR and LTUR using our two longand short-term user representation methods, i.e., LSTUR-ini and LSTUR-con, can effectively improve the performance. This result validates that incorporating both long-term and short-term user representations is useful to capture the diverse user interests more accurately and is beneficial for   also conducted experiments to explore the performance of combining both LSTUR-con and LSTUR-ini in the same model, but the performance improvement is very limited, implying that each of them can fully capture the long- and short-term user interests for news recommendation. 4.3 Effectiveness of Long- and Short-Term User Representation In this section, we conducted several experiments to explore the effectiveness of our approach in learning both long-term and short-term user representations. We compare the performance of our LSTUR methods with the long-term user representation model LTUR and the short-term user representation model STUR. The results are summarized in Fig. 4. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-6ed1a98565ec22824cdc72d53742fe1a",
        "description": "The image is a table that compares the performance of various methods for news recommendation using different evaluation metrics. The table has four columns: Methods, AUC, MRR, nDCG@5, and nDCG@10. Each row represents a different method. The methods listed are LibFM, DeepFM, Wide & Deep, DSSM, CNN, DKN, GRU, LSTUR-con, and LSTUR-ini. The values in each column represent the performance metrics with their respective standard deviations. For example, LibFM has an AUC of 56.52 ± 1.31, MRR of 25.53 ± 0.81, nDCG@5 of 26.66 ± 1.04, and nDCG@10 of 34.72 ± 0.95. The highest AUC value is achieved by LSTUR-ini at 63.56 ± 0.42, followed closely by LSTUR-con at 63.47 ± 0.10. Similarly, the highest MRR value is also achieved by LSTUR-ini at 30.98 ± 0.32, and the highest nDCG@5 and nDCG@10 values are both achieved by LSTUR-ini at 33.45 ± 0.39 and 41.37 ± 0.36 respectively. The table highlights the superior performance of the LSTUR methods, especially LSTUR-ini, over other methods in all evaluated metrics.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_6.jpg",
        "caption": [
            "Table 2: The performance of different methods on news recommendation. ",
            "Figure 4: The effectiveness of incorporating long-tern Figure 5: The comparisons of different methods in user representations (LTUR) and short-term user rep- learning short-term user representations from recently resentations (STUR). browsed news articles. "
        ],
        "footnote": [],
        "context": "From the results we find both LTUR and STUR are useful for news recommendation, and the STUR model can outperform the LTUR model. According to the statistics in Table 1, the longterm representations of many users in test data are unavailable, which leads to relative weak performance of LTUR on these users. In addition, combining STUR and LTUR using our two longand short-term user representation methods, i.e., LSTUR-ini and LSTUR-con, can effectively improve the performance. This result validates that incorporating both long-term and short-term user representations is useful to capture the diverse user interests more accurately and is beneficial for  also conducted experiments to explore the performance of combining both LSTUR-con and LSTUR-ini in the same model, but the performance improvement is very limited, implying that each of them can fully capture the long- and short-term user interests for news recommendation. 4.3 Effectiveness of Long- and Short-Term User Representation In this section, we conducted several experiments to explore the effectiveness of our approach in learning both long-term and short-term user representations. We compare the performance of our LSTUR methods with the long-term user representation model LTUR and the short-term user representation model STUR. The results are summarized in Fig. 4.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-6ed1a98565ec22824cdc72d53742fe1a",
        "description": "The image consists of two bar charts side by side, each with a y-axis and an x-axis. The left chart has a y-axis ranging from 0.600 to 0.640 in increments of 0.005, and the x-axis has two categories: AUC and nDCG@10. The bars represent different methods: LTUR (yellow), STUR (green), LSTUR-con (light green), and LSTUR-ini (dark green). For AUC, the values are approximately 0.610 for LTUR, 0.625 for STUR, 0.635 for LSTUR-con, and 0.635 for LSTUR-ini. For nDCG@10, the values are approximately 0.395 for LTUR, 0.405 for STUR, 0.415 for LSTUR-con, and 0.415 for LSTUR-ini. The right chart has a y-axis ranging from 0.626 to 0.638 in increments of 0.002, and the x-axis has the same categories: AUC and nDCG@10. The bars represent different methods: Average (yellow), Attention (green), LSTM (light green), and GRU (dark green). For AUC, the values are approximately 0.630 for Average, 0.632 for Attention, 0.634 for LSTM, and 0.636 for GRU. For nDCG@10, the values are approximately 0.410 for Average, 0.412 for Attention, 0.414 for LSTM, and 0.416 for GRU. Both charts show that combining long-term and short-term user representations generally improves performance.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_7.jpg",
        "caption": [
            "Figure 6: The comparisons of different methods in learning news title representations and the effectiveness of attention machenism in selecting important words. "
        ],
        "footnote": [],
        "context": "In this section, we conduct experiments to validate the effectiveness of incorporating topic and subtopic of news in the news encoder. We compare the performance of our approach with its variants without topic 4.5.1 Effectiveness of News Topic According to Fig. 6, using attention mechanism in both encoders based on CNN and LSTM can achieve better performance. This is probably because the attention network can select important words, which can learn more informative news representations. In addition, encoders using CNN outperform those using LSTM. This may be because local contexts in news titles are more important for learning news representations.   In addition, GRU achieves better performance than LSTM. This may be because GRU contains fewer parameters and has lower risk of overfitting . Thus, we select GRU as the news encoder in STUR. 4.5 Effectiveness of News Title Encoders In this section, we conduct experiments to compare different news title encoders. In our approach, the news encoder is a combination of CNN network and an attention network (denoted as $\\mathrm{CNN+Att})$ . We compare it with several variants, i.e., CNN, LSTM, and LSTM with attention $(\\mathrm{LSTM}{+}\\mathrm{Att})$ , to validate the effectiveness of our approach. The results are summarized in Fig. 6. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-6ed1a98565ec22824cdc72d53742fe1a",
        "description": "The image consists of two bar charts labeled (a) AUC and (b) nDCG@10. Each chart compares the performance of different methods in learning news title representations. The x-axis of both charts is labeled with two categories: LSTUR-ini and LSTUR-con. The y-axis of chart (a) ranges from 0.615 to 0.640, while the y-axis of chart (b) ranges from 0.395 to 0.420. Both charts contain four bars for each category, representing four different methods: LSTM (yellow), LSTM+Att (light green), CNN (green), and CNN+Att (dark green). In chart (a), the highest value is achieved by CNN+Att for LSTUR-con at approximately 0.635. In chart (b), the highest value is also achieved by CNN+Att for LSTUR-con at approximately 0.415. The trends show that incorporating attention mechanisms generally improves performance, with CNN+Att outperforming other methods in both metrics.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_8.jpg",
        "caption": [
            "Figure 7: The effectiveness of incorporating news topic and subtopic information for news recommendation. "
        ],
        "footnote": [],
        "context": "In this section, we conduct experiments to validate the effectiveness of incorporating topic and subtopic of news in the news encoder. We compare the performance of our approach with its variants without topic 4.5.1 Effectiveness of News Topic According to Fig. 6, using attention mechanism in both encoders based on CNN and LSTM can achieve better performance. This is probably because the attention network can select important words, which can learn more informative news representations. In addition, encoders using CNN outperform those using LSTM. This may be because local contexts in news titles are more important for learning news representations.  In addition, GRU achieves better performance than LSTM. This may be because GRU contains fewer parameters and has lower risk of overfitting . Thus, we select GRU as the news encoder in STUR. 4.5 Effectiveness of News Title Encoders In this section, we conduct experiments to compare different news title encoders. In our approach, the news encoder is a combination of CNN network and an attention network (denoted as $\\mathrm{CNN+Att})$ . We compare it with several variants, i.e., CNN, LSTM, and LSTM with attention $(\\mathrm{LSTM}{+}\\mathrm{Att})$ , to validate the effectiveness of our approach. The results are summarized in Fig. 6.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-6ed1a98565ec22824cdc72d53742fe1a",
        "description": "The image consists of two bar charts side by side, labeled (a) AUC and (b) nDCG@10. Each chart compares the performance of different variants of a news recommendation system, specifically LSTUR-ini and LSTUR-con, with and without incorporating topic and subtopic information. The x-axis of both charts lists the variants: LSTUR-ini and LSTUR-con. The y-axis represents the performance metric values, with the left chart showing AUC values ranging from approximately 0.626 to 0.638, and the right chart showing nDCG@10 values ranging from approximately 0.406 to 0.420. Each variant is represented by a set of four bars, colored yellow, green, dark green, and teal, corresponding to 'None', '+Topic', '+Subtopic', and '+Both' respectively. For example, in the AUC chart, LSTUR-ini with '+Both' achieves the highest value of around 0.638, while LSTUR-con with '+Both' achieves a value of around 0.636. Similarly, in the nDCG@10 chart, LSTUR-ini with '+Both' achieves the highest value of around 0.420, while LSTUR-con with '+Both' achieves a value of around 0.418. The charts clearly show that incorporating both topic and subtopic information generally leads to better performance for both metrics.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1033/images/image_9.jpg",
        "caption": [
            "Figure 8: The influence of mask probability $p$ on the performance of our approach. "
        ],
        "footnote": [],
        "context": "cause the useful information in LTUR cannot be effectively incorporated. Thus, the performance is also not optimal. A moderate choice on $p$ (e.g., 0.5) is most appropriate for both LSTUR-ini and LSTUR-con methods, which can properly balance the learning of LTUR and Figure 9: Visualization of the word-level attentions. 2019 CES Highlights $:$ Innovations in Enviro-Sensing for Robocars California dries off after storm batter state for days 15 Recipes Inspired By Vintage Movies Texas State Rep . Dennis Bonnen Elected As House Speaker Should You Buy American Express Stock After Earnings ? How Meghan Markle Has Changed Prince Harry Considerably long-term user representation in model training. We vary the value of $p$ from 0.0 to 0.9 with a step of 0.1 for both LSTUR-ini and LSTUR-con. The results are summarized in Fig. 8. According to Fig. 8, the results of LSTUR-ini and LSTUR-con have similar patterns. The performance of both methods improves when $p$ increases from 0. When $p$ is too small, the model will tend to overfit on the LTUR, since LTUR has many parameters. Thus, the performance is not optimal. However, when $p$ is too large, the performance of both methods starts to decline. This may be be ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-278340890fd0f192f781f9e8a3aa3bbf",
        "description": "The image consists of two line graphs, labeled (a) LSTUR-ini and (b) LSTUR-con, which illustrate the influence of mask probability \\( p \\) on the performance of two approaches. The x-axis represents the mask probability \\( p \\) ranging from 0.0 to 0.9 with a step of 0.1. The y-axis shows the performance metrics in terms of AUC, MRR, nDCG@5, and nDCG@10. Each graph contains four lines representing different performance metrics: AUC (green), MRR (orange), nDCG@5 (blue), and nDCG@10 (red). The lines are plotted with error bars indicating the variability or confidence intervals. For both graphs, the performance metrics generally increase as the mask probability \\( p \\) increases from 0.0 to around 0.5, after which they start to decline. The optimal performance is observed at a moderate value of \\( p \\) around 0.5 for both LSTUR-ini and LSTUR-con methods.",
        "segmentation": false
    }
}