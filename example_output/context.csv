
    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"TABLE 1",	"GEO",	"Table 1 in the paper provides the number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.",	1
1,	"NEGATIVE EXAMPLES",	"CONCEPT",	"Negative examples are discussed in the paper as significantly outnumbering positive ones, causing a severe data imbalance issue.",	0
2,	"POSITIVE EXAMPLES",	"CONCEPT",	"Positive examples are discussed in the context of data imbalance, where they are significantly outnumbered by negative examples.",	0
3,	"IMAGE_1",	"ORI_IMG",	"The image is a table labeled 'Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.' The table is structured with four main columns: Task, # neg, # pos, and ratio. Each row represents a different NLP task and contains the following values: \n- For CoNLL03 NER, the number of negative examples (# neg) is 170K, the number of positive examples (# pos) is 34K, and the ratio is 4.98. \n- For OntoNotes5.0 NER, the number of negative examples (# neg) is 1.96M, the number of positive examples (# pos) is 239K, and the ratio is 8.18. \n- For SQuAD 1.1 (Rajpurkar et al., 2016), the number of negative examples (# neg) is 10.3M, the number of positive examples (# pos) is 175K, and the ratio is 55.9. \n- For SQuAD 2.0 (Rajpurkar et al., 2018), the number of negative examples (# neg) is 15.4M, the number of positive examples (# pos) is 188K, and the ratio is 82.0. \n- For QUOREF (Dasigi et al., 2019), the number of negative examples (# neg) is 6.52M, the number of positive examples (# pos) is 38.6K, and the ratio is 169. \nThe table highlights the significant data imbalance in these NLP tasks, with the number of negative examples far exceeding the number of positive examples.",	6
4,	"DATA IMBALANCE",	"CONCEPT",	"Data imbalance is a central issue in the paper, referring to the problem of negative examples significantly outnumbering positive ones in NLP tasks.",	0
5,	"QUOREF",	"EVENT",	"Quoref is a QA dataset that tests the coreferential reasoning capability of reading comprehension systems."<SEP>"Quoref is a dataset used for machine reading comprehension tasks."<SEP>"Quoref is a dataset used in the task of question answering, mentioned in the context of evaluating model performance.",	4
6,	"+POSITIVE & NEGATIVE",	"EVENT",	"The performance metric of the BERT model after adding both positive and negative examples to the training dataset. Positive and negative augmentation refers to the process of augmenting the original training data with additional positive and negative examples while maintaining the same data distribution.",	5
7,	"-NEGATIVE",	"EVENT",	"The performance metric of the BERT model after removing negative examples from the training dataset. Negative downsampling is the process of reducing the number of negative examples to balance the training set.",	5
8,	"ENGLISH QUOREF",	"GEO",	"QuoRef MRC is an English dataset used for experiments to test the effect of hyperparameters in Tversky Index. The table contains performance metrics for the English QuoRef dataset.",	3
9,	"+POSITIVE",	"EVENT",	"The performance metric of the BERT model after adding positive examples to the training dataset. Positive augmentation refers to the process of creating a balanced dataset by adding positive examples to the original set.",	6
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"+POSITIVE",	"BERT+CE",	"When positive examples are added, BERT's performance increases to 92.27, showing an improvement over the original performance.",	8.0,	27
1,	"+POSITIVE & NEGATIVE",	"BERT+CE",	"When both positive and negative examples are added, BERT's performance peaks at 93.14, demonstrating the highest improvement over the original performance.",	9.0,	26
2,	"-NEGATIVE",	"BERT+CE",	"When negative examples are removed, BERT's performance drops to 89.73, showing a significant decrease compared to the original performance.",	6.0,	26
3,	"+POSITIVE",	"BERT+DSC",	"Positive examples further enhance BERT+DSC's performance to 92.92, showing a clear benefit.",	7.0,	22
4,	"+POSITIVE",	"BERT+DL",	"Positive examples boost BERT+DL's performance to 92.87, indicating a strong positive impact.",	7.0,	22
5,	"+POSITIVE & NEGATIVE",	"BERT+DSC",	"Combining positive and negative examples achieves the best performance for BERT+DSC at 93.63.",	9.0,	21
6,	"+POSITIVE & NEGATIVE",	"BERT+DL",	"Incorporating both types of examples results in the highest performance for BERT+DL at 93.52.",	9.0,	21
7,	"+POSITIVE",	"BERT+FL",	"With positive examples, BERT+FL's performance further improves to 92.64, showing a consistent trend of enhancement.",	7.0,	21
8,	"-NEGATIVE",	"BERT+DSC",	"Removing negative examples improves BERT+DSC's performance to 90.80.",	7.0,	21
9,	"-NEGATIVE",	"BERT+DL",	"Eliminating negative examples improves BERT+DL's performance to 90.49.",	7.0,	21
10,	"+POSITIVE & NEGATIVE",	"BERT+FL",	"Combining both positive and negative examples yields the best performance for BERT+FL at 93.45.",	9.0,	20
11,	"-NEGATIVE",	"BERT+FL",	"Removing negative examples leads to a notable increase in BERT+FL's performance, reaching 90.79.",	7.0,	20
12,	"+POSITIVE",	"IMAGE_10",	"+ positive是从image_10中提取的实体。",	10.0,	16
13,	"+POSITIVE & NEGATIVE",	"IMAGE_10",	"+ positive & negative是从image_10中提取的实体。",	10.0,	15
14,	"-NEGATIVE",	"IMAGE_10",	"- negative是从image_10中提取的实体。",	10.0,	15
15,	"IMAGE_1",	"QUOREF",	"QUOREF是从image_1中提取的实体。",	10.0,	10
16,	"IMAGE_1",	"SQUAD 1.1",	"SQuAD 1.1是从image_1中提取的实体。",	10.0,	9
17,	"IMAGE_1",	"SQUAD 2.0",	"SQuAD 2.0是从image_1中提取的实体。",	10.0,	9
18,	"CONLL03",	"IMAGE_1",	"CoNLL03 NER是从image_1中提取的实体。",	10.0,	8
19,	"IMAGE_1",	"ONTONOTES5.0",	"OntoNotes5.0 NER是从image_1中提取的实体。",	10.0,	8
20,	"ENGLISH QUOREF",	"TVERSKY INDEX",	"Tversky Index is used with the QuoRef MRC dataset to explore the effect of hyperparameters on performance.",	7.0,	8
21,	"IMAGE_1",	"TABLE 1",	"IMAGE_1" is the image of "TABLE 1".",	10.0,	7
22,	"+POSITIVE",	"ORIGINAL TRAINING SET",	"Positive augmentation is created by adding positive examples to the original training set to balance it.",	7.0,	7
23,	"QUOREF",	"SQUAD 2.0",	"Both are natural language processing tasks but QUOREF focuses on coreference resolution while SQuAD 2.0 is a question-answering task.",	6.0,	7
24,	"QUOREF",	"SQUAD 1.1",	"Both are natural language processing tasks but QUOREF focuses on coreference resolution while SQuAD 1.1 is a question-answering task.",	6.0,	7
25,	"ENGLISH QUOREF",	"Performance Metrics Table",	"The table contains performance metrics for the English QuoRef dataset.",	9.0,	6
26,	"ALPHA VALUES",	"ENGLISH QUOREF",	"Performance metrics for English QuoRef vary with different alpha values.",	8.0,	5
27,	"QUOREF",	"SQUAD",	"SQuAD and Quoref are both QA datasets used for machine reading comprehension.",	6.0,	5
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"# Dice Loss for Data-imbalanced NLP Tasks  

# Xiaoya $\mathrm{Li^{2}}$ , Xiaofei $\mathbf{Sun^{\pmb{\ast}}}$ , Yuxian Meng♣, Junjun Liang♣, Fei $\mathbf{W}\mathbf{u}^{\star}$ and Jiwei $\mathrm{Li}^{\omega\bullet}$  

♠Department of Computer Science and Technology, Zhejiang University ♣Shannon.AI xiaoya li, xiaofei sun, yuxian meng, jiwei li @shannonai.com, wufei $@$ cs.zju.edu.cn  

# Abstract  

Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.  

In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977), which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.  

With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP.  

  

Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.  

# 1 Introduction  

Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016; Nguyen et al., 2016; Rajpurkar et al., 2018; Koˇcisk\`y et al., 2018; Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background.  

Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label. This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, whereas at test time, F1 gives equal weight to positive and negative examples; (2) the overwhelming effect of easy-negative examples. As pointed out by Meng et al. (2019), a significantly large number of negative examples also means that the number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks (Lample et al., 2016; Wu et al., 2019; Devlin et al., 2018; Yu et al., 2018a; McCann et al., 2018; Ma and Hovy, 2016; Chen et al., 2017), handles neither of the issues.  

To handle the first issue, we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The Sørensen–Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the $F_{\beta}$ score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.  

Only using dice loss or Tversky index is not enough since they are unable to address the dominating"
1,	"lattice lstm. arXiv preprint arXiv:1805.02023.  

# A Dataset Details  

# A.1 Part-of-Speech Tagging  

Datasets We conduct experiments on three widely used benchmark, i.e., Chinese Treebank $5.0^{2}/6.0^{3}$ and $\mathrm{UD1.4^{4}}$ .  

CTB5 is a Chinese dataset for tagging and parsing, which contains 507,222 words, 824,983 characters and 18,782 sentences extracted from newswire sources, including 698 articles from Xinhua (1994-1998), 55 articles from Information Services Department of HKSAR (1997) and 132 articles from Sinorama Magazine (1996-1998 & 2000-2001).  

CTB6 is an extension of CTB5, containing 781,351 words, 1,285,149 characters and 28,295 sentences.  

UD is the abbreviation of Universal Dependencies, which is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. In this work, we use UD1.4 for Chinese POS tagging.  

# A.2 Named Entity Recognition  

Datasets For the NER task, we consider both Chinese datasets, i.e., OntoNotes $.4.0^{5}$ and MSRA6 , and English datasets, i.e., CoNLL2003  and OntoNote $\div5.0^{8}$ .  

CoNLL2003 is an English dataset with 4 entity types: Location, Organization, Person and Miscellaneous. We followed data processing protocols in (Ma and Hovy, 2016).  

English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types. We use the standard train/dev/test split of CoNLL2012 shared task.  

Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Since the development set is not provided in the original MSRA dataset, we randomly split the training set into training and development splits by 9:1. We use the official test set for evaluation.  

Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. In this paper, we take the same data split as Wu et al. (2019) did.  

# A.3 Machine Reading Comprephension  

Datasets For MRC task, we use three datasets: $\mathrm{SQuADv1.1/v2.0^{9}}$ and Queref  datasets.  

SQuAD v1.1 and SQuAD $\mathbf{v}2.\mathbf{0}$ are the most widely used QA benchmarks. SQuAD1.1 is a collection of 100K crowdsourced question-answer pairs, and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage.  

Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems, containing 24K questions over 4.7K paragraphs from Wikipedia.  

# A.4 Paraphrase Identification  

Datasets Experiments are conducted on two PI datasets: $\mathbf{M}\mathbf{R}\mathbf{P}\mathbf{C}^{11}$ and $\mathsf{Q Q P}^{12}$ .  

MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations of whether the sentence pairs are semantically equivalent. The MRPC dataset has imbalanced classes (6800 pairs in total, and $68\%$ for positive, $32\%$ for negative).  

QQP is a collection of question pairs from the community question-answering website Quora. The class distribution in QQP is also unbalanced (over 400,000 question pairs in total, and $37\%$ for positive, $63\%$ for negative)."
2,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
    ```
    
The ratio of negative to positive examples for the Quoref task is 169.
mm_response:
['The ratio of negative to positive examples for the Quoref task is 169.']
merged_mm_response:
The ratio of negative to positive examples for the Quoref task is 169.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"BERT+CE",	"MODEL",	"A model that combines BERT with Cross-Entropy loss, achieving 94.90% accuracy on SST-2 and 55.57% on SST-5. BERT is used in experiments for sentiment classification tasks and is fine-tuned with different training objectives.",	21
1,	"BERT-MRC+DL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.22, recall of 93.12, and F1 score of 93.17, showing an improvement of +0.12 over BERT-MRC.",	4
2,	"BERT-MRC+DSC",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.41, recall of 93.25, and F1 score of 93.33, showing the highest improvement of +0.29 over BERT-MRC.",	7
3,	"IMAGE_11",	"ORI_IMG",	"The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' The table is structured with two main columns: SST-2 and SST-5, each representing different datasets. Under each dataset, there are two sub-columns labeled 'Acc' for accuracy. The rows represent different models used for data augmentation: BERT+CE, BERT+DL, and BERT+DSC. The values in the table are as follows: for SST-2, BERT+CE has an accuracy of 94.90%, BERT+DL has 94.37%, and BERT+DSC has 94.84%. For SST-5, BERT+CE has an accuracy of 55.57%, BERT+DL has 54.63%, and BERT+DSC has 55.19%. The table highlights the performance of different models across the two datasets, with BERT+CE achieving the highest accuracy on both SST-2 and SST-5.",	7
4,	"BERT-MRC+FL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.13, recall of 93.09, and F1 score of 93.11, showing an improvement of +0.06 over BERT-MRC.",	4
5,	"BERT-MRC (LI ET AL., 2019)",	"ORGANIZATION",	"A model developed by Li et al. in 2019, achieving a precision of 92.98, recall of 89.95, and an F1 score of 91.11 on the English OntoNotes 5.0 dataset.",	2
6,	"BERT+DL",	"ORGANIZATION",	"BERT+DL is an enhanced version of BERT-Tagger with dynamic loss, which is a strategy to address data imbalance issues in training by adjusting the loss function to give less weight to the majority class and more weight to the minority class.",	16
7,	"IMAGE_10",	"ORI_IMG",	"The image is a table that presents the performance of different models on a dataset with various data augmentation strategies. The table has six columns: 'original', '+ positive', '+ negative', '- negative', and '+ positive & negative'. Each row represents a different model or model configuration. The first row shows the results for the BERT model, with the original accuracy being 91.3%. The subsequent rows show the results for BERT combined with different techniques: FL (Focal Loss), DL (Dynamic Loss), and DSC (Dynamic Sample Contribution). For each model, the table provides the accuracy for the original dataset and the datasets augmented with positive examples, negative examples, down-sampled negative examples, and both positive and negative examples. The values are presented in percentages, with the changes in accuracy relative to the original model shown in parentheses. For example, BERT+FL has an accuracy of 91.86% (+0.56) on the original dataset, 92.64% (+0.37) on the + positive dataset, 90.61% (+0.53) on the + negative dataset, 90.79% (+1.06) on the - negative dataset, and 93.45% (+0.31) on the + positive & negative dataset.",	10
8,	"IMAGE_6",	"ORI_IMG",	"The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision values range from 90.01 to 92.70, the recall values range from 88.35 to 92.56, and the F1 scores range from 88.8 to 92.07. Notably, BERT-MRC+DSC achieves the highest F1 score of 92.07, which is an improvement of +0.96 over BERT-MRC.",	8
9,	"BERT+FL",	"ORGANIZATION",	"An enhanced version of BERT that incorporates a focal loss function to improve its performance on imbalanced datasets. Focal loss is one of the losses compared in the paper, showing little performance improvement on CTB5 and CTB6.",	15
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"BERT+DSC",	"BERT+DSC achieves the highest EM and F1 scores among all BERT-based models on SQuAD v1.1 and QuoRef.",	9.0,	37
1,	"BERT+CE",	"BERT+DL",	"BERT+DL demonstrates significant improvements over BERT in EM and F1 scores on SQuAD v1.1 and QuoRef.",	8.0,	37
2,	"BERT+CE",	"BERT+FL",	"BERT+FL shows slight improvements over BERT in EM and F1 scores on SQuAD v1.1 and QuoRef.",	7.0,	36
3,	"BERT+CE",	"IMAGE_10",	"BERT是从image_10中提取的实体。",	10.0,	31
4,	"BERT+CE",	"IMAGE_8",	"BERT是从image_8中提取的实体。",	10.0,	31
5,	"BERT+CE",	"IMAGE_9",	"BERT (Devlin et al., 2018)是从image_9中提取的实体。",	10.0,	30
6,	"BERT+DL",	"CTB5",	"The performance of BERT+DL was evaluated on CTB5.",	8.0,	29
7,	"BERT+CE",	"IMAGE_11",	"BERT+CE是从image_11中提取的实体。",	10.0,	28
8,	"BERT+CE",	"XLNet",	"XLNet surpasses BERT in EM and F1 scores on SQuAD v1.1 and v2.0.",	8.0,	28
9,	"BERT+FL",	"CTB5",	"The performance of BERT+FL was evaluated on CTB5.",	8.0,	28
10,	"BERT+DL",	"IMAGE_3",	"BERT+DL是从image_3中提取的实体。",	10.0,	27
11,	"BERT+CE",	"ORIGINAL",	"The original performance of BERT is 91.3, which serves as the baseline for comparison with other variants and conditions.",	9.0,	27
12,	"+POSITIVE",	"BERT+CE",	"When positive examples are added, BERT's performance increases to 92.27, showing an improvement over the original performance.",	8.0,	27
13,	"BERT+DL",	"CTB6",	"The performance of BERT+DL was evaluated on CTB6.",	8.0,	27
14,	"BERT+DL",	"IMAGE_8",	"BERT+DL是从image_8中提取的实体。",	10.0,	26
15,	"BERT+FL",	"IMAGE_3",	"BERT+FL是从image_3中提取的实体。",	10.0,	26
16,	"BERT+DL",	"IMAGE_10",	"BERT+DL是从image_10中提取的实体。",	10.0,	26
17,	"BERT+DSC",	"IMAGE_10",	"BERT+DSC是从image_10中提取的实体。",	10.0,	26
18,	"+POSITIVE & NEGATIVE",	"BERT+CE",	"When both positive and negative examples are added, BERT's performance peaks at 93.14, demonstrating the highest improvement over the original performance.",	9.0,	26
19,	"BERT+FL",	"CTB6",	"The performance of BERT+FL was evaluated on CTB6.",	8.0,	26
20,	"BERT+DL",	"UD1.4",	"The performance of BERT+DL was evaluated on UD1.4.",	8.0,	26
21,	"+NEGATIVE",	"BERT+CE",	"When negative examples are added, BERT's performance decreases slightly to 90.08, indicating a minor degradation compared to the original performance.",	7.0,	26
22,	"-NEGATIVE",	"BERT+CE",	"When negative examples are removed, BERT's performance drops to 89.73, showing a significant decrease compared to the original performance.",	6.0,	26
23,	"BERT+DL",	"IMAGE_9",	"BERT+DL是从image_9中提取的实体。",	10.0,	25
24,	"BERT+FL",	"IMAGE_8",	"BERT+FL是从image_8中提取的实体。",	10.0,	25
25,	"BERT+FL",	"IMAGE_10",	"BERT+FL是从image_10中提取的实体。",	10.0,	25
26,	"BERT+CE",	"SST-5",	"The BERT+CE model was tested on the SST-5 dataset.",	9.0,	25
27,	"BERT+CE",	"SST-2",	"The BERT+CE model was tested on the SST-2 dataset.",	9.0,	25
28,	"BERT+FL",	"UD1.4",	"The performance of BERT+FL was evaluated on UD1.4.",	8.0,	25
29,	"BERT+FL",	"IMAGE_9",	"BERT+FL是从image_9中提取的实体。",	10.0,	24
30,	"BERT+CE",	"TABLE 6",	"BERT's performance is evaluated in Table 6 for the MRC task, showing significant improvements with the proposed DSC loss.",	7.0,	24
31,	"BERT+DL",	"IMAGE_11",	"BERT+DL是从image_11中提取的实体。",	10.0,	23
32,	"BERT+DSC",	"IMAGE_11",	"BERT+DSC是从image_11中提取的实体。",	10.0,	23
33,	"BERT+CE",	"QANET",	"BERT outperforms QANet in EM and F1 scores on SQuAD v1.1 and QuoRef.",	8.0,	23
34,	"BERT+CE",	"TABLE 7",	"BERT's performance is evaluated in Table 7 for the PI task, showing a performance boost with DSC as the training objective.",	7.0,	23
35,	"BERT+CE",	"TABLE",	"BERT's performance is detailed in Table 9, which compares different training objectives on sentiment classification tasks.",	6.0,	23
36,	"BERT+CE",	"STANFORD SENTIMENT TREEBANK",	"BERT is fine-tuned and tested on the Stanford Sentiment Treebank datasets for sentiment classification tasks.",	8.0,	22
37,	"BERT+DL",	"ORIGINAL",	"BERT+DL enhances the original performance by 0.62 points, achieving a score of 91.92.",	8.0,	22
38,	"+POSITIVE",	"BERT+DL",	"Positive examples boost BERT+DL's performance to 92.87, indicating a strong positive impact.",	7.0,	22
39,	"BERT+CE",	"NATIONAL NATURAL SCIENCE FOUNDATION OF CHINA",	"The National Natural Science Foundation of China supports the work involving BERT fine-tuning and experiments.",	6.0,	22
40,	"+POSITIVE & NEGATIVE",	"BERT+DL",	"Incorporating both types of examples results in the highest performance for BERT+DL at 93.52.",	9.0,	21
41,	"BERT+FL",	"ORIGINAL",	"BERT+FL improves the original performance by 0.56 points, bringing the score to 91.86.",	8.0,	21
42,	"+POSITIVE",	"BERT+FL",	"With positive examples, BERT+FL's performance further improves to 92.64, showing a consistent trend of enhancement.",	7.0,	21
43,	"-NEGATIVE",	"BERT+DL",	"Eliminating negative examples improves BERT+DL's performance to 90.49.",	7.0,	21
44,	"+NEGATIVE",	"BERT+DL",	"Negative examples cause a minor decline in BERT+DL's performance, dropping to 90.22.",	6.0,	21
45,	"BERT+DL",	"SST-5",	"The BERT+DL model was tested on the SST-5 dataset.",	9.0,	20
46,	"BERT+DL",	"SST-2",	"The BERT+DL model was tested on the SST-2 dataset.",	9.0,	20
47,	"+POSITIVE & NEGATIVE",	"BERT+FL",	"Combining both positive and negative examples yields the best performance for BERT+FL at 93.45.",	9.0,	20
48,	"-NEGATIVE",	"BERT+FL",	"Removing negative examples leads to a notable increase in BERT+FL's performance, reaching 90.79.",	7.0,	20
49,	"+NEGATIVE",	"BERT+FL",	"Adding negative examples results in a slight drop in performance for BERT+FL, reducing the score to 90.61.",	6.0,	20
50,	"BERT+FL",	"DICE LOSS (DL)",	"Dice loss and focal loss are both used to address the issue of easy-negative examples dominating training in imbalanced datasets.",	6.0,	19
51,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"IMAGE_6",	"BERT-Tagger (Devlin et al., 2018)是从image_6中提取的实体。",	10.0,	17
52,	"+POSITIVE",	"IMAGE_10",	"+ positive是从image_10中提取的实体。",	10.0,	16
53,	"BERT-MRC+DSC",	"IMAGE_5",	"BERT-MRC+DSC是从image_5中提取的实体。",	10.0,	16
54,	"IMAGE_10",	"ORIGINAL",	"original是从image_10中提取的实体。",	10.0,	16
55,	"BERT+FL",	"SELF-PACED LEARNING",	"Self-paced learning and focal loss both involve dynamic weight adjustment during training, but they have different focuses and methods.",	5.0,	16
56,	"+NEGATIVE",	"IMAGE_10",	"+ negative是从image_10中提取的实体。",	10.0,	15
57,	"+POSITIVE & NEGATIVE",	"IMAGE_10",	"+ positive & negative是从image_10中提取的实体。",	10.0,	15
58,	"BERT-MRC+DSC",	"IMAGE_6",	"BERT-MRC+DSC是从image_6中提取的实体。",	10.0,	15
59,	"ENGLISH ONTONOTES 5.0",	"IMAGE_6",	"English OntoNotes 5.0是从image_6中提取的实体。",	10.0,	15
60,	"-NEGATIVE",	"IMAGE_10",	"- negative是从image_10中提取的实体。",	10.0,	15
61,	"BERT-MRC+DSC",	"ENGLISH CONLL 2003",	"BERT-MRC+DSC was evaluated in the English CoNLL 2003 event.",	8.0,	15
62,	"BERT-MRC+DSC",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DSC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	14
63,	"BERT-MRC+DL",	"IMAGE_5",	"BERT-MRC+DL是从image_5中提取的实体。",	10.0,	13
64,	"BERT-MRC+FL",	"IMAGE_5",	"BERT-MRC+FL是从image_5中提取的实体。",	10.0,	13
65,	"IMAGE_10",	"TABLE 8",	"IMAGE_10" is the image of "TABLE 8".",	10.0,	12
66,	"BERT-MRC+DL",	"IMAGE_6",	"BERT-MRC+DL是从image_6中提取的实体。",	10.0,	12
67,	"BERT-MRC+FL",	"IMAGE_6",	"BERT-MRC+FL是从image_6中提取的实体。",	10.0,	12
68,	"BERT-MRC+DL",	"ENGLISH CONLL 2003",	"BERT-MRC+DL was evaluated in the English CoNLL 2003 event.",	8.0,	12
69,	"BERT-MRC+FL",	"ENGLISH CONLL 2003",	"BERT-MRC+FL was evaluated in the English CoNLL 2003 event.",	8.0,	12
70,	"IMAGE_11",	"SST-5",	"SST-5是从image_11中提取的实体。",	10.0,	11
71,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
72,	"IMAGE_11",	"SST-2",	"SST-2是从image_11中提取的实体。",	10.0,	11
73,	"BERT-MRC+DL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
74,	"BERT-MRC+FL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+FL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
75,	"CVT (CLARK ET AL., 2018)",	"IMAGE_6",	"CVT (Clark et al., 2018)是从image_6中提取的实体。",	10.0,	10
76,	"BERT-MRC (LI ET AL., 2019)",	"IMAGE_6",	"BERT-MRC (Li et al., 2019)是从image_6中提取的实体。",	10.0,	10
77,	"IMAGE_6",	"TABLE 3",	"IMAGE_6" is the image of "TABLE 3".",	10.0,	10
78,	"IMAGE_11",	"TABLE",	"Table是从image_11中提取的实体。",	10.0,	9
79,	"IMAGE_11",	"TABLE 8",	"IMAGE_11" is the image of "TABLE 8".",	10.0,	9
80,	"BERT-MRC+DSC",	"CHINESE MSRA",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
81,	"BERT-MRC+DSC",	"CHINESE ONTONOTES 4.0",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
82,	"BERT-MRC (LI ET AL., 2019)",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
1,	"false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the $F_{\beta}$ score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.  

Only using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy-negative examples. This is intrinsically because dice loss is actually a soft version of the F1 score. Taking the binary classification task as an example, at test time, an example will be classified as negative as long as its probability is smaller than 0.5, but training will push the value to 0 as much as possible. This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easynegative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hard-negative examples and positive ones. Inspired by the idea of focal loss (Lin et al., 2017) in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$ , and this weight dynamically changes as training proceeds. This strategy helps deemphasize confident examples during training as their probability $p$ approaches 1, making the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples. Combing both strategies, we observe significant performance boosts on a wide range of data imbalanced NLP tasks.  

The rest of this paper is organized as follows: related work is presented in Section 2. We describe different proposed losses in Section 3. Experimental results are presented in Section 4. We perform ablation studies in Section 5, followed by a brief conclusion in Section 6.  

# 2 Related Work  

# 2.1 Data Resampling  

The idea of weighting training examples has a long history. Importance sampling (Kahn and Marshall, 1953) assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost (Kanduri et al., 2018) select harder examples to train subsequent classifiers. Similarly, hard example mining (Malisiewicz et al., 2011) downsamples the majority class and exploits the most difficult examples. Oversampling (Chen et al., 2010; Chawla et al., 2002) is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss (Lin et al., 2017) used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first. At each training step, selfpaced learning algorithm optimizes model parameters and example weights jointly. Other works (Chang et al., 2017; Katharopoulos and Fleuret, 2018) adjusted the weights of different training examples based on training loss. Besides, recent work (Jiang et al., 2017; Fan et al., 2018) proposed to learn a separate network to predict sample weights.  

# 2.2 Data Imbalance Issue in Computer Vision  

The background-object label imbalance issue is severe and thus well studied in the field of object detection (Li et al., 2015; Girshick, 2015; He et al., 2015; Girshick et al., 2013; Ren et al., 2015). The idea of hard negative mining (HNM) (Girshick et al., 2013) has gained much attention recently. Pang et al. (2019) proposed a novel method called IoU-balanced sampling and Chen et al. (2019) designed a ranking model to replace the conventional classification task with an average-precision loss to alleviate the class imbalance issue. The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP.  

Sudre et al. (2017) addressed the severe class imbalance issue for the image segmentation task. They proposed to use the class re-balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks. Shen et al. (2018) investigated the influence of Dice-based loss for multi-class organ segmentation using a dataset of abdominal CT volumes. Kodym et al. (2018) proposed to use the batch soft Dice loss function to train the CNN network for the task of segmentation of organs at risk (OAR) of medical images. Shamir et al. (2019) extended the definition of the classical Dice coefficient to facilitate the direct comparison of a ground truth binary image with a probabilistic map. In this paper, we introduce dice loss into NLP tasks as the training objective and propose a dynamic weight adjusting strategy to address the dominating influence of easy-negative examples.  

# 3 Losses  

# 3.1 Notation  

For illustration purposes, we use the binary classification task to demonstrate how different losses work. The mechanism can be easily extended to multi-class classification. Let $X$ denote a set of training instances and each instance $x_{i}\in X$ is associated with a golden binary label $y_{i}=\left[y_{i0},y_{i1}\right]$ denoting the ground-truth class $x_{i}$ belongs to, and $p_{i}\,=\,[p_{i0},p_{i1}]$ is the predicted probabilities of the two classes respectively, where $y_{i0},y_{i"
    ```
    
The accuracy of BERT+CE on SST-2 is 94.90%, while the accuracy of BERT+DL on SST-2 is 94.37%. The difference in accuracy between BERT+CE and BERT+DL on SST-2 is 0.53%.
mm_response:
['The accuracy of BERT+CE on SST-2 is 94.90%, while the accuracy of BERT+DL on SST-2 is 94.37%. Therefore, the accuracy of BERT+CE exceeds that of BERT+DL by 0.53 percentage points on SST-2.', 'The accuracy of BERT+CE on SST-2 is 94.90%, while the accuracy of BERT+DL on SST-2 is 94.37%. Therefore, the accuracy of BERT+CE exceeds that of BERT+DL by 0.53 percentage points.', 'The accuracy of BERT+CE on SST-2 is 94.90%, while that of BERT+DL is 94.37%. Therefore, the accuracy of BERT+CE exceeds that of BERT+DL by 0.53%.']
merged_mm_response:
The accuracy of BERT+CE on SST-2 exceeds that of BERT+DL by 0.53 percentage points.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"BERT+CE",	"MODEL",	"A model that combines BERT with Cross-Entropy loss, achieving 94.90% accuracy on SST-2 and 55.57% on SST-5. BERT is used in experiments for sentiment classification tasks and is fine-tuned with different training objectives.",	21
1,	"BERT-MRC+DL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.22, recall of 93.12, and F1 score of 93.17, showing an improvement of +0.12 over BERT-MRC.",	4
2,	"BERT-MRC+DSC",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.41, recall of 93.25, and F1 score of 93.33, showing the highest improvement of +0.29 over BERT-MRC.",	7
3,	"BERT+DL",	"ORGANIZATION",	"BERT+DL is an enhanced version of BERT-Tagger with dynamic loss, which is a strategy to address data imbalance issues in training by adjusting the loss function to give less weight to the majority class and more weight to the minority class.",	16
4,	"BERT+DSC",	"ORGANIZATION",	"BERT+DSC is an enhanced version of BERT-Tagger with dynamic sampling consistency, which is a method to address data imbalance by changing the data distribution during training, either through weighting factors or resampling the datasets.",	16
5,	"BERT-MRC+FL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.13, recall of 93.09, and F1 score of 93.11, showing an improvement of +0.06 over BERT-MRC.",	4
6,	"Table 6: Experimental results for MRC task.",	"IMG_ENTITY",	"A table comparing the performance of different models on two datasets, MRPC and QQP, using the F1 score metric. The table includes variations of BERT and XLNet models with additional techniques (FL, DL, DSC) applied to them.",	1
7,	"IMAGE_11",	"ORI_IMG",	"The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' The table is structured with two main columns: SST-2 and SST-5, each representing different datasets. Under each dataset, there are two sub-columns labeled 'Acc' for accuracy. The rows represent different models used for data augmentation: BERT+CE, BERT+DL, and BERT+DSC. The values in the table are as follows: for SST-2, BERT+CE has an accuracy of 94.90%, BERT+DL has 94.37%, and BERT+DSC has 94.84%. For SST-5, BERT+CE has an accuracy of 55.57%, BERT+DL has 54.63%, and BERT+DSC has 55.19%. The table highlights the performance of different models across the two datasets, with BERT+CE achieving the highest accuracy on both SST-2 and SST-5.",	7
8,	"BERT+FL",	"ORGANIZATION",	"An enhanced version of BERT that incorporates a focal loss function to improve its performance on imbalanced datasets. Focal loss is one of the losses compared in the paper, showing little performance improvement on CTB5 and CTB6.",	15
9,	"ORIGINAL",	"EVENT",	"The original performance metric of the BERT model without any additional enhancements or modifications. SQuAD v1.1 is a dataset used for evaluating the performance of models in the task of answering questions.",	6
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"BERT+DSC",	"BERT+DSC achieves the highest EM and F1 scores among all BERT-based models on SQuAD v1.1 and QuoRef.",	9.0,	37
1,	"BERT+CE",	"BERT+DL",	"BERT+DL demonstrates significant improvements over BERT in EM and F1 scores on SQuAD v1.1 and QuoRef.",	8.0,	37
2,	"BERT+CE",	"BERT+FL",	"BERT+FL shows slight improvements over BERT in EM and F1 scores on SQuAD v1.1 and QuoRef.",	7.0,	36
3,	"BERT+CE",	"IMAGE_8",	"BERT是从image_8中提取的实体。",	10.0,	31
4,	"BERT+CE",	"IMAGE_10",	"BERT是从image_10中提取的实体。",	10.0,	31
5,	"BERT+CE",	"IMAGE_9",	"BERT (Devlin et al., 2018)是从image_9中提取的实体。",	10.0,	30
6,	"BERT+DSC",	"CTB5",	"The performance of BERT+DSC was evaluated on CTB5.",	8.0,	29
7,	"BERT+DL",	"CTB5",	"The performance of BERT+DL was evaluated on CTB5.",	8.0,	29
8,	"BERT+CE",	"IMAGE_11",	"BERT+CE是从image_11中提取的实体。",	10.0,	28
9,	"BERT+CE",	"XLNet",	"XLNet surpasses BERT in EM and F1 scores on SQuAD v1.1 and v2.0.",	8.0,	28
10,	"BERT+FL",	"CTB5",	"The performance of BERT+FL was evaluated on CTB5.",	8.0,	28
11,	"BERT+DSC",	"IMAGE_3",	"BERT+DSC是从image_3中提取的实体。",	10.0,	27
12,	"BERT+DL",	"IMAGE_3",	"BERT+DL是从image_3中提取的实体。",	10.0,	27
13,	"BERT+CE",	"ORIGINAL",	"The original performance of BERT is 91.3, which serves as the baseline for comparison with other variants and conditions.",	9.0,	27
14,	"+POSITIVE",	"BERT+CE",	"When positive examples are added, BERT's performance increases to 92.27, showing an improvement over the original performance.",	8.0,	27
15,	"BERT+DL",	"CTB6",	"The performance of BERT+DL was evaluated on CTB6.",	8.0,	27
16,	"BERT+DSC",	"CTB6",	"The performance of BERT+DSC was evaluated on CTB6.",	8.0,	27
17,	"BERT+DL",	"IMAGE_10",	"BERT+DL是从image_10中提取的实体。",	10.0,	26
18,	"BERT+DSC",	"IMAGE_8",	"BERT+DSC是从image_8中提取的实体。",	10.0,	26
19,	"BERT+DSC",	"IMAGE_10",	"BERT+DSC是从image_10中提取的实体。",	10.0,	26
20,	"BERT+FL",	"IMAGE_3",	"BERT+FL是从image_3中提取的实体。",	10.0,	26
21,	"BERT+DL",	"IMAGE_8",	"BERT+DL是从image_8中提取的实体。",	10.0,	26
22,	"+POSITIVE & NEGATIVE",	"BERT+CE",	"When both positive and negative examples are added, BERT's performance peaks at 93.14, demonstrating the highest improvement over the original performance.",	9.0,	26
23,	"BERT+FL",	"CTB6",	"The performance of BERT+FL was evaluated on CTB6.",	8.0,	26
24,	"BERT+DL",	"UD1.4",	"The performance of BERT+DL was evaluated on UD1.4.",	8.0,	26
25,	"BERT+DSC",	"UD1.4",	"The performance of BERT+DSC was evaluated on UD1.4.",	8.0,	26
26,	"+NEGATIVE",	"BERT+CE",	"When negative examples are added, BERT's performance decreases slightly to 90.08, indicating a minor degradation compared to the original performance.",	7.0,	26
27,	"-NEGATIVE",	"BERT+CE",	"When negative examples are removed, BERT's performance drops to 89.73, showing a significant decrease compared to the original performance.",	6.0,	26
28,	"BERT+FL",	"IMAGE_8",	"BERT+FL是从image_8中提取的实体。",	10.0,	25
29,	"BERT+FL",	"IMAGE_10",	"BERT+FL是从image_10中提取的实体。",	10.0,	25
30,	"BERT+DL",	"IMAGE_9",	"BERT+DL是从image_9中提取的实体。",	10.0,	25
31,	"BERT+DSC",	"IMAGE_9",	"BERT+DSC是从image_9中提取的实体。",	10.0,	25
32,	"BERT+CE",	"SST-2",	"The BERT+CE model was tested on the SST-2 dataset.",	9.0,	25
33,	"BERT+CE",	"SST-5",	"The BERT+CE model was tested on the SST-5 dataset.",	9.0,	25
34,	"BERT+FL",	"UD1.4",	"The performance of BERT+FL was evaluated on UD1.4.",	8.0,	25
35,	"BERT+FL",	"IMAGE_9",	"BERT+FL是从image_9中提取的实体。",	10.0,	24
36,	"BERT+CE",	"TABLE 6",	"BERT's performance is evaluated in Table 6 for the MRC task, showing significant improvements with the proposed DSC loss.",	7.0,	24
37,	"BERT+DL",	"IMAGE_11",	"BERT+DL是从image_11中提取的实体。",	10.0,	23
38,	"BERT+DSC",	"IMAGE_11",	"BERT+DSC是从image_11中提取的实体。",	10.0,	23
39,	"BERT+CE",	"QANET",	"BERT outperforms QANet in EM and F1 scores on SQuAD v1.1 and QuoRef.",	8.0,	23
40,	"BERT+CE",	"TABLE 7",	"BERT's performance is evaluated in Table 7 for the PI task, showing a performance boost with DSC as the training objective.",	7.0,	23
41,	"BERT+CE",	"TABLE",	"BERT's performance is detailed in Table 9, which compares different training objectives on sentiment classification tasks.",	6.0,	23
42,	"BERT+DL",	"ORIGINAL",	"BERT+DL enhances the original performance by 0.62 points, achieving a score of 91.92.",	8.0,	22
43,	"BERT+DSC",	"ORIGINAL",	"BERT+DSC outperforms the original BERT by 0.81 points, scoring 92.11.",	8.0,	22
44,	"BERT+CE",	"STANFORD SENTIMENT TREEBANK",	"BERT is fine-tuned and tested on the Stanford Sentiment Treebank datasets for sentiment classification tasks.",	8.0,	22
45,	"+POSITIVE",	"BERT+DSC",	"Positive examples further enhance BERT+DSC's performance to 92.92, showing a clear benefit.",	7.0,	22
46,	"+POSITIVE",	"BERT+DL",	"Positive examples boost BERT+DL's performance to 92.87, indicating a strong positive impact.",	7.0,	22
47,	"BERT+CE",	"NATIONAL NATURAL SCIENCE FOUNDATION OF CHINA",	"The National Natural Science Foundation of China supports the work involving BERT fine-tuning and experiments.",	6.0,	22
48,	"+POSITIVE & NEGATIVE",	"BERT+DL",	"Incorporating both types of examples results in the highest performance for BERT+DL at 93.52.",	9.0,	21
49,	"+POSITIVE & NEGATIVE",	"BERT+DSC",	"Combining positive and negative examples achieves the best performance for BERT+DSC at 93.63.",	9.0,	21
50,	"BERT+FL",	"ORIGINAL",	"BERT+FL improves the original performance by 0.56 points, bringing the score to 91.86.",	8.0,	21
51,	"+POSITIVE",	"BERT+FL",	"With positive examples, BERT+FL's performance further improves to 92.64, showing a consistent trend of enhancement.",	7.0,	21
52,	"-NEGATIVE",	"BERT+DL",	"Eliminating negative examples improves BERT+DL's performance to 90.49.",	7.0,	21
53,	"-NEGATIVE",	"BERT+DSC",	"Removing negative examples improves BERT+DSC's performance to 90.80.",	7.0,	21
54,	"+NEGATIVE",	"BERT+DL",	"Negative examples cause a minor decline in BERT+DL's performance, dropping to 90.22.",	6.0,	21
55,	"+NEGATIVE",	"BERT+DSC",	"Negative examples lead to a slight decrease in BERT+DSC's performance, reducing it to 90.78.",	6.0,	21
56,	"+POSITIVE & NEGATIVE",	"BERT+FL",	"Combining both positive and negative examples yields the best performance for BERT+FL at 93.45.",	9.0,	20
57,	"BERT+DSC",	"SST-5",	"The BERT+DSC model was tested on the SST-5 dataset.",	9.0,	20
58,	"BERT+DL",	"SST-2",	"The BERT+DL model was tested on the SST-2 dataset.",	9.0,	20
59,	"BERT+DL",	"SST-5",	"The BERT+DL model was tested on the SST-5 dataset.",	9.0,	20
60,	"BERT+DSC",	"SST-2",	"The BERT+DSC model was tested on the SST-2 dataset.",	9.0,	20
61,	"-NEGATIVE",	"BERT+FL",	"Removing negative examples leads to a notable increase in BERT+FL's performance, reaching 90.79.",	7.0,	20
62,	"+NEGATIVE",	"BERT+FL",	"Adding negative examples results in a slight drop in performance for BERT+FL, reducing the score to 90.61.",	6.0,	20
63,	"BERT+FL",	"DICE LOSS (DL)",	"Dice loss and focal loss are both used to address the issue of easy-negative examples dominating training in imbalanced datasets.",	6.0,	19
64,	"IMAGE_10",	"ORIGINAL",	"original是从image_10中提取的实体。",	10.0,	16
65,	"BERT-MRC+DSC",	"IMAGE_5",	"BERT-MRC+DSC是从image_5中提取的实体。",	10.0,	16
66,	"BERT+FL",	"SELF-PACED LEARNING",	"Self-paced learning and focal loss both involve dynamic weight adjustment during training, but they have different focuses and methods.",	5.0,	16
67,	"BERT-MRC+DSC",	"IMAGE_6",	"BERT-MRC+DSC是从image_6中提取的实体。",	10.0,	15
68,	"BERT-MRC+DSC",	"ENGLISH CONLL 2003",	"BERT-MRC+DSC was evaluated in the English CoNLL 2003 event.",	8.0,	15
69,	"BERT-MRC+DSC",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DSC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	14
70,	"BERT-MRC+DL",	"IMAGE_5",	"BERT-MRC+DL是从image_5中提取的实体。",	10.0,	13
71,	"BERT-MRC+FL",	"IMAGE_5",	"BERT-MRC+FL是从image_5中提取的实体。",	10.0,	13
72,	"BERT-MRC+FL",	"IMAGE_6",	"BERT-MRC+FL是从image_6中提取的实体。",	10.0,	12
73,	"BERT-MRC+DL",	"IMAGE_6",	"BERT-MRC+DL是从image_6中提取的实体。",	10.0,	12
74,	"BERT-MRC+FL",	"ENGLISH CONLL 2003",	"BERT-MRC+FL was evaluated in the English CoNLL 2003 event.",	8.0,	12
75,	"BERT-MRC+DL",	"ENGLISH CONLL 2003",	"BERT-MRC+DL was evaluated in the English CoNLL 2003 event.",	8.0,	12
76,	"IMAGE_11",	"SST-5",	"SST-5是从image_11中提取的实体。",	10.0,	11
77,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
78,	"IMAGE_11",	"SST-2",	"SST-2是从image_11中提取的实体。",	10.0,	11
79,	"BERT-MRC+FL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+FL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
80,	"BERT-MRC+DL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
81,	"IMAGE_9",	"Table 6: Experimental results for MRC task.",	"IMAGE_9" is the image of Table 6: Experimental results for MRC task..",	10.0,	10
82,	"IMAGE_11",	"TABLE 8",	"IMAGE_11" is the image of "TABLE 8".",	10.0,	9
83,	"IMAGE_11",	"TABLE",	"Table是从image_11中提取的实体。",	10.0,	9
84,	"BERT-MRC+DSC",	"CHINESE MSRA",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
85,	"BERT-MRC+DSC",	"CHINESE ONTONOTES 4.0",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
86,	"MRC TASK",	"ORIGINAL",	"SQuAD v1.1 is a dataset used to evaluate performance in the MRC task.",	8.0,	9
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
1,	"false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the $F_{\beta}$ score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.  

Only using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy-negative examples. This is intrinsically because dice loss is actually a soft version of the F1 score. Taking the binary classification task as an example, at test time, an example will be classified as negative as long as its probability is smaller than 0.5, but training will push the value to 0 as much as possible. This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easynegative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hard-negative examples and positive ones. Inspired by the idea of focal loss (Lin et al., 2017) in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$ , and this weight dynamically changes as training proceeds. This strategy helps deemphasize confident examples during training as their probability $p$ approaches 1, making the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples. Combing both strategies, we observe significant performance boosts on a wide range of data imbalanced NLP tasks.  

The rest of this paper is organized as follows: related work is presented in Section 2. We describe different proposed losses in Section 3. Experimental results are presented in Section 4. We perform ablation studies in Section 5, followed by a brief conclusion in Section 6.  

# 2 Related Work  

# 2.1 Data Resampling  

The idea of weighting training examples has a long history. Importance sampling (Kahn and Marshall, 1953) assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost (Kanduri et al., 2018) select harder examples to train subsequent classifiers. Similarly, hard example mining (Malisiewicz et al., 2011) downsamples the majority class and exploits the most difficult examples. Oversampling (Chen et al., 2010; Chawla et al., 2002) is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss (Lin et al., 2017) used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first. At each training step, selfpaced learning algorithm optimizes model parameters and example weights jointly. Other works (Chang et al., 2017; Katharopoulos and Fleuret, 2018) adjusted the weights of different training examples based on training loss. Besides, recent work (Jiang et al., 2017; Fan et al., 2018) proposed to learn a separate network to predict sample weights.  

# 2.2 Data Imbalance Issue in Computer Vision  

The background-object label imbalance issue is severe and thus well studied in the field of object detection (Li et al., 2015; Girshick, 2015; He et al., 2015; Girshick et al., 2013; Ren et al., 2015). The idea of hard negative mining (HNM) (Girshick et al., 2013) has gained much attention recently. Pang et al. (2019) proposed a novel method called IoU-balanced sampling and Chen et al. (2019) designed a ranking model to replace the conventional classification task with an average-precision loss to alleviate the class imbalance issue. The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP.  

Sudre et al. (2017) addressed the severe class imbalance issue for the image segmentation task. They proposed to use the class re-balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks. Shen et al. (2018) investigated the influence of Dice-based loss for multi-class organ segmentation using a dataset of abdominal CT volumes. Kodym et al. (2018) proposed to use the batch soft Dice loss function to train the CNN network for the task of segmentation of organs at risk (OAR) of medical images. Shamir et al. (2019) extended the definition of the classical Dice coefficient to facilitate the direct comparison of a ground truth binary image with a probabilistic map. In this paper, we introduce dice loss into NLP tasks as the training objective and propose a dynamic weight adjusting strategy to address the dominating influence of easy-negative examples.  

# 3 Losses  

# 3.1 Notation  

For illustration purposes, we use the binary classification task to demonstrate how different losses work. The mechanism can be easily extended to multi-class classification. Let $X$ denote a set of training instances and each instance $x_{i}\in X$ is associated with a golden binary label $y_{i}=\left[y_{i0},y_{i1}\right]$ denoting the ground-truth class $x_{i}$ belongs to, and $p_{i}\,=\,[p_{i0},p_{i1}]$ is the predicted probabilities of the two classes respectively, where $y_{i0},y_{i"
2,	"a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019).  

Baselines We used the following baselines:  

QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions.   
BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction.   
XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that  

  

Table 6: Experimental results for MRC task.   

  
Table 7: Experimental results for PI task.  

enables learning bidirectional contexts.  

Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1.  

# 4.4 Paraphrase Identification  

Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning or not. We conduct experiments on the two widely-used datasets: MRPC (Dolan and Brockett, 2005) and QQP. F1 score is reported for comparison. We use BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) as baselines.  

Results Table 7 shows the results. We find that replacing the training objective with DSC introduces performance boost for both settings, $+0.58$ for MRPC and $+0.73$ for QQP.  

# 5 Ablation Studies  

# 5.1 Datasets imbalanced to different extents  

It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents. We use the paraphrase identification dataset QQP ( $37\%$ positive and $63\%$ negative) for studies. To construct datasets with different imbalance degrees, we used the original QQP dataset to construct synthetic training sets with different positive-negative ratios. Models are trained on these different synthetic sets and then test on the same original test set.  

Original training set (original) The original dataset with 363,871 examples, with $37\%$ being positive and $63\%$ being negative Positive augmentation ( $\downarrow$ positive) We created a balanced dataset by adding positive examples. We first randomly chose positive training examples in the original training set as templates. Then we used Spacy to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia. The augmented set contains 458,477 examples, with $50\%$ being positive and $50\%$ being negative. Negative augmentation ( $\mp$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\%$ being positive and $79\%$ being negative.  

  

# Negative downsampling (- negative)  

We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\%$ being positive and $50\%$ being negative. Positive and negative augmentation ( $\mathbf{\Psi}+$ positive $\pmb{\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\%$ being positive and $50\%$ being negative.  

Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.   

  

Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that $^+$ positive outperforms original, and +negative underperforms original. This is in line with our expectation since $^+$ positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances.  

DSC achieves the highest F1 score across all datasets. Specially, for $^+$ positive, DSC achieves minor improvements $\left(+0.05\,\mathrm{F}1\right)$ ) over DL. In contrast, it significantly outperforms DL for +negative dataset. This is in line with our expectation since DSC helps more on more imbalanced datasets. The performance of FL and DL are not consistent across different datasets, while DSC consistently performs the best on all datasets.  

# 5.2 Dice loss for accuracy-oriented tasks?  

We argue that the cross-entropy objective is actually accuracy-oriented, whereas the"
    ```
    
The performance of BERT+DL on SST-5 is compared to BERT+DSC in the provided data. According to IMAGE_11, BERT+DL has an accuracy of 54.63% on SST-5, while BERT+DSC has an accuracy of 55.19% on the same dataset. Therefore, BERT+DSC outperforms BERT+DL with a higher accuracy on the SST-5 dataset.
mm_response:
['The performance of BERT+DL on SST-5 is 54.63% in terms of accuracy, while that of BERT+DSC is 55.19%. Therefore, BERT+DSC performs slightly better than BERT+DL on the SST-5 dataset.']
merged_mm_response:
After analyzing the responses, I can see that there is only one response provided. This response states that the performance of BERT+DL on SST-5 is 54.63% in terms of accuracy, while that of BERT+DSC is 55.19%. Therefore, BERT+DSC performs slightly better than BERT+DL on the SST-5 dataset.

Since there are no contradictions, repetitions, or inconsistencies among the responses, I can confidently determine that this response best aligns with the user query based on the provided information.

Unified answer: The performance of BERT+DL on SST-5 is 54.63% in terms of accuracy, while that of BERT+DSC is 55.19%. Therefore, BERT+DSC performs slightly better than BERT+DL on the SST-5 dataset.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"BERT-MRC+DSC",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.41, recall of 93.25, and F1 score of 93.33, showing the highest improvement of +0.29 over BERT-MRC.",	7
1,	"Table 6: Experimental results for MRC task.",	"IMG_ENTITY",	"A table comparing the performance of different models on two datasets, MRPC and QQP, using the F1 score metric. The table includes variations of BERT and XLNet models with additional techniques (FL, DL, DSC) applied to them.",	1
2,	"BERT-MRC+DL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.22, recall of 93.12, and F1 score of 93.17, showing an improvement of +0.12 over BERT-MRC.",	4
3,	"BERT-MRC+FL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.13, recall of 93.09, and F1 score of 93.11, showing an improvement of +0.06 over BERT-MRC.",	4
4,	"BERT+DSC",	"ORGANIZATION",	"BERT+DSC is an enhanced version of BERT-Tagger with dynamic sampling consistency, which is a method to address data imbalance by changing the data distribution during training, either through weighting factors or resampling the datasets.",	16
5,	"IMAGE_9",	"ORI_IMG",	"The image is a table labeled 'Table 6: Experimental results for MRC task.' The table compares the performance of different models on two datasets, MRPC and QQP, using the F1 score metric. The rows represent different models and their variations, while the columns show the F1 scores for each dataset. The models listed are BERT (Devlin et al., 2018), BERT+FL, BERT+DL, BERT+DSC, XLNet (Yang et al., 2019), XLNet+FL, XLNet+DL, and XLNet+DSC. For the MRPC dataset, the F1 scores are as follows: BERT has an F1 score of 88.0, BERT+FL has 88.43 (+0.43), BERT+DL has 88.71 (+0.71), and BERT+DSC has 88.92 (+0.92). For the QQP dataset, the F1 scores are: BERT has an F1 score of 91.3, BERT+FL has 91.86 (+0.56), BERT+DL has 91.92 (+0.62), and BERT+DSC has 92.11 (+0.81). For XLNet, the F1 scores are: XLNet has an F1 score of 89.2 for MRPC and 91.8 for QQP, XLNet+FL has 89.25 (+0.05) for MRPC and 92.31 (+0.51) for QQP, XLNet+DL has 89.33 (+0.13) for MRPC and 92.39 (+0.59) for QQP, and XLNet+DSC has 89.78 (+0.58) for MRPC and 92.60 (+0.79) for QQP. The table highlights the performance improvements when additional techniques (FL, DL, DSC) are applied to both BERT and XLNet.",	9
6,	"IMAGE_5",	"ORI_IMG",	"The image is a table labeled 'English CoNLL 2003' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores are provided for each model. Notably, BERT-MRC+DSC achieves the highest F1 score of 93.33, which is an improvement of +0.29 over BERT-MRC. The table highlights the superior performance of the DSC loss in enhancing the F1 score compared to other losses.",	9
7,	"IMAGE_6",	"ORI_IMG",	"The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision values range from 90.01 to 92.70, the recall values range from 88.35 to 92.56, and the F1 scores range from 88.8 to 92.07. Notably, BERT-MRC+DSC achieves the highest F1 score of 92.07, which is an improvement of +0.96 over BERT-MRC.",	8
8,	"BERT-MRC (LI ET AL., 2019)",	"ORGANIZATION",	"A model developed by Li et al. in 2019, achieving a precision of 92.98, recall of 89.95, and an F1 score of 91.11 on the English OntoNotes 5.0 dataset.",	2
9,	"IMAGE_8",	"ORI_IMG",	"The image is a table that presents the experimental results for the machine reading comprehension (MRC) task using different models and datasets. The table is divided into rows and columns. The rows represent different models: QANet, BERT, BERT+FL, BERT+DL, BERT+DSC, XLNet, XLNet+FL, XLNet+DL, and XLNet+DSC. The columns are labeled with the datasets SQuAD v1.1, SQuAD v2.0, and QuoRef, each further divided into EM (Exact Match) and F1 scores. For example, QANet scores 73.6 on EM and 82.7 on F1 for SQuAD v1.1, while BERT scores 84.1 on EM and 90.9 on F1 for the same dataset. The table also shows the performance improvements of the proposed methods over the baseline models. For instance, BERT+DSC achieves 85.34 on EM and 91.97 on F1 for SQuAD v1.1, which is an improvement of +1.24 and +1.07 respectively over BERT. Similarly, XLNet+DSC achieves 89.79 on EM and 95.77 on F1 for SQuAD v1.1, which is an improvement of +0.84 and +1.25 respectively over XLNet. The table highlights the significant performance boost obtained by the proposed DSC loss method on both EM and F1 scores across all datasets.",	10
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"BERT+DSC",	"BERT+DSC achieves the highest EM and F1 scores among all BERT-based models on SQuAD v1.1 and QuoRef.",	9.0,	37
1,	"BERT+CE",	"IMAGE_8",	"BERT是从image_8中提取的实体。",	10.0,	31
2,	"BERT+CE",	"IMAGE_9",	"BERT (Devlin et al., 2018)是从image_9中提取的实体。",	10.0,	30
3,	"BERT+DSC",	"CTB5",	"The performance of BERT+DSC was evaluated on CTB5.",	8.0,	29
4,	"BERT+DSC",	"IMAGE_3",	"BERT+DSC是从image_3中提取的实体。",	10.0,	27
5,	"BERT+DSC",	"CTB6",	"The performance of BERT+DSC was evaluated on CTB6.",	8.0,	27
6,	"BERT+DSC",	"IMAGE_8",	"BERT+DSC是从image_8中提取的实体。",	10.0,	26
7,	"BERT+DSC",	"IMAGE_10",	"BERT+DSC是从image_10中提取的实体。",	10.0,	26
8,	"BERT+DL",	"IMAGE_8",	"BERT+DL是从image_8中提取的实体。",	10.0,	26
9,	"BERT+DSC",	"UD1.4",	"The performance of BERT+DSC was evaluated on UD1.4.",	8.0,	26
10,	"BERT+FL",	"IMAGE_8",	"BERT+FL是从image_8中提取的实体。",	10.0,	25
11,	"BERT+DSC",	"IMAGE_9",	"BERT+DSC是从image_9中提取的实体。",	10.0,	25
12,	"BERT+DL",	"IMAGE_9",	"BERT+DL是从image_9中提取的实体。",	10.0,	25
13,	"BERT+FL",	"IMAGE_9",	"BERT+FL是从image_9中提取的实体。",	10.0,	24
14,	"BERT+DSC",	"IMAGE_11",	"BERT+DSC是从image_11中提取的实体。",	10.0,	23
15,	"BERT+DSC",	"ORIGINAL",	"BERT+DSC outperforms the original BERT by 0.81 points, scoring 92.11.",	8.0,	22
16,	"+POSITIVE",	"BERT+DSC",	"Positive examples further enhance BERT+DSC's performance to 92.92, showing a clear benefit.",	7.0,	22
17,	"+POSITIVE & NEGATIVE",	"BERT+DSC",	"Combining positive and negative examples achieves the best performance for BERT+DSC at 93.63.",	9.0,	21
18,	"-NEGATIVE",	"BERT+DSC",	"Removing negative examples improves BERT+DSC's performance to 90.80.",	7.0,	21
19,	"+NEGATIVE",	"BERT+DSC",	"Negative examples lead to a slight decrease in BERT+DSC's performance, reducing it to 90.78.",	6.0,	21
20,	"BERT+DSC",	"SST-5",	"The BERT+DSC model was tested on the SST-5 dataset.",	9.0,	20
21,	"BERT+DSC",	"SST-2",	"The BERT+DSC model was tested on the SST-2 dataset.",	9.0,	20
22,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"IMAGE_6",	"BERT-Tagger (Devlin et al., 2018)是从image_6中提取的实体。",	10.0,	17
23,	"IMAGE_8",	"XLNet",	"XLNet是从image_8中提取的实体。",	10.0,	17
24,	"ENGLISH CONLL 2003",	"IMAGE_5",	"English CoNLL 2003是从image_5中提取的实体。",	10.0,	17
25,	"BERT-MRC+DSC",	"IMAGE_5",	"BERT-MRC+DSC是从image_5中提取的实体。",	10.0,	16
26,	"ENGLISH ONTONOTES 5.0",	"IMAGE_6",	"English OntoNotes 5.0是从image_6中提取的实体。",	10.0,	15
27,	"BERT-MRC+DSC",	"IMAGE_6",	"BERT-MRC+DSC是从image_6中提取的实体。",	10.0,	15
28,	"BERT-MRC+DSC",	"ENGLISH CONLL 2003",	"BERT-MRC+DSC was evaluated in the English CoNLL 2003 event.",	8.0,	15
29,	"BERT-MRC+DSC",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DSC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	14
30,	"BERT-MRC+DL",	"IMAGE_5",	"BERT-MRC+DL是从image_5中提取的实体。",	10.0,	13
31,	"IMAGE_8",	"MRC TASK",	"IMAGE_8" is the image of "MRC TASK".",	10.0,	13
32,	"IMAGE_8",	"XLNET+DSC",	"XLNet+DSC是从image_8中提取的实体。",	10.0,	13
33,	"IMAGE_8",	"XLNET+FL",	"XLNet+FL是从image_8中提取的实体。",	10.0,	13
34,	"BERT-MRC+FL",	"IMAGE_5",	"BERT-MRC+FL是从image_5中提取的实体。",	10.0,	13
35,	"IMAGE_8",	"XLNET+DL",	"XLNet+DL是从image_8中提取的实体。",	10.0,	13
36,	"BERT-MRC+FL",	"IMAGE_6",	"BERT-MRC+FL是从image_6中提取的实体。",	10.0,	12
37,	"IMAGE_9",	"XLNET+DL",	"XLNet+DL是从image_9中提取的实体。",	10.0,	12
38,	"IMAGE_9",	"XLNET+DSC",	"XLNet+DSC是从image_9中提取的实体。",	10.0,	12
39,	"IMAGE_9",	"XLNET+FL",	"XLNet+FL是从image_9中提取的实体。",	10.0,	12
40,	"BERT-MRC+DL",	"IMAGE_6",	"BERT-MRC+DL是从image_6中提取的实体。",	10.0,	12
41,	"IMAGE_8",	"QANET",	"QANet是从image_8中提取的实体。",	10.0,	12
42,	"BERT-MRC+DL",	"ENGLISH CONLL 2003",	"BERT-MRC+DL was evaluated in the English CoNLL 2003 event.",	8.0,	12
43,	"BERT-MRC+FL",	"ENGLISH CONLL 2003",	"BERT-MRC+FL was evaluated in the English CoNLL 2003 event.",	8.0,	12
44,	"CVT(CLARK ET AL., 2018)",	"IMAGE_5",	"CVT(Clark et al., 2018)是从image_5中提取的实体。",	10.0,	11
45,	"ELMO(PETERS ET AL., 2018)",	"IMAGE_5",	"ELMo(Peters et al., 2018)是从image_5中提取的实体。",	10.0,	11
46,	"BERT-MRC(LI ET AL., 2019)",	"IMAGE_5",	"BERT-MRC(Li et al., 2019)是从image_5中提取的实体。",	10.0,	11
47,	"IMAGE_5",	"TABLE 5",	"IMAGE_5" is the image of "TABLE 5".",	10.0,	11
48,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
49,	"BERT-TAGGER (Devlin et al., 2018)",	"IMAGE_5",	"BERT-Tagger(Devlin et al., 2018)是从image_5中提取的实体。",	10.0,	11
50,	"BERT-MRC+FL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+FL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
51,	"BERT-MRC+DL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
52,	"IMAGE_6",	"TABLE 3",	"IMAGE_6" is the image of "TABLE 3".",	10.0,	10
53,	"IMAGE_9",	"Table 6: Experimental results for MRC task.",	"IMAGE_9" is the image of Table 6: Experimental results for MRC task..",	10.0,	10
54,	"CVT (CLARK ET AL., 2018)",	"IMAGE_6",	"CVT (Clark et al., 2018)是从image_6中提取的实体。",	10.0,	10
55,	"IMAGE_9",	"XLNET (YANG ET AL., 2019)",	"XLNet (Yang et al., 2019)是从image_9中提取的实体。",	10.0,	10
56,	"BERT-MRC (LI ET AL., 2019)",	"IMAGE_6",	"BERT-MRC (Li et al., 2019)是从image_6中提取的实体。",	10.0,	10
57,	"BERT-MRC+DSC",	"CHINESE ONTONOTES 4.0",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
58,	"BERT-MRC+DSC",	"CHINESE MSRA",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
59,	"BERT-MRC (LI ET AL., 2019)",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
    ```
    -----Sources-----
    ```csv
    "id",	"content"
    ```
    
BERT+DSC achieved a performance boost on the MRPC dataset. According to the data:

(1) BERT, the base model, had an F1 score of 88.0 on MRPC.
(2) BERT+DSC improved upon the base model, achieving an F1 score of 88.92 on MRPC.
(3) The improvement of BERT+DSC over BERT is +0.92 in F1 score for the MRPC dataset.
mm_response:
['BERT+DSC achieved an F1 score of 88.92 for the MRPC dataset, which is an improvement of +0.92 over the baseline BERT model.', 'For the MRPC dataset, BERT+DSC achieved an F1 score of 88.92, which is an improvement of +0.92 over the baseline BERT model with an F1 score of 88.0.', 'For the MRPC dataset, BERT+DSC achieved an F1 score of 88.92, which is an improvement of +0.92 over the baseline BERT model that had an F1 score of 88.0.']
merged_mm_response:
The performance boost that BERT+DSC achieved for the MRPC dataset is an improvement of +0.92 in the F1 score over the baseline BERT model.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"CVT (CLARK ET AL., 2018)",	"ORGANIZATION",	"A model developed by Clark et al. in 2018, achieving an F1 score of 88.8 on the English OntoNotes 5.0 dataset.",	2
1,	"ENGLISH WSJ",	"EVENT",	"ENGLISH WSJ is a dataset used for evaluating the performance of different models in terms of precision, recall, and F1 score. Wall Street Journal is an English dataset used for part-of-speech tagging experiments.",	6
2,	"F1",	"UNKNOWN",	"The F1 score is used to measure the performance of models, and Eq.5 is a soft form of F1 using a continuous probability p.",	1
3,	"F1 SCORE",	"CONCEPT",	"F1 score is a measure of a model's accuracy, specifically the weighted average of precision and recall."<SEP>"F1 score is a measure that is more concerned with positive examples and is used to evaluate the performance of models in the paper."<SEP>"F1 score is a metric used to evaluate machine reading comprehension tasks."<SEP>"F1 score is used as a performance metric, with the proposed loss function aiming to bridge the gap between training objectives and F1 score.",	2
4,	"ELMO(PETERS ET AL., 2018)",	"ORGANIZATION",	"A model developed by Peters et al. in 2018 with an F1 score of 92.22.",	2
5,	"IMAGE_4",	"ORI_IMG",	"The image is a table labeled 'Table 4: Experimental results for English POS datasets.' The table is divided into two main sections, each representing different datasets: English WSJ and English Tweets. Each section contains rows corresponding to different models and their performance metrics. For the English WSJ dataset, the columns are 'Model,' 'Prec.', 'Rec.', and 'F1.' The models listed include Meta BiLSTM, BERT-Tagger, BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The precision (Prec.), recall (Rec.), and F1 scores are provided for each model. For example, BERT-Tagger has a precision of 99.21, recall of 98.36, and an F1 score of 98.86. The best-performing model in this dataset is BERT-Tagger+DSC with a precision of 99.41, recall of 98.93, and an F1 score of 99.38. For the English Tweets dataset, the same structure is followed. The models listed are FastText+CNN+CRF, BERT-Tagger, BERT-Tagger+FL, BERT-Tagger+DL, and BERT-Tagger+DSC. The precision, recall, and F1 scores are provided for each model. For example, BERT-Tagger has a precision of 92.33, recall of 91.98, and an F1 score of 92.34. The best-performing model in this dataset is BERT-Tagger+DSC with a precision of 92.87, recall of 93.54, and an F1 score of 92.58.",	9
6,	"IMAGE_8",	"ORI_IMG",	"The image is a table that presents the experimental results for the machine reading comprehension (MRC) task using different models and datasets. The table is divided into rows and columns. The rows represent different models: QANet, BERT, BERT+FL, BERT+DL, BERT+DSC, XLNet, XLNet+FL, XLNet+DL, and XLNet+DSC. The columns are labeled with the datasets SQuAD v1.1, SQuAD v2.0, and QuoRef, each further divided into EM (Exact Match) and F1 scores. For example, QANet scores 73.6 on EM and 82.7 on F1 for SQuAD v1.1, while BERT scores 84.1 on EM and 90.9 on F1 for the same dataset. The table also shows the performance improvements of the proposed methods over the baseline models. For instance, BERT+DSC achieves 85.34 on EM and 91.97 on F1 for SQuAD v1.1, which is an improvement of +1.24 and +1.07 respectively over BERT. Similarly, XLNet+DSC achieves 89.79 on EM and 95.77 on F1 for SQuAD v1.1, which is an improvement of +0.84 and +1.25 respectively over XLNet. The table highlights the significant performance boost obtained by the proposed DSC loss method on both EM and F1 scores across all datasets.",	10
7,	"BERT-MRC (LI ET AL., 2019)",	"ORGANIZATION",	"A model developed by Li et al. in 2019, achieving a precision of 92.98, recall of 89.95, and an F1 score of 91.11 on the English OntoNotes 5.0 dataset.",	2
8,	"CROSS-ENTROPY",	"CONCEPT",	"Cross-entropy is discussed as an accuracy-oriented objective, compared to the proposed losses which are soft versions of F1 score.",	0
9,	"IMAGE_5",	"ORI_IMG",	"The image is a table labeled 'English CoNLL 2003' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores are provided for each model. Notably, BERT-MRC+DSC achieves the highest F1 score of 93.33, which is an improvement of +0.29 over BERT-MRC. The table highlights the superior performance of the DSC loss in enhancing the F1 score compared to other losses.",	9
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"IMAGE_8",	"BERT是从image_8中提取的实体。",	10.0,	31
1,	"BERT+DL",	"IMAGE_8",	"BERT+DL是从image_8中提取的实体。",	10.0,	26
2,	"BERT+DSC",	"IMAGE_8",	"BERT+DSC是从image_8中提取的实体。",	10.0,	26
3,	"BERT+FL",	"IMAGE_8",	"BERT+FL是从image_8中提取的实体。",	10.0,	25
4,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"IMAGE_4",	"BERT-Tagger是从image_4中提取的实体。",	10.0,	18
5,	"ENGLISH CONLL 2003",	"IMAGE_5",	"English CoNLL 2003是从image_5中提取的实体。",	10.0,	17
6,	"IMAGE_8",	"XLNet",	"XLNet是从image_8中提取的实体。",	10.0,	17
7,	"BERT-MRC+DSC",	"IMAGE_5",	"BERT-MRC+DSC是从image_5中提取的实体。",	10.0,	16
8,	"ENGLISH WSJ",	"IMAGE_4",	"English WSJ是从image_4中提取的实体。",	10.0,	15
9,	"English Tweets Dataset",	"IMAGE_4",	"English Tweets是从image_4中提取的实体。",	10.0,	15
10,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"ENGLISH WSJ",	"The BERT-Tagger model was evaluated on the English WSJ dataset.",	8.0,	15
11,	"BERT-TAGGER+FL",	"IMAGE_4",	"BERT-Tagger+FL是从image_4中提取的实体。",	10.0,	14
12,	"IMAGE_8",	"XLNET+DSC",	"XLNet+DSC是从image_8中提取的实体。",	10.0,	13
13,	"IMAGE_8",	"XLNET+DL",	"XLNet+DL是从image_8中提取的实体。",	10.0,	13
14,	"IMAGE_8",	"XLNET+FL",	"XLNet+FL是从image_8中提取的实体。",	10.0,	13
15,	"IMAGE_8",	"MRC TASK",	"IMAGE_8" is the image of "MRC TASK".",	10.0,	13
16,	"BERT-MRC+FL",	"IMAGE_5",	"BERT-MRC+FL是从image_5中提取的实体。",	10.0,	13
17,	"BERT-MRC+DL",	"IMAGE_5",	"BERT-MRC+DL是从image_5中提取的实体。",	10.0,	13
18,	"BERT-TAGGER+DL",	"IMAGE_4",	"BERT-Tagger+DL是从image_4中提取的实体。",	10.0,	13
19,	"BERT-TAGGER+DSC",	"IMAGE_4",	"BERT-Tagger+DSC是从image_4中提取的实体。",	10.0,	12
20,	"IMAGE_8",	"QANET",	"QANet是从image_8中提取的实体。",	10.0,	12
21,	"ELMO(PETERS ET AL., 2018)",	"IMAGE_5",	"ELMo(Peters et al., 2018)是从image_5中提取的实体。",	10.0,	11
22,	"CVT(CLARK ET AL., 2018)",	"IMAGE_5",	"CVT(Clark et al., 2018)是从image_5中提取的实体。",	10.0,	11
23,	"BERT-MRC(LI ET AL., 2019)",	"IMAGE_5",	"BERT-MRC(Li et al., 2019)是从image_5中提取的实体。",	10.0,	11
24,	"FASTTEXT+CNN+CRF",	"IMAGE_4",	"FastText+CNN+CRF是从image_4中提取的实体。",	10.0,	11
25,	"IMAGE_5",	"TABLE 5",	"IMAGE_5" is the image of "TABLE 5".",	10.0,	11
26,	"BERT-TAGGER (Devlin et al., 2018)",	"IMAGE_5",	"BERT-Tagger(Devlin et al., 2018)是从image_5中提取的实体。",	10.0,	11
27,	"IMAGE_4",	"META BILSTM",	"Meta BiLSTM是从image_4中提取的实体。",	10.0,	11
28,	"BERT-TAGGER+FL",	"ENGLISH WSJ",	"The BERT-Tagger+FL model was evaluated on the English WSJ dataset.",	8.0,	11
29,	"BERT-MRC (LI ET AL., 2019)",	"IMAGE_6",	"BERT-MRC (Li et al., 2019)是从image_6中提取的实体。",	10.0,	10
30,	"CVT (CLARK ET AL., 2018)",	"IMAGE_6",	"CVT (Clark et al., 2018)是从image_6中提取的实体。",	10.0,	10
31,	"IMAGE_4",	"TABLE 4",	"IMAGE_4" is the image of "TABLE 4".",	10.0,	10
32,	"BERT-TAGGER+DL",	"ENGLISH WSJ",	"The BERT-Tagger+DL model was evaluated on the English WSJ dataset.",	8.0,	10
33,	"ELMO(PETERS ET AL., 2018)",	"ENGLISH CONLL 2003",	"ELMo was evaluated in the English CoNLL 2003 event.",	8.0,	10
34,	"CVT (CLARK ET AL., 2018)",	"ENGLISH ONTONOTES 5.0",	"The CVT model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
35,	"BERT-MRC (LI ET AL., 2019)",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
36,	"BERT-TAGGER+DSC",	"ENGLISH WSJ",	"The BERT-Tagger+DSC model was evaluated on the English WSJ dataset.",	8.0,	9
37,	"ENGLISH WSJ",	"META BILSTM",	"The Meta BiLSTM model was evaluated on the English WSJ dataset.",	8.0,	8
38,	"EQ.5",	"F1 SCORE",	"Eq.5 is a soft form of the F1 score, using a continuous probability p instead of a binary indicator function.",	7.0,	3
39,	"F1 SCORE",	"F1",	"The F1 score is used to measure the performance of models, and Eq.5 is a soft form of F1 using a continuous probability p.",	6.0,	3
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
1,	"i})=\frac{2p_{i1}y_{i1}+\gamma}{p_{i1}+y_{i1}+\gamma}
$$  

As can be seen, negative examples whose DSC is $\frac{\gamma}{p_{i1}\!+\!\gamma}$ , also contribute to the training. Additionally, Milletari et al. (2016) proposed to change the denominator to the square form for faster convergence, which leads to the following dice loss (DL):  

$$
\mathrm{DL}=\frac{1}{N}\sum_{i}\left[1-\frac{2p_{i1}y_{i1}+\gamma}{p_{i1}^{2}+y_{i1}^{2}+\gamma}\right]
$$  

Another version of DL is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient, which is easier for optimization:  

$$
\mathrm{DL}=1-\frac{2\sum_{i}p_{i1}y_{i1}+\gamma}{\sum_{i}p_{i1}^{2}+\sum_{i}y_{i1}^{2}+\gamma}
$$  

Tversky index (TI), which can be thought as the approximation of the $F_{\beta}$ score, extends dice coeffi- cient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows:  

$$
\mathrm{TI}=\frac{|A\cap B|}{|A\cap B|+\alpha|A\backslash B|+\beta|B\backslash A|}
$$  

Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\alpha=\beta=0.5$ The Tversky loss (TL) is thus given as follows:  

$$
\mathrm{TL}=\frac{1}{N}\sum_{i}\left[1-\frac{p_{i1}y_{i1}+\gamma}{p_{i1}y_{i1}+\alpha\,p_{i1}y_{i0}+\beta\,p_{i0}y_{i1}+\gamma}\right]
$$  

# 3.4 Self-adjusting Dice Loss  

Consider a simple case where the dataset consists of only one example $x_{i}$ , which is classified as positive as long as $p_{i1}$ is larger than 0.5. The computation of $F1$ score is actually as follows:  

$$
\mathrm{F}1(x_{i})=2\;{\frac{\mathbb{I}(p_{i1}>0.5)y_{i1}}{\mathbb{I}(p_{i1}>0.5)+y_{i1}}}
$$  

  
Figure 1: An illustration of derivatives of the four losses. The derivative of DSC approaches zero right after $p$ exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F1$ , using a continuous $p$ rather than the binary $\mathbb{I}(p_{i1}>0.5)$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance.  

To address this issue, we propose to multiply the soft probability $p$ with a decaying factor $(1-p)$ changing Eq.11 to the following adaptive variant of DSC:  

$$
\operatorname{DSC}(x_{i})={\frac{2(1-p_{i1})p_{i1}\cdot y_{i1}+\gamma}{(1-p_{i1})p_{i1}+y_{i1}+\gamma}}
$$  

One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them.  

A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\gamma}$ factor, leading the final loss to be $-(1-p)^{\gamma}\log p$ .  

  
Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only"
2,	"0.05\,\mathrm{F}1\right)$ ) over DL. In contrast, it significantly outperforms DL for +negative dataset. This is in line with our expectation since DSC helps more on more imbalanced datasets. The performance of FL and DL are not consistent across different datasets, while DSC consistently performs the best on all datasets.  

# 5.2 Dice loss for accuracy-oriented tasks?  

We argue that the cross-entropy objective is actually accuracy-oriented, whereas the proposed losses perform as a soft version of F1 score. To Table 9: The effect of DL and DSC on sentiment classification tasks. BERT $\mathrm{\bf\Sigma+}\mathrm{\bfCE}$ refers to fine-tuning BERT and setting cross-entropy as the training objective.  

explore the effect of the dice loss on accuracyoriented tasks such as text classification, we conduct experiments on the Stanford Sentiment Treebank (SST) datasets including SST-2 and SST-5. We fine-tuned $\mathsf{B E R T_{L a r g e}}$ with different training objectives. Experimental results for SST are shown in Table 9. For SST-5, BERT with CE achieves 55.57 in terms of accuracy, while DL and DSC perform slightly worse (54.63 and 55.19, respectively). Similar phenomenon is observed for SST-2. These results verify that the proposed dice loss is not accuracy-oriented, and should not be used for accuracy-oriented tasks.  

# 5.3 Hyper-parameters in Tversky Index  

As mentioned in Section 3.3, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., $\alpha$ and $\beta$ ) in TI to test how they manipulate the tradeoff. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset. Experimental results are shown in Table 10. The highest F1 on Chinese OntoNotes4.0 is 84.67 when $\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\alpha$ is set to 0.4. In addition, we can observe that the performance varies a lot as $\alpha$ changes in distinct datasets, which shows that the hyperparameters $\alpha,\beta$ acturally play an important role in TI.  

# 6 Conclusion  

In this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (F1 score). Experimental results show that the proposed loss function help Table 10: The effect of hyperparameters in Tversky Index. We set $\beta=1-\alpha$ and thus we only list $\alpha$ here.  

  

to achieve significant performance boost without changing model architectures.  

# Acknowledgement  

We thank all anonymous reviewers, as well as Qinghong Han, Wei Wu and Jiawei Wu for their comments and suggestions. The work is supported by the National Natural Science Foundation of China (NSFC No. 61625107 and 61751209).  

# References  

Bernd Bohnet, Ryan T. McDonald, Gon¸calo Simo˜es, Daniel Andor, Emily Pitler, and Joshua Maynez. 2018. Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2642–2652.  

Haw-Shiuan Chang, Erik G. Learned-Miller, and Andrew McCallum. 2017. Active bias: Training more accurate neural networks by emphasizing high variance samples. In NIPS.  

N. V. Chawla, K. W. Bowyer, Lawrence O. Hall, and W. P. Kegelmeyer. 2002. Smote: Synthetic minority over-sampling technique. J. Artif. Intell. Res., 16:321– 357.  

Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051.  

Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang, Lingyu Duan, Zhibo Chen, Changwei He, and Junni Zou. 2019. Towards accurate one-stage object detection with ap-loss. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 5119–5127.  

Shijuan Chen, Haibo He, and Edwardo A. Garcia. 2010. Ramoboost: Ranked minority oversampling in boosting. IEEE Transactions on Neural Networks, 21:1624– 1642.  

Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc V. Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Procfessing, Brussels, Belgium, October 31 - November 4, 2018, pages 1914–1925.  

Pradeep Dasigi, Nelson F Liu, Ana Marasovic, Noah A Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. arXiv preprint arXiv"
    ```
    
The model that achieved the highest F1 score in the English WSJ dataset is BERT-Tagger+DSC, with an F1 score of 99.38.
mm_response:
['The BERT-Tagger+DSC model achieved the highest F1 score in the English WSJ dataset, with an F1 score of 99.38.', 'The BERT-Tagger+DSC model achieved the highest F1 score in the English WSJ dataset with a score of 99.38.', 'The BERT-Tagger+DSC model achieved the highest F1 score of 99.38 in the English WSJ dataset.']
merged_mm_response:
The model that achieved the highest F1 score in the English WSJ dataset is the BERT-Tagger+DSC model, with an F1 score of 99.38.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"CVT (CLARK ET AL., 2018)",	"ORGANIZATION",	"A model developed by Clark et al. in 2018, achieving an F1 score of 88.8 on the English OntoNotes 5.0 dataset.",	2
1,	"IMAGE_12",	"ORI_IMG",	"The image is a table that presents the effect of hyperparameters in the Tversky Index. The table has three columns: 'α', 'Chinese Onto4.0', and 'English QuoRef'. The first column lists values of α ranging from 0.1 to 0.9 in increments of 0.1. The second column shows corresponding F1 scores for the Chinese Onto4.0 dataset, with values ranging from 80.13 to 84.67. The third column displays F1 scores for the English QuoRef dataset, ranging from 63.23 to 68.44. Notably, the highest F1 score for Chinese Onto4.0 is 84.67 at α = 0.6, while for English QuoRef, the highest F1 score is 68.44 at α = 0.4.",	2
2,	"CHINESE ONTONOTES 4.0",	"EVENT",	"CHINESE ONTONOTES 4.0 is another dataset used for evaluating named entity recognition models, where BERT-MRC+DSC achieves a F1 score of 84.47. It is also mentioned as a dataset used for named entity recognition experiments, proposed by Pradhan et al. in 2011.",	2
3,	"IMAGE_7",	"ORI_IMG",	"The image is a table labeled 'Table 5: Experimental results for NER task.' The table is divided into two main sections, each representing different datasets: Chinese MSRA and Chinese OntoNotes 4.0. Each section contains rows listing various models used for Named Entity Recognition (NER) tasks and their corresponding performance metrics: Precision (Prec.), Recall (Rec.), and F1 score. For the Chinese MSRA dataset, the models listed are Lattice-LSTM, BERT-Tagger, Glyce-BERT, BERT-MRC, BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The highest F1 score is achieved by BERT-MRC+DSC with a value of 96.72. For the Chinese OntoNotes 4.0 dataset, the same models are listed, and the highest F1 score is also achieved by BERT-MRC+DSC with a value of 84.47. The table highlights the performance improvements of the DSC loss over other models, with specific gains indicated in parentheses next to the F1 scores.",	4
4,	"TABLE 7",	"EVENT",	"Table 7 displays the results for the PI task, highlighting performance improvements in F1 score.",	2
5,	"BERT-MRC (LI ET AL., 2019)",	"ORGANIZATION",	"A model developed by Li et al. in 2019, achieving a precision of 92.98, recall of 89.95, and an F1 score of 91.11 on the English OntoNotes 5.0 dataset.",	2
6,	"IMAGE_6",	"ORI_IMG",	"The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision values range from 90.01 to 92.70, the recall values range from 88.35 to 92.56, and the F1 scores range from 88.8 to 92.07. Notably, BERT-MRC+DSC achieves the highest F1 score of 92.07, which is an improvement of +0.96 over BERT-MRC.",	8
7,	"TABLE 8",	"EVENT",	"Table 8 illustrates the effect of different data augmentation methods on the QQP dataset in terms of F1-score.",	2
8,	"Performance Metrics Table",	"OBJECT",	"A table that presents a comparative analysis of performance metrics for two datasets, Chinese Onto4.0 and English QuoRef, across varying values of alpha (α) from 0.1 to 0.9. This table is likely part of a study or research paper detailing the effects of different alpha values on model performance, possibly in the context of machine learning or data analysis.",	3
9,	"ONTONOTES5.0",	"EVENT",	"OntoNotes5.0 is a dataset used in the paper for the named entity recognition task. A named entity recognition task with 1.96M negative and 239K positive samples, resulting in a ratio of 8.18.",	2
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"TABLE 7",	"BERT's performance is evaluated in Table 7 for the PI task, showing a performance boost with DSC as the training objective.",	7.0,	23
1,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"IMAGE_6",	"BERT-Tagger (Devlin et al., 2018)是从image_6中提取的实体。",	10.0,	17
2,	"ENGLISH ONTONOTES 5.0",	"IMAGE_6",	"English OntoNotes 5.0是从image_6中提取的实体。",	10.0,	15
3,	"BERT-MRC+DSC",	"IMAGE_6",	"BERT-MRC+DSC是从image_6中提取的实体。",	10.0,	15
4,	"BERT-MRC+DL",	"IMAGE_6",	"BERT-MRC+DL是从image_6中提取的实体。",	10.0,	12
5,	"IMAGE_10",	"TABLE 8",	"IMAGE_10" is the image of "TABLE 8".",	10.0,	12
6,	"BERT-MRC+FL",	"IMAGE_6",	"BERT-MRC+FL是从image_6中提取的实体。",	10.0,	12
7,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
8,	"IMAGE_6",	"TABLE 3",	"IMAGE_6" is the image of "TABLE 3".",	10.0,	10
9,	"BERT-MRC (LI ET AL., 2019)",	"IMAGE_6",	"BERT-MRC (Li et al., 2019)是从image_6中提取的实体。",	10.0,	10
10,	"CVT (CLARK ET AL., 2018)",	"IMAGE_6",	"CVT (Clark et al., 2018)是从image_6中提取的实体。",	10.0,	10
11,	"IMAGE_11",	"TABLE 8",	"IMAGE_11" is the image of "TABLE 8".",	10.0,	9
12,	"BERT-MRC+DSC",	"CHINESE ONTONOTES 4.0",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
13,	"BERT-MRC (LI ET AL., 2019)",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
14,	"CVT (CLARK ET AL., 2018)",	"ENGLISH ONTONOTES 5.0",	"The CVT model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
15,	"TABLE 7",	"XLNet",	"XLNet's performance is evaluated in Table 7 for the PI task, also showing a performance boost with DSC as the training objective.",	7.0,	9
16,	"IMAGE_1",	"ONTONOTES5.0",	"OntoNotes5.0 NER是从image_1中提取的实体。",	10.0,	8
17,	"CHINESE ONTO4.0",	"Performance Metrics Table",	"The table contains performance metrics for the Chinese Onto4.0 dataset.",	9.0,	7
18,	"IMAGE_7",	"TABLE 5",	"IMAGE_7" is the image of "TABLE 5".",	10.0,	6
19,	"CHINESE MSRA",	"IMAGE_7",	"Chinese MSRA是从image_7中提取的实体。",	10.0,	6
20,	"CHINESE ONTONOTES 4.0",	"IMAGE_7",	"Chinese OntoNotes 4.0是从image_7中提取的实体。",	10.0,	6
21,	"ENGLISH QUOREF",	"Performance Metrics Table",	"The table contains performance metrics for the English QuoRef dataset.",	9.0,	6
22,	"IMAGE_12",	"Performance Metrics Table",	"Table是从image_12中提取的实体。",	10.0,	5
23,	"CONLL03",	"ONTONOTES5.0",	"Both are named entity recognition tasks but OntoNotes5.0 has a larger dataset size and higher ratio of negative to positive samples.",	7.0,	4
24,	"IMAGE_12",	"TABLE 10",	"IMAGE_12" is the image of "TABLE 10".",	10.0,	3
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
1,	"a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019).  

Baselines We used the following baselines:  

QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions.   
BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction.   
XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that  

  

Table 6: Experimental results for MRC task.   

  
Table 7: Experimental results for PI task.  

enables learning bidirectional contexts.  

Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1.  

# 4.4 Paraphrase Identification  

Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning or not. We conduct experiments on the two widely-used datasets: MRPC (Dolan and Brockett, 2005) and QQP. F1 score is reported for comparison. We use BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) as baselines.  

Results Table 7 shows the results. We find that replacing the training objective with DSC introduces performance boost for both settings, $+0.58$ for MRPC and $+0.73$ for QQP.  

# 5 Ablation Studies  

# 5.1 Datasets imbalanced to different extents  

It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents. We use the paraphrase identification dataset QQP ( $37\%$ positive and $63\%$ negative) for studies. To construct datasets with different imbalance degrees, we used the original QQP dataset to construct synthetic training sets with different positive-negative ratios. Models are trained on these different synthetic sets and then test on the same original test set.  

Original training set (original) The original dataset with 363,871 examples, with $37\%$ being positive and $63\%$ being negative Positive augmentation ( $\downarrow$ positive) We created a balanced dataset by adding positive examples. We first randomly chose positive training examples in the original training set as templates. Then we used Spacy to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia. The augmented set contains 458,477 examples, with $50\%$ being positive and $50\%$ being negative. Negative augmentation ( $\mp$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\%$ being positive and $79\%$ being negative.  

  

# Negative downsampling (- negative)  

We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\%$ being positive and $50\%$ being negative. Positive and negative augmentation ( $\mathbf{\Psi}+$ positive $\pmb{\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\%$ being positive and $50\%$ being negative.  

Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.   

  

Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that $^+$ positive outperforms original, and +negative underperforms original. This is in line with our expectation since $^+$ positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances.  

DSC achieves the highest F1 score across all datasets. Specially, for $^+$ positive, DSC achieves minor improvements $\left(+0.05\,\mathrm{F}1\right)$ ) over DL. In contrast, it significantly outperforms DL for +negative dataset. This is in line with our expectation since DSC helps more on more imbalanced datasets. The performance of FL and DL are not consistent across different datasets, while DSC consistently performs the best on all datasets.  

# 5.2 Dice loss for accuracy-oriented tasks?  

We argue that the cross-entropy objective is actually accuracy-oriented, whereas the"
2,	"# Dice Loss for Data-imbalanced NLP Tasks  

# Xiaoya $\mathrm{Li^{2}}$ , Xiaofei $\mathbf{Sun^{\pmb{\ast}}}$ , Yuxian Meng♣, Junjun Liang♣, Fei $\mathbf{W}\mathbf{u}^{\star}$ and Jiwei $\mathrm{Li}^{\omega\bullet}$  

♠Department of Computer Science and Technology, Zhejiang University ♣Shannon.AI xiaoya li, xiaofei sun, yuxian meng, jiwei li @shannonai.com, wufei $@$ cs.zju.edu.cn  

# Abstract  

Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.  

In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977), which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.  

With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP.  

  

Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.  

# 1 Introduction  

Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016; Nguyen et al., 2016; Rajpurkar et al., 2018; Koˇcisk\`y et al., 2018; Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background.  

Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label. This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, whereas at test time, F1 gives equal weight to positive and negative examples; (2) the overwhelming effect of easy-negative examples. As pointed out by Meng et al. (2019), a significantly large number of negative examples also means that the number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks (Lample et al., 2016; Wu et al., 2019; Devlin et al., 2018; Yu et al., 2018a; McCann et al., 2018; Ma and Hovy, 2016; Chen et al., 2017), handles neither of the issues.  

To handle the first issue, we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The Sørensen–Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the $F_{\beta}$ score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.  

Only using dice loss or Tversky index is not enough since they are unable to address the dominating"
    ```
    
The highest F1 score achieved on the Chinese OntoNotes 4.0 dataset, according to the provided data, is 84.47. This score was achieved by the BERT-MRC+DSC model.
mm_response:
['The highest F1 score achieved on the Chinese OntoNotes4.0 dataset, according to Table 10, is 84.67 at α = 0.6.', 'The highest F1 score achieved on the Chinese OntoNotes4.0 dataset, according to Table 10, is 84.47 by BERT-MRC+DSC.', 'The highest F1 score achieved on the Chinese OntoNotes4.0 dataset is 84.67, according to IMAGE_12.']
merged_mm_response:
Analyzing the Responses:
- The first response states that the highest F1 score is 84.67 at α = 0.6.
- The second response states that the highest F1 score is 84.47 by BERT-MRC+DSC.
- The third response states that the highest F1 score is 84.67, according to IMAGE_12.

Reasonably Determine:
There is a contradiction between the first and second responses regarding the highest F1 score. The first and third responses agree on the score of 84.67, but the second response provides a different score of 84.47. Since the user specifically asked for the information according to Table 10, and the first and third responses both mention Table 10, these responses are more relevant. The third response, however, does not mention α = 0.6, which is a detail provided in the first response. Given that the user's query did not specify α, the detail about α is not necessary for answering the user's question.

Provide a Unified Answer:
The highest F1 score achieved on the Chinese OntoNotes4.0 dataset, according to Table 10, is 84.67.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"TABLE 8",	"EVENT",	"Table 8 illustrates the effect of different data augmentation methods on the QQP dataset in terms of F1-score.",	2
1,	"IMAGE_11",	"ORI_IMG",	"The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' The table is structured with two main columns: SST-2 and SST-5, each representing different datasets. Under each dataset, there are two sub-columns labeled 'Acc' for accuracy. The rows represent different models used for data augmentation: BERT+CE, BERT+DL, and BERT+DSC. The values in the table are as follows: for SST-2, BERT+CE has an accuracy of 94.90%, BERT+DL has 94.37%, and BERT+DSC has 94.84%. For SST-5, BERT+CE has an accuracy of 55.57%, BERT+DL has 54.63%, and BERT+DSC has 55.19%. The table highlights the performance of different models across the two datasets, with BERT+CE achieving the highest accuracy on both SST-2 and SST-5.",	7
2,	"BERT-MRC+FL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.13, recall of 93.09, and F1 score of 93.11, showing an improvement of +0.06 over BERT-MRC.",	4
3,	"IMAGE_10",	"ORI_IMG",	"The image is a table that presents the performance of different models on a dataset with various data augmentation strategies. The table has six columns: 'original', '+ positive', '+ negative', '- negative', and '+ positive & negative'. Each row represents a different model or model configuration. The first row shows the results for the BERT model, with the original accuracy being 91.3%. The subsequent rows show the results for BERT combined with different techniques: FL (Focal Loss), DL (Dynamic Loss), and DSC (Dynamic Sample Contribution). For each model, the table provides the accuracy for the original dataset and the datasets augmented with positive examples, negative examples, down-sampled negative examples, and both positive and negative examples. The values are presented in percentages, with the changes in accuracy relative to the original model shown in parentheses. For example, BERT+FL has an accuracy of 91.86% (+0.56) on the original dataset, 92.64% (+0.37) on the + positive dataset, 90.61% (+0.53) on the + negative dataset, 90.79% (+1.06) on the - negative dataset, and 93.45% (+0.31) on the + positive & negative dataset.",	10
4,	"BERT-MRC+DSC",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.41, recall of 93.25, and F1 score of 93.33, showing the highest improvement of +0.29 over BERT-MRC.",	7
5,	"IMAGE_5",	"ORI_IMG",	"The image is a table labeled 'English CoNLL 2003' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores are provided for each model. Notably, BERT-MRC+DSC achieves the highest F1 score of 93.33, which is an improvement of +0.29 over BERT-MRC. The table highlights the superior performance of the DSC loss in enhancing the F1 score compared to other losses.",	9
6,	"IMAGE_6",	"ORI_IMG",	"The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision values range from 90.01 to 92.70, the recall values range from 88.35 to 92.56, and the F1 scores range from 88.8 to 92.07. Notably, BERT-MRC+DSC achieves the highest F1 score of 92.07, which is an improvement of +0.96 over BERT-MRC.",	8
7,	"IMAGE_8",	"ORI_IMG",	"The image is a table that presents the experimental results for the machine reading comprehension (MRC) task using different models and datasets. The table is divided into rows and columns. The rows represent different models: QANet, BERT, BERT+FL, BERT+DL, BERT+DSC, XLNet, XLNet+FL, XLNet+DL, and XLNet+DSC. The columns are labeled with the datasets SQuAD v1.1, SQuAD v2.0, and QuoRef, each further divided into EM (Exact Match) and F1 scores. For example, QANet scores 73.6 on EM and 82.7 on F1 for SQuAD v1.1, while BERT scores 84.1 on EM and 90.9 on F1 for the same dataset. The table also shows the performance improvements of the proposed methods over the baseline models. For instance, BERT+DSC achieves 85.34 on EM and 91.97 on F1 for SQuAD v1.1, which is an improvement of +1.24 and +1.07 respectively over BERT. Similarly, XLNet+DSC achieves 89.79 on EM and 95.77 on F1 for SQuAD v1.1, which is an improvement of +0.84 and +1.25 respectively over XLNet. The table highlights the significant performance boost obtained by the proposed DSC loss method on both EM and F1 scores across all datasets.",	10
8,	"BERT-MRC (LI ET AL., 2019)",	"ORGANIZATION",	"A model developed by Li et al. in 2019, achieving a precision of 92.98, recall of 89.95, and an F1 score of 91.11 on the English OntoNotes 5.0 dataset.",	2
9,	"BERT-MRC+DL",	"ORGANIZATION",	"An enhanced version of BERT-MRC with a precision of 93.22, recall of 93.12, and F1 score of 93.17, showing an improvement of +0.12 over BERT-MRC.",	4
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"IMAGE_10",	"BERT是从image_10中提取的实体。",	10.0,	31
1,	"BERT+CE",	"IMAGE_8",	"BERT是从image_8中提取的实体。",	10.0,	31
2,	"BERT+CE",	"IMAGE_11",	"BERT+CE是从image_11中提取的实体。",	10.0,	28
3,	"BERT+DL",	"IMAGE_10",	"BERT+DL是从image_10中提取的实体。",	10.0,	26
4,	"BERT+DSC",	"IMAGE_8",	"BERT+DSC是从image_8中提取的实体。",	10.0,	26
5,	"BERT+DL",	"IMAGE_8",	"BERT+DL是从image_8中提取的实体。",	10.0,	26
6,	"BERT+DSC",	"IMAGE_10",	"BERT+DSC是从image_10中提取的实体。",	10.0,	26
7,	"BERT+FL",	"IMAGE_10",	"BERT+FL是从image_10中提取的实体。",	10.0,	25
8,	"BERT+FL",	"IMAGE_8",	"BERT+FL是从image_8中提取的实体。",	10.0,	25
9,	"BERT+DSC",	"IMAGE_11",	"BERT+DSC是从image_11中提取的实体。",	10.0,	23
10,	"BERT+DL",	"IMAGE_11",	"BERT+DL是从image_11中提取的实体。",	10.0,	23
11,	"IMAGE_8",	"XLNet",	"XLNet是从image_8中提取的实体。",	10.0,	17
12,	"ENGLISH CONLL 2003",	"IMAGE_5",	"English CoNLL 2003是从image_5中提取的实体。",	10.0,	17
13,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"IMAGE_6",	"BERT-Tagger (Devlin et al., 2018)是从image_6中提取的实体。",	10.0,	17
14,	"+POSITIVE",	"IMAGE_10",	"+ positive是从image_10中提取的实体。",	10.0,	16
15,	"IMAGE_10",	"ORIGINAL",	"original是从image_10中提取的实体。",	10.0,	16
16,	"BERT-MRC+DSC",	"IMAGE_5",	"BERT-MRC+DSC是从image_5中提取的实体。",	10.0,	16
17,	"+POSITIVE & NEGATIVE",	"IMAGE_10",	"+ positive & negative是从image_10中提取的实体。",	10.0,	15
18,	"-NEGATIVE",	"IMAGE_10",	"- negative是从image_10中提取的实体。",	10.0,	15
19,	"ENGLISH ONTONOTES 5.0",	"IMAGE_6",	"English OntoNotes 5.0是从image_6中提取的实体。",	10.0,	15
20,	"+NEGATIVE",	"IMAGE_10",	"+ negative是从image_10中提取的实体。",	10.0,	15
21,	"BERT-MRC+DSC",	"IMAGE_6",	"BERT-MRC+DSC是从image_6中提取的实体。",	10.0,	15
22,	"BERT-MRC+DSC",	"ENGLISH CONLL 2003",	"BERT-MRC+DSC was evaluated in the English CoNLL 2003 event.",	8.0,	15
23,	"BERT-MRC+DSC",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DSC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	14
24,	"BERT-MRC+DL",	"IMAGE_5",	"BERT-MRC+DL是从image_5中提取的实体。",	10.0,	13
25,	"IMAGE_8",	"MRC TASK",	"IMAGE_8" is the image of "MRC TASK".",	10.0,	13
26,	"IMAGE_8",	"XLNET+DL",	"XLNet+DL是从image_8中提取的实体。",	10.0,	13
27,	"IMAGE_8",	"XLNET+FL",	"XLNet+FL是从image_8中提取的实体。",	10.0,	13
28,	"IMAGE_8",	"XLNET+DSC",	"XLNet+DSC是从image_8中提取的实体。",	10.0,	13
29,	"BERT-MRC+FL",	"IMAGE_5",	"BERT-MRC+FL是从image_5中提取的实体。",	10.0,	13
30,	"BERT-MRC+DL",	"IMAGE_6",	"BERT-MRC+DL是从image_6中提取的实体。",	10.0,	12
31,	"BERT-MRC+FL",	"IMAGE_6",	"BERT-MRC+FL是从image_6中提取的实体。",	10.0,	12
32,	"IMAGE_10",	"TABLE 8",	"IMAGE_10" is the image of "TABLE 8".",	10.0,	12
33,	"IMAGE_8",	"QANET",	"QANet是从image_8中提取的实体。",	10.0,	12
34,	"BERT-MRC+DL",	"ENGLISH CONLL 2003",	"BERT-MRC+DL was evaluated in the English CoNLL 2003 event.",	8.0,	12
35,	"BERT-MRC+FL",	"ENGLISH CONLL 2003",	"BERT-MRC+FL was evaluated in the English CoNLL 2003 event.",	8.0,	12
36,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
37,	"IMAGE_11",	"SST-2",	"SST-2是从image_11中提取的实体。",	10.0,	11
38,	"ELMO(PETERS ET AL., 2018)",	"IMAGE_5",	"ELMo(Peters et al., 2018)是从image_5中提取的实体。",	10.0,	11
39,	"BERT-TAGGER (Devlin et al., 2018)",	"IMAGE_5",	"BERT-Tagger(Devlin et al., 2018)是从image_5中提取的实体。",	10.0,	11
40,	"BERT-MRC(LI ET AL., 2019)",	"IMAGE_5",	"BERT-MRC(Li et al., 2019)是从image_5中提取的实体。",	10.0,	11
41,	"IMAGE_11",	"SST-5",	"SST-5是从image_11中提取的实体。",	10.0,	11
42,	"CVT(CLARK ET AL., 2018)",	"IMAGE_5",	"CVT(Clark et al., 2018)是从image_5中提取的实体。",	10.0,	11
43,	"IMAGE_5",	"TABLE 5",	"IMAGE_5" is the image of "TABLE 5".",	10.0,	11
44,	"BERT-MRC+FL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+FL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
45,	"BERT-MRC+DL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
46,	"IMAGE_6",	"TABLE 3",	"IMAGE_6" is the image of "TABLE 3".",	10.0,	10
47,	"CVT (CLARK ET AL., 2018)",	"IMAGE_6",	"CVT (Clark et al., 2018)是从image_6中提取的实体。",	10.0,	10
48,	"BERT-MRC (LI ET AL., 2019)",	"IMAGE_6",	"BERT-MRC (Li et al., 2019)是从image_6中提取的实体。",	10.0,	10
49,	"IMAGE_11",	"TABLE",	"Table是从image_11中提取的实体。",	10.0,	9
50,	"IMAGE_11",	"TABLE 8",	"IMAGE_11" is the image of "TABLE 8".",	10.0,	9
51,	"BERT-MRC+DSC",	"CHINESE ONTONOTES 4.0",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
52,	"BERT-MRC+DSC",	"CHINESE MSRA",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
53,	"BERT-MRC (LI ET AL., 2019)",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019).  

Baselines We used the following baselines:  

QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions.   
BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction.   
XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that  

  

Table 6: Experimental results for MRC task.   

  
Table 7: Experimental results for PI task.  

enables learning bidirectional contexts.  

Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1.  

# 4.4 Paraphrase Identification  

Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning or not. We conduct experiments on the two widely-used datasets: MRPC (Dolan and Brockett, 2005) and QQP. F1 score is reported for comparison. We use BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) as baselines.  

Results Table 7 shows the results. We find that replacing the training objective with DSC introduces performance boost for both settings, $+0.58$ for MRPC and $+0.73$ for QQP.  

# 5 Ablation Studies  

# 5.1 Datasets imbalanced to different extents  

It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents. We use the paraphrase identification dataset QQP ( $37\%$ positive and $63\%$ negative) for studies. To construct datasets with different imbalance degrees, we used the original QQP dataset to construct synthetic training sets with different positive-negative ratios. Models are trained on these different synthetic sets and then test on the same original test set.  

Original training set (original) The original dataset with 363,871 examples, with $37\%$ being positive and $63\%$ being negative Positive augmentation ( $\downarrow$ positive) We created a balanced dataset by adding positive examples. We first randomly chose positive training examples in the original training set as templates. Then we used Spacy to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia. The augmented set contains 458,477 examples, with $50\%$ being positive and $50\%$ being negative. Negative augmentation ( $\mp$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\%$ being positive and $79\%$ being negative.  

  

# Negative downsampling (- negative)  

We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\%$ being positive and $50\%$ being negative. Positive and negative augmentation ( $\mathbf{\Psi}+$ positive $\pmb{\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\%$ being positive and $50\%$ being negative.  

Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.   

  

Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that $^+$ positive outperforms original, and +negative underperforms original. This is in line with our expectation since $^+$ positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances.  

DSC achieves the highest F1 score across all datasets. Specially, for $^+$ positive, DSC achieves minor improvements $\left(+0.05\,\mathrm{F}1\right)$ ) over DL. In contrast, it significantly outperforms DL for +negative dataset. This is in line with our expectation since DSC helps more on more imbalanced datasets. The performance of FL and DL are not consistent across different datasets, while DSC consistently performs the best on all datasets.  

# 5.2 Dice loss for accuracy-oriented tasks?  

We argue that the cross-entropy objective is actually accuracy-oriented, whereas the"
    ```
    
Based on Table 8, the data augmentation method that resulted in the highest F1-score for the BERT model is BERT+CE (Cross-Entropy). This is evident from the information in Image_11, which states that BERT+CE achieved the highest accuracy on both SST-2 and SST-5 datasets, with 94.90% and 55.57% respectively.
mm_response:
['According to Table 8, the data augmentation method that resulted in the highest F1-score for the BERT model is BERT+DSC.', 'The data augmentation method that resulted in the highest F1-score for the BERT model according to Table 8 is "+ positive & negative," with an F1-score of 93.14.', 'According to Table 8, the data augmentation method that resulted in the highest F1-score for the BERT model is "+ positive & negative," achieving an F1-score of 93.45%.']
merged_mm_response:
Analyzing the Responses:
- The first response states that the highest F1-score for the BERT model is achieved with the BERT+DSC method.
- The second response mentions "+ positive & negative" as the method with the highest F1-score, with a specific value of 93.14.
- The third response also mentions "+ positive & negative" as the method with the highest F1-score, but with a different value of 93.45%.

There are contradictions in the responses regarding the specific data augmentation method and the F1-score values. The first response is inconsistent with the other two, which both mention "+ positive & negative" as the method with the highest F1-score.

Reasonably Determine:
Based on the information provided, the second and third responses are more consistent with each other, despite the slight discrepancy in the F1-score values. Both responses mention "+ positive & negative" as the method resulting in the highest F1-score for the BERT model according to Table 8. Therefore, the response that best aligns with the user query is the one that mentions "+ positive & negative" as the data augmentation method.

Provide a Unified Answer:
The data augmentation method that resulted in the highest F1-score for the BERT model according to Table 8 is "+ positive & negative." The exact F1-score value is not consistent across the responses, but it is clear that this method achieved the highest score.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"IMAGE_8",	"ORI_IMG",	"The image is a table that presents the experimental results for the machine reading comprehension (MRC) task using different models and datasets. The table is divided into rows and columns. The rows represent different models: QANet, BERT, BERT+FL, BERT+DL, BERT+DSC, XLNet, XLNet+FL, XLNet+DL, and XLNet+DSC. The columns are labeled with the datasets SQuAD v1.1, SQuAD v2.0, and QuoRef, each further divided into EM (Exact Match) and F1 scores. For example, QANet scores 73.6 on EM and 82.7 on F1 for SQuAD v1.1, while BERT scores 84.1 on EM and 90.9 on F1 for the same dataset. The table also shows the performance improvements of the proposed methods over the baseline models. For instance, BERT+DSC achieves 85.34 on EM and 91.97 on F1 for SQuAD v1.1, which is an improvement of +1.24 and +1.07 respectively over BERT. Similarly, XLNet+DSC achieves 89.79 on EM and 95.77 on F1 for SQuAD v1.1, which is an improvement of +0.84 and +1.25 respectively over XLNet. The table highlights the significant performance boost obtained by the proposed DSC loss method on both EM and F1 scores across all datasets.",	10
1,	"CVT (CLARK ET AL., 2018)",	"ORGANIZATION",	"A model developed by Clark et al. in 2018, achieving an F1 score of 88.8 on the English OntoNotes 5.0 dataset.",	2
2,	"F1",	"UNKNOWN",	"The F1 score is used to measure the performance of models, and Eq.5 is a soft form of F1 using a continuous probability p.",	1
3,	"Table 6: Experimental results for MRC task.",	"IMG_ENTITY",	"A table comparing the performance of different models on two datasets, MRPC and QQP, using the F1 score metric. The table includes variations of BERT and XLNet models with additional techniques (FL, DL, DSC) applied to them.",	1
4,	"IMAGE_9",	"ORI_IMG",	"The image is a table labeled 'Table 6: Experimental results for MRC task.' The table compares the performance of different models on two datasets, MRPC and QQP, using the F1 score metric. The rows represent different models and their variations, while the columns show the F1 scores for each dataset. The models listed are BERT (Devlin et al., 2018), BERT+FL, BERT+DL, BERT+DSC, XLNet (Yang et al., 2019), XLNet+FL, XLNet+DL, and XLNet+DSC. For the MRPC dataset, the F1 scores are as follows: BERT has an F1 score of 88.0, BERT+FL has 88.43 (+0.43), BERT+DL has 88.71 (+0.71), and BERT+DSC has 88.92 (+0.92). For the QQP dataset, the F1 scores are: BERT has an F1 score of 91.3, BERT+FL has 91.86 (+0.56), BERT+DL has 91.92 (+0.62), and BERT+DSC has 92.11 (+0.81). For XLNet, the F1 scores are: XLNet has an F1 score of 89.2 for MRPC and 91.8 for QQP, XLNet+FL has 89.25 (+0.05) for MRPC and 92.31 (+0.51) for QQP, XLNet+DL has 89.33 (+0.13) for MRPC and 92.39 (+0.59) for QQP, and XLNet+DSC has 89.78 (+0.58) for MRPC and 92.60 (+0.79) for QQP. The table highlights the performance improvements when additional techniques (FL, DL, DSC) are applied to both BERT and XLNet.",	9
5,	"IMAGE_5",	"ORI_IMG",	"The image is a table labeled 'English CoNLL 2003' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are ELMo (Peters et al., 2018), CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision, recall, and F1 scores are provided for each model. Notably, BERT-MRC+DSC achieves the highest F1 score of 93.33, which is an improvement of +0.29 over BERT-MRC. The table highlights the superior performance of the DSC loss in enhancing the F1 score compared to other losses.",	9
6,	"IMAGE_12",	"ORI_IMG",	"The image is a table that presents the effect of hyperparameters in the Tversky Index. The table has three columns: 'α', 'Chinese Onto4.0', and 'English QuoRef'. The first column lists values of α ranging from 0.1 to 0.9 in increments of 0.1. The second column shows corresponding F1 scores for the Chinese Onto4.0 dataset, with values ranging from 80.13 to 84.67. The third column displays F1 scores for the English QuoRef dataset, ranging from 63.23 to 68.44. Notably, the highest F1 score for Chinese Onto4.0 is 84.67 at α = 0.6, while for English QuoRef, the highest F1 score is 68.44 at α = 0.4.",	2
7,	"IMAGE_6",	"ORI_IMG",	"The image is a table labeled 'English OntoNotes 5.0' that provides performance metrics for various models on the named entity recognition (NER) task. The table is structured with four main columns: Model, Precision (Prec.), Recall (Rec.), and F1 score. Each row represents a different model and its corresponding performance metrics. The models listed are CVT (Clark et al., 2018), BERT-Tagger (Devlin et al., 2018), BERT-MRC (Li et al., 2019), BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The precision values range from 90.01 to 92.70, the recall values range from 88.35 to 92.56, and the F1 scores range from 88.8 to 92.07. Notably, BERT-MRC+DSC achieves the highest F1 score of 92.07, which is an improvement of +0.96 over BERT-MRC.",	8
8,	"IMAGE_7",	"ORI_IMG",	"The image is a table labeled 'Table 5: Experimental results for NER task.' The table is divided into two main sections, each representing different datasets: Chinese MSRA and Chinese OntoNotes 4.0. Each section contains rows listing various models used for Named Entity Recognition (NER) tasks and their corresponding performance metrics: Precision (Prec.), Recall (Rec.), and F1 score. For the Chinese MSRA dataset, the models listed are Lattice-LSTM, BERT-Tagger, Glyce-BERT, BERT-MRC, BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The highest F1 score is achieved by BERT-MRC+DSC with a value of 96.72. For the Chinese OntoNotes 4.0 dataset, the same models are listed, and the highest F1 score is also achieved by BERT-MRC+DSC with a value of 84.47. The table highlights the performance improvements of the DSC loss over other models, with specific gains indicated in parentheses next to the F1 scores.",	4
9,	"QUOREF",	"EVENT",	"Quoref is a QA dataset that tests the coreferential reasoning capability of reading comprehension systems."<SEP>"Quoref is a dataset used for machine reading comprehension tasks."<SEP>"Quoref is a dataset used in the task of question answering, mentioned in the context of evaluating model performance.",	4
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT+CE",	"IMAGE_8",	"BERT是从image_8中提取的实体。",	10.0,	31
1,	"BERT+CE",	"IMAGE_9",	"BERT (Devlin et al., 2018)是从image_9中提取的实体。",	10.0,	30
2,	"BERT+DSC",	"IMAGE_8",	"BERT+DSC是从image_8中提取的实体。",	10.0,	26
3,	"BERT+DL",	"IMAGE_8",	"BERT+DL是从image_8中提取的实体。",	10.0,	26
4,	"BERT+FL",	"IMAGE_8",	"BERT+FL是从image_8中提取的实体。",	10.0,	25
5,	"BERT+DSC",	"IMAGE_9",	"BERT+DSC是从image_9中提取的实体。",	10.0,	25
6,	"BERT+DL",	"IMAGE_9",	"BERT+DL是从image_9中提取的实体。",	10.0,	25
7,	"BERT+FL",	"IMAGE_9",	"BERT+FL是从image_9中提取的实体。",	10.0,	24
8,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"IMAGE_6",	"BERT-Tagger (Devlin et al., 2018)是从image_6中提取的实体。",	10.0,	17
9,	"IMAGE_8",	"XLNet",	"XLNet是从image_8中提取的实体。",	10.0,	17
10,	"ENGLISH CONLL 2003",	"IMAGE_5",	"English CoNLL 2003是从image_5中提取的实体。",	10.0,	17
11,	"BERT-MRC+DSC",	"IMAGE_5",	"BERT-MRC+DSC是从image_5中提取的实体。",	10.0,	16
12,	"BERT-MRC+DSC",	"IMAGE_6",	"BERT-MRC+DSC是从image_6中提取的实体。",	10.0,	15
13,	"ENGLISH ONTONOTES 5.0",	"IMAGE_6",	"English OntoNotes 5.0是从image_6中提取的实体。",	10.0,	15
14,	"IMAGE_8",	"XLNET+DSC",	"XLNet+DSC是从image_8中提取的实体。",	10.0,	13
15,	"IMAGE_8",	"XLNET+FL",	"XLNet+FL是从image_8中提取的实体。",	10.0,	13
16,	"BERT-MRC+FL",	"IMAGE_5",	"BERT-MRC+FL是从image_5中提取的实体。",	10.0,	13
17,	"BERT-MRC+DL",	"IMAGE_5",	"BERT-MRC+DL是从image_5中提取的实体。",	10.0,	13
18,	"IMAGE_8",	"XLNET+DL",	"XLNet+DL是从image_8中提取的实体。",	10.0,	13
19,	"IMAGE_8",	"MRC TASK",	"IMAGE_8" is the image of "MRC TASK".",	10.0,	13
20,	"IMAGE_9",	"XLNET+DL",	"XLNet+DL是从image_9中提取的实体。",	10.0,	12
21,	"IMAGE_9",	"XLNET+DSC",	"XLNet+DSC是从image_9中提取的实体。",	10.0,	12
22,	"BERT-MRC+FL",	"IMAGE_6",	"BERT-MRC+FL是从image_6中提取的实体。",	10.0,	12
23,	"BERT-MRC+DL",	"IMAGE_6",	"BERT-MRC+DL是从image_6中提取的实体。",	10.0,	12
24,	"IMAGE_8",	"QANET",	"QANet是从image_8中提取的实体。",	10.0,	12
25,	"IMAGE_9",	"XLNET+FL",	"XLNet+FL是从image_9中提取的实体。",	10.0,	12
26,	"CVT(CLARK ET AL., 2018)",	"IMAGE_5",	"CVT(Clark et al., 2018)是从image_5中提取的实体。",	10.0,	11
27,	"BERT-TAGGER (Devlin et al., 2018)",	"IMAGE_5",	"BERT-Tagger(Devlin et al., 2018)是从image_5中提取的实体。",	10.0,	11
28,	"IMAGE_5",	"TABLE 5",	"IMAGE_5" is the image of "TABLE 5".",	10.0,	11
29,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
30,	"BERT-MRC(LI ET AL., 2019)",	"IMAGE_5",	"BERT-MRC(Li et al., 2019)是从image_5中提取的实体。",	10.0,	11
31,	"ELMO(PETERS ET AL., 2018)",	"IMAGE_5",	"ELMo(Peters et al., 2018)是从image_5中提取的实体。",	10.0,	11
32,	"BERT-MRC (LI ET AL., 2019)",	"IMAGE_6",	"BERT-MRC (Li et al., 2019)是从image_6中提取的实体。",	10.0,	10
33,	"IMAGE_6",	"TABLE 3",	"IMAGE_6" is the image of "TABLE 3".",	10.0,	10
34,	"IMAGE_9",	"Table 6: Experimental results for MRC task.",	"IMAGE_9" is the image of Table 6: Experimental results for MRC task..",	10.0,	10
35,	"IMAGE_1",	"QUOREF",	"QUOREF是从image_1中提取的实体。",	10.0,	10
36,	"IMAGE_9",	"XLNET (YANG ET AL., 2019)",	"XLNet (Yang et al., 2019)是从image_9中提取的实体。",	10.0,	10
37,	"CVT (CLARK ET AL., 2018)",	"IMAGE_6",	"CVT (Clark et al., 2018)是从image_6中提取的实体。",	10.0,	10
38,	"CVT (CLARK ET AL., 2018)",	"ENGLISH ONTONOTES 5.0",	"The CVT model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
39,	"QUOREF",	"SQUAD 2.0",	"Both are natural language processing tasks but QUOREF focuses on coreference resolution while SQuAD 2.0 is a question-answering task.",	6.0,	7
40,	"QUOREF",	"SQUAD 1.1",	"Both are natural language processing tasks but QUOREF focuses on coreference resolution while SQuAD 1.1 is a question-answering task.",	6.0,	7
41,	"CHINESE ONTONOTES 4.0",	"IMAGE_7",	"Chinese OntoNotes 4.0是从image_7中提取的实体。",	10.0,	6
42,	"CHINESE MSRA",	"IMAGE_7",	"Chinese MSRA是从image_7中提取的实体。",	10.0,	6
43,	"IMAGE_7",	"TABLE 5",	"IMAGE_7" is the image of "TABLE 5".",	10.0,	6
44,	"IMAGE_12",	"Performance Metrics Table",	"Table是从image_12中提取的实体。",	10.0,	5
45,	"QUOREF",	"SQUAD",	"SQuAD and Quoref are both QA datasets used for machine reading comprehension.",	6.0,	5
46,	"IMAGE_12",	"TABLE 10",	"IMAGE_12" is the image of "TABLE 10".",	10.0,	3
47,	"F1 SCORE",	"F1",	"The F1 score is used to measure the performance of models, and Eq.5 is a soft form of F1 using a continuous probability p.",	6.0,	3
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"i})=\frac{2p_{i1}y_{i1}+\gamma}{p_{i1}+y_{i1}+\gamma}
$$  

As can be seen, negative examples whose DSC is $\frac{\gamma}{p_{i1}\!+\!\gamma}$ , also contribute to the training. Additionally, Milletari et al. (2016) proposed to change the denominator to the square form for faster convergence, which leads to the following dice loss (DL):  

$$
\mathrm{DL}=\frac{1}{N}\sum_{i}\left[1-\frac{2p_{i1}y_{i1}+\gamma}{p_{i1}^{2}+y_{i1}^{2}+\gamma}\right]
$$  

Another version of DL is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient, which is easier for optimization:  

$$
\mathrm{DL}=1-\frac{2\sum_{i}p_{i1}y_{i1}+\gamma}{\sum_{i}p_{i1}^{2}+\sum_{i}y_{i1}^{2}+\gamma}
$$  

Tversky index (TI), which can be thought as the approximation of the $F_{\beta}$ score, extends dice coeffi- cient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows:  

$$
\mathrm{TI}=\frac{|A\cap B|}{|A\cap B|+\alpha|A\backslash B|+\beta|B\backslash A|}
$$  

Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\alpha=\beta=0.5$ The Tversky loss (TL) is thus given as follows:  

$$
\mathrm{TL}=\frac{1}{N}\sum_{i}\left[1-\frac{p_{i1}y_{i1}+\gamma}{p_{i1}y_{i1}+\alpha\,p_{i1}y_{i0}+\beta\,p_{i0}y_{i1}+\gamma}\right]
$$  

# 3.4 Self-adjusting Dice Loss  

Consider a simple case where the dataset consists of only one example $x_{i}$ , which is classified as positive as long as $p_{i1}$ is larger than 0.5. The computation of $F1$ score is actually as follows:  

$$
\mathrm{F}1(x_{i})=2\;{\frac{\mathbb{I}(p_{i1}>0.5)y_{i1}}{\mathbb{I}(p_{i1}>0.5)+y_{i1}}}
$$  

  
Figure 1: An illustration of derivatives of the four losses. The derivative of DSC approaches zero right after $p$ exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F1$ , using a continuous $p$ rather than the binary $\mathbb{I}(p_{i1}>0.5)$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance.  

To address this issue, we propose to multiply the soft probability $p$ with a decaying factor $(1-p)$ changing Eq.11 to the following adaptive variant of DSC:  

$$
\operatorname{DSC}(x_{i})={\frac{2(1-p_{i1})p_{i1}\cdot y_{i1}+\gamma}{(1-p_{i1})p_{i1}+y_{i1}+\gamma}}
$$  

One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them.  

A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\gamma}$ factor, leading the final loss to be $-(1-p)^{\gamma}\log p$ .  

  
Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only"
1,	"lattice lstm. arXiv preprint arXiv:1805.02023.  

# A Dataset Details  

# A.1 Part-of-Speech Tagging  

Datasets We conduct experiments on three widely used benchmark, i.e., Chinese Treebank $5.0^{2}/6.0^{3}$ and $\mathrm{UD1.4^{4}}$ .  

CTB5 is a Chinese dataset for tagging and parsing, which contains 507,222 words, 824,983 characters and 18,782 sentences extracted from newswire sources, including 698 articles from Xinhua (1994-1998), 55 articles from Information Services Department of HKSAR (1997) and 132 articles from Sinorama Magazine (1996-1998 & 2000-2001).  

CTB6 is an extension of CTB5, containing 781,351 words, 1,285,149 characters and 28,295 sentences.  

UD is the abbreviation of Universal Dependencies, which is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. In this work, we use UD1.4 for Chinese POS tagging.  

# A.2 Named Entity Recognition  

Datasets For the NER task, we consider both Chinese datasets, i.e., OntoNotes $.4.0^{5}$ and MSRA6 , and English datasets, i.e., CoNLL2003  and OntoNote $\div5.0^{8}$ .  

CoNLL2003 is an English dataset with 4 entity types: Location, Organization, Person and Miscellaneous. We followed data processing protocols in (Ma and Hovy, 2016).  

English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types. We use the standard train/dev/test split of CoNLL2012 shared task.  

Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Since the development set is not provided in the original MSRA dataset, we randomly split the training set into training and development splits by 9:1. We use the official test set for evaluation.  

Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. In this paper, we take the same data split as Wu et al. (2019) did.  

# A.3 Machine Reading Comprephension  

Datasets For MRC task, we use three datasets: $\mathrm{SQuADv1.1/v2.0^{9}}$ and Queref  datasets.  

SQuAD v1.1 and SQuAD $\mathbf{v}2.\mathbf{0}$ are the most widely used QA benchmarks. SQuAD1.1 is a collection of 100K crowdsourced question-answer pairs, and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage.  

Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems, containing 24K questions over 4.7K paragraphs from Wikipedia.  

# A.4 Paraphrase Identification  

Datasets Experiments are conducted on two PI datasets: $\mathbf{M}\mathbf{R}\mathbf{P}\mathbf{C}^{11}$ and $\mathsf{Q Q P}^{12}$ .  

MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations of whether the sentence pairs are semantically equivalent. The MRPC dataset has imbalanced classes (6800 pairs in total, and $68\%$ for positive, $32\%$ for negative).  

QQP is a collection of question pairs from the community question-answering website Quora. The class distribution in QQP is also unbalanced (over 400,000 question pairs in total, and $37\%$ for positive, $63\%$ for negative)."
2,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
    ```
    
The model variant that has the highest improvement in F1 score for the QuoRef dataset when compared to the base XLNet model is XLNet+DSC. According to the data from IMAGE_8, XLNet+DSC achieves an F1 score of 95.77 for QuoRef, which is an improvement of +1.25 over the base XLNet model that has an F1 score of 94.52.
mm_response:
['The model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset when compared to the base XLNet model, with an improvement of +1.41.', 'The model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset when compared to the base XLNet model, with an improvement of +0.79.', "The model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset when compared to the base XLNet model. The F1 score for XLNet+DSC is 95.77, which is an improvement of +1.25 over the base XLNet model's F1 score of 94.52."]
merged_mm_response:
Analyzing the Responses:
- The first response states that the model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset with an improvement of +1.41.
- The second response states that the model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset with an improvement of +0.79.
- The third response states that the model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset with an improvement of +1.25.

Reasonably Determine:
There is a contradiction between the first and second responses regarding the exact improvement in F1 score (+1.41 vs. +0.79). However, the third response provides a more detailed breakdown of the F1 scores, stating that the F1 score for XLNet+DSC is 95.77, which is an improvement of +1.25 over the base XLNet model's F1 score of 94.52. This response aligns with the first response in terms of the model variant being XLNet+DSC and provides a specific numerical improvement that is consistent with the first response.

Provide a Unified Answer:
The model variant XLNet+DSC has the highest improvement in F1 score for the QuoRef dataset when compared to the base XLNet model, with an improvement of +1.25. This information is consistent with the detailed breakdown provided in the third response and aligns with the first response, which also indicates a higher improvement than the second response.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"CHINESE ONTONOTES 4.0",	"EVENT",	"CHINESE ONTONOTES 4.0 is another dataset used for evaluating named entity recognition models, where BERT-MRC+DSC achieves a F1 score of 84.47. It is also mentioned as a dataset used for named entity recognition experiments, proposed by Pradhan et al. in 2011.",	2
1,	"ONTONOTES4.0",	"EVENT",	"OntoNotes4.0 is a dataset used in the paper for the named entity recognition task.",	0
2,	"ENGLISH ONTONOTES 5.0",	"EVENT",	"A dataset used for evaluating the performance of different models in natural language processing tasks, including named entity recognition.",	7
3,	"ONTONOTES5.0",	"EVENT",	"OntoNotes5.0 is a dataset used in the paper for the named entity recognition task. A named entity recognition task with 1.96M negative and 239K positive samples, resulting in a ratio of 8.18.",	2
4,	"ONTONOTES",	"DATASET",	"OntoNotes is a dataset used for Named Entity Recognition, consisting of texts from various sources and containing multiple entity types.",	2
5,	"CHINESE ONTO4.0",	"GEO",	"Chinese Onto4.0 is a dataset used for experiments to test the effect of hyperparameters in Tversky Index. The table contains performance metrics for the Chinese Onto4.0 dataset.",	4
6,	"IMAGE_7",	"ORI_IMG",	"The image is a table labeled 'Table 5: Experimental results for NER task.' The table is divided into two main sections, each representing different datasets: Chinese MSRA and Chinese OntoNotes 4.0. Each section contains rows listing various models used for Named Entity Recognition (NER) tasks and their corresponding performance metrics: Precision (Prec.), Recall (Rec.), and F1 score. For the Chinese MSRA dataset, the models listed are Lattice-LSTM, BERT-Tagger, Glyce-BERT, BERT-MRC, BERT-MRC+FL, BERT-MRC+DL, and BERT-MRC+DSC. The highest F1 score is achieved by BERT-MRC+DSC with a value of 96.72. For the Chinese OntoNotes 4.0 dataset, the same models are listed, and the highest F1 score is also achieved by BERT-MRC+DSC with a value of 84.47. The table highlights the performance improvements of the DSC loss over other models, with specific gains indicated in parentheses next to the F1 scores.",	4
7,	"CHINESE MSRA",	"EVENT",	"CHINESE MSRA is a dataset used for evaluating named entity recognition models, where BERT-MRC+DSC achieves a F1 score of 96.72. It is also mentioned as a dataset used for named entity recognition experiments, proposed by Levow in 2006.",	2
8,	"CONLL2003",	"EVENT",	"CoNLL2003 is an English dataset with 4 entity types used for Named Entity Recognition.",	2
9,	"CONLL2012",	"EVENT",	"CoNLL2012 is the shared task event that provided the standard train/dev/test split for the OntoNotes5.0 dataset.",	0
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT-TAGGER (DEVLIN ET AL., 2018)",	"ENGLISH ONTONOTES 5.0",	"The BERT-Tagger model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	16
1,	"ENGLISH ONTONOTES 5.0",	"IMAGE_6",	"English OntoNotes 5.0是从image_6中提取的实体。",	10.0,	15
2,	"BERT-MRC+DSC",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DSC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	14
3,	"BERT-MRC+DSC",	"IMAGE_7",	"BERT-MRC+DSC是从image_7中提取的实体。",	10.0,	11
4,	"BERT-MRC+FL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+FL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
5,	"BERT-MRC+DL",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC+DL model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	11
6,	"BERT-MRC+DSC",	"CHINESE MSRA",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
7,	"BERT-MRC+DSC",	"CHINESE ONTONOTES 4.0",	"The model is evaluated on this dataset and achieves the highest F1 score.",	9.0,	9
8,	"BERT-MRC (LI ET AL., 2019)",	"ENGLISH ONTONOTES 5.0",	"The BERT-MRC model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
9,	"CVT (CLARK ET AL., 2018)",	"ENGLISH ONTONOTES 5.0",	"The CVT model was evaluated on the English OntoNotes 5.0 dataset.",	8.0,	9
10,	"CHINESE ONTO4.0",	"TVERSKY INDEX",	"Tversky Index is used with the Chinese OntoNotes4.0 dataset to explore the effect of hyperparameters on performance.",	7.0,	9
11,	"IMAGE_1",	"ONTONOTES5.0",	"OntoNotes5.0 NER是从image_1中提取的实体。",	10.0,	8
12,	"CHINESE ONTO4.0",	"Performance Metrics Table",	"The table contains performance metrics for the Chinese Onto4.0 dataset.",	9.0,	7
13,	"CHINESE ONTONOTES 4.0",	"IMAGE_7",	"Chinese OntoNotes 4.0是从image_7中提取的实体。",	10.0,	6
14,	"CHINESE MSRA",	"IMAGE_7",	"Chinese MSRA是从image_7中提取的实体。",	10.0,	6
15,	"IMAGE_7",	"TABLE 5",	"IMAGE_7" is the image of "TABLE 5".",	10.0,	6
16,	"ALPHA VALUES",	"CHINESE ONTO4.0",	"Performance metrics for Chinese Onto4.0 vary with different alpha values.",	8.0,	6
17,	"CHINESE ONTO4.0",	"WU ET AL.",	"Chinese OntoNotes4.0 uses the same data split as Wu et al.",	6.0,	5
18,	"CONLL03",	"ONTONOTES5.0",	"Both are named entity recognition tasks but OntoNotes5.0 has a larger dataset size and higher ratio of negative to positive samples.",	7.0,	4
19,	"CONLL2003",	"ONTONOTES",	"CoNLL2003 and OntoNotes are both datasets used for Named Entity Recognition.",	6.0,	4
20,	"CONLL2003",	"MA AND HOVY, 2016",	"The data processing protocols for CoNLL2003 were followed as described in Ma and Hovy, 2016.",	6.0,	3
21,	"MSRA",	"ONTONOTES",	"MSRA is a Chinese benchmark dataset that is part of the OntoNotes collection.",	6.0,	3
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"results for Chinese POS datasets including CTB5, CTB6 and UD1.4.  

  
Table 4: Experimental results for English POS datasets.  

In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  

# 4 Experiments  

We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.  

# 4.1 Part-of-Speech Tagging  

Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) $5.0/6.0$ and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.  

# Baselines We used the following baselines:  

Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.  

Results Table 3 presents the experimental results on Chinese datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue.  

Table 4 presents the experimental results for English datasets.  

  

  

  
Table 5: Experimental results for NER task.  

# 4.2 Named Entity Recognition  

Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.  

Baselines We use the following baselines:  

ELMo: a tagging model with pretraining from Peters et al. (2018).   
Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice, only used in Chinese datasets.   
CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder.   
Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task.   
Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining.   
BERT-MRC: Li et al. (2019) formulates NER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.  

Results Table 5 shows experimental results on NER datasets. DSC outperforms BERT-MRC(Li et al., 2019) by $+0.29$ ,$+0.96$ ,$+0.97$ and $+2.36$ respectively on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.  

# 4.3 Machine Reading Comprehension  

Settings The task of machine reading comprehension (MRC) (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019)."
1,	"# Dice Loss for Data-imbalanced NLP Tasks  

# Xiaoya $\mathrm{Li^{2}}$ , Xiaofei $\mathbf{Sun^{\pmb{\ast}}}$ , Yuxian Meng♣, Junjun Liang♣, Fei $\mathbf{W}\mathbf{u}^{\star}$ and Jiwei $\mathrm{Li}^{\omega\bullet}$  

♠Department of Computer Science and Technology, Zhejiang University ♣Shannon.AI xiaoya li, xiaofei sun, yuxian meng, jiwei li @shannonai.com, wufei $@$ cs.zju.edu.cn  

# Abstract  

Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.  

In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977), which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.  

With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP.  

  

Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.  

# 1 Introduction  

Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016; Nguyen et al., 2016; Rajpurkar et al., 2018; Koˇcisk\`y et al., 2018; Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background.  

Data imbalance results in the following two issues: (1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label. This actually creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, whereas at test time, F1 gives equal weight to positive and negative examples; (2) the overwhelming effect of easy-negative examples. As pointed out by Meng et al. (2019), a significantly large number of negative examples also means that the number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks (Lample et al., 2016; Wu et al., 2019; Devlin et al., 2018; Yu et al., 2018a; McCann et al., 2018; Ma and Hovy, 2016; Chen et al., 2017), handles neither of the issues.  

To handle the first issue, we propose to replace CE or MLE with losses based on the Sørensen–Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The Sørensen–Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the $F_{\beta}$ score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.  

Only using dice loss or Tversky index is not enough since they are unable to address the dominating"
2,	"lattice lstm. arXiv preprint arXiv:1805.02023.  

# A Dataset Details  

# A.1 Part-of-Speech Tagging  

Datasets We conduct experiments on three widely used benchmark, i.e., Chinese Treebank $5.0^{2}/6.0^{3}$ and $\mathrm{UD1.4^{4}}$ .  

CTB5 is a Chinese dataset for tagging and parsing, which contains 507,222 words, 824,983 characters and 18,782 sentences extracted from newswire sources, including 698 articles from Xinhua (1994-1998), 55 articles from Information Services Department of HKSAR (1997) and 132 articles from Sinorama Magazine (1996-1998 & 2000-2001).  

CTB6 is an extension of CTB5, containing 781,351 words, 1,285,149 characters and 28,295 sentences.  

UD is the abbreviation of Universal Dependencies, which is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. In this work, we use UD1.4 for Chinese POS tagging.  

# A.2 Named Entity Recognition  

Datasets For the NER task, we consider both Chinese datasets, i.e., OntoNotes $.4.0^{5}$ and MSRA6 , and English datasets, i.e., CoNLL2003  and OntoNote $\div5.0^{8}$ .  

CoNLL2003 is an English dataset with 4 entity types: Location, Organization, Person and Miscellaneous. We followed data processing protocols in (Ma and Hovy, 2016).  

English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types. We use the standard train/dev/test split of CoNLL2012 shared task.  

Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Since the development set is not provided in the original MSRA dataset, we randomly split the training set into training and development splits by 9:1. We use the official test set for evaluation.  

Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. In this paper, we take the same data split as Wu et al. (2019) did.  

# A.3 Machine Reading Comprephension  

Datasets For MRC task, we use three datasets: $\mathrm{SQuADv1.1/v2.0^{9}}$ and Queref  datasets.  

SQuAD v1.1 and SQuAD $\mathbf{v}2.\mathbf{0}$ are the most widely used QA benchmarks. SQuAD1.1 is a collection of 100K crowdsourced question-answer pairs, and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage.  

Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems, containing 24K questions over 4.7K paragraphs from Wikipedia.  

# A.4 Paraphrase Identification  

Datasets Experiments are conducted on two PI datasets: $\mathbf{M}\mathbf{R}\mathbf{P}\mathbf{C}^{11}$ and $\mathsf{Q Q P}^{12}$ .  

MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations of whether the sentence pairs are semantically equivalent. The MRPC dataset has imbalanced classes (6800 pairs in total, and $68\%$ for positive, $32\%$ for negative).  

QQP is a collection of question pairs from the community question-answering website Quora. The class distribution in QQP is also unbalanced (over 400,000 question pairs in total, and $37\%$ for positive, $63\%$ for negative)."
    ```
    
The OntoNotes4.0 dataset is used for both Chinese and English Named Entity Recognition tasks. (1) Chinese OntoNotes 4.0 is mentioned as a dataset used for named entity recognition experiments in Chinese. (2) OntoNotes4.0 is also mentioned in the context of English datasets, indicating its use in English Named Entity Recognition tasks as well.
mm_response:
['The OntoNotes4.0 dataset used for the Named Entity Recognition task is Chinese.']
merged_mm_response:
Analyze the Responses:
The provided response states that the OntoNotes4.0 dataset used for the Named Entity Recognition task is Chinese. There is no contradiction, repetition, or inconsistency in the response as it directly answers the user's query.

Reasonably Determine:
Based on the provided information, the response accurately addresses the user's query by stating that the OntoNotes4.0 dataset used for the Named Entity Recognition task is Chinese.

Provide a Unified Answer:
The OntoNotes4.0 dataset used for the Named Entity Recognition task is Chinese.

    -----Entities-----
    ```csv
    "id",	"entity",	"type",	"description",	"rank"
0,	"$\ALPHA_{I}$",	"EVENT",	"$\alpha_{i}$ is the weighting factor associated with instance $x_{i}$, used to adjust the Cross Entropy Loss equation.",	4
1,	"CROSS ENTROPY LOSS",	"EVENT",	"Cross Entropy Loss is a method used to calculate the loss in classification problems, where each instance contributes equally to the final objective.",	3
2,	"CROSS-ENTROPY",	"CONCEPT",	"Cross-entropy is discussed as an accuracy-oriented objective, compared to the proposed losses which are soft versions of F1 score.",	0
3,	"$X_{I}$",	"EVENT",	"$x_{i}$ represents an instance in the training set, associated with a binary label and contributes to the calculation of Cross Entropy Loss and Dice Coefficient.",	3
4,	"$N$",	"EVENT",	"$n$ is the total number of samples in the training set, used in the calculation of the weighting factor $\alpha$.",	1
5,	"TVERSKY INDEX",	"EVENT",	"The Tversky index extends the dice loss and is used as a basis for the proposed loss function in the paper, offering more flexibility."<SEP>"Tversky Index is mentioned as a method offering flexibility in controlling the tradeoff between false-negatives and false-positives."<SEP>"Tversky Index is related to the Dice Coefficient and is used in the context of measuring similarity between sets."<SEP>"Tversky index is an extension of the dice loss that uses a weight trading precision and recall, offering more flexibility.",	5
6,	"$K$",	"EVENT",	"$K$ is a hyperparameter used in the calculation of the weighting factor $\alpha$.",	1
7,	"$N_{T}$",	"EVENT",	"$n_{t}$ is the number of samples with class $t$, used in the calculation of the weighting factor $\alpha$.",	1
8,	"FIGURE 1",	"EVENT",	"Figure 1 is an illustration of the derivatives of the four losses mentioned in the text. Figure 1 provides an explanation from the perspective in derivative regarding the behavior of different losses.",	5
9,	"TVERSKY LOSS (TL)",	"EVENT",	"Tversky loss is a loss function derived from the Tversky index, used for optimizing the tradeoff between false-negatives and false-positives. The graph shows the derivative of Tversky Loss with β=0.5, represented by a yellow line that starts at 0 and increases gradually as the probability of the ground-truth label approaches 1.",	2
    ```
    -----Relationships-----
    ```csv
    "id",	"source",	"target",	"description",	"weight",	"rank"
0,	"BERT-TAGGER+FL",	"FIGURE 1",	"The graph shows the derivative of Focal Loss with γ=1, represented by a blue line that starts at -2 and increases sharply as the probability of the ground-truth label approaches 1.",	9.0,	10
1,	"DICE SIMILARITY COEFFICIENT (DSC)",	"FIGURE 1",	"The graph shows the derivative of Dice Similarity Coefficient, represented by a purple line that starts at -1 and increases gradually as the probability of the ground-truth label approaches 1.",	9.0,	10
2,	"DICE LOSS (DL)",	"FIGURE 1",	"The graph shows the derivative of Dice Loss with γ=1, represented by an orange line that starts at -1 and increases gradually as the probability of the ground-truth label approaches 1.",	9.0,	9
3,	"DICE LOSS (DL)",	"TVERSKY INDEX",	"Tversky index extends the concept of dice loss by incorporating a weight that trades precision and recall.",	7.0,	9
4,	"CHINESE ONTO4.0",	"TVERSKY INDEX",	"Tversky Index is used with the Chinese OntoNotes4.0 dataset to explore the effect of hyperparameters on performance.",	7.0,	9
5,	"ENGLISH QUOREF",	"TVERSKY INDEX",	"Tversky Index is used with the QuoRef MRC dataset to explore the effect of hyperparameters on performance.",	7.0,	8
6,	"SØRENSEN–DICE COEFFICIENT",	"TVERSKY INDEX",	"Sørensen–Dice Coefficient and Tversky Index are related measures used to evaluate the similarity between sets, with Tversky Index being a generalization of the Dice Coefficient.",	6.0,	8
7,	"CROSS ENTROPY LOSS",	"TVERSKY INDEX",	"Cross Entropy Loss and Tversky Index are both statistical measures used in classification, with Tversky Index being an extension of the Dice Coefficient.",	4.0,	8
8,	"FIGURE 1",	"TVERSKY LOSS (TL)",	"The graph shows the derivative of Tversky Loss with β=0.5, represented by a yellow line that starts at 0 and increases gradually as the probability of the ground-truth label approaches 1.",	9.0,	7
9,	"$X_{I}$",	"$\ALPHA_{I}$",	"Instance $x_{i}$ is associated with the weighting factor $\alpha_{i}$, which adjusts its contribution to the Cross Entropy Loss.",	7.0,	7
10,	"FIGURE 1",	"IMAGE_2",	"IMAGE_2" is the image of "FIGURE 1".",	10.0,	6
11,	"CROSS ENTROPY LOSS",	"SØRENSEN–DICE COEFFICIENT",	"Both Cross Entropy Loss and Sørensen–Dice Coefficient are used in the context of evaluating classification performance, though they serve different purposes.",	5.0,	6
12,	"$N_{T}$",	"$\ALPHA_{I}$",	"The weighting factor $\alpha_{i}$ is calculated using the number of samples with class $t$, $n_{t}$.",	8.0,	5
13,	"$N$",	"$\ALPHA_{I}$",	"The weighting factor $\alpha_{i}$ is calculated using the total number of samples in the training set, $n$.",	8.0,	5
14,	"$K$",	"$\ALPHA_{I}$",	"The hyperparameter $K$ is used in the calculation of the weighting factor $\alpha_{i}$.",	7.0,	5
15,	"$P_{I}$",	"$X_{I}$",	"Instance $x_{i}$ has predicted probabilities $p_{i}$ for the two classes.",	9.0,	4
16,	"$X_{I}$",	"$Y_{I}$",	"Instance $x_{i}$ is associated with the binary label $y_{i}$, which denotes its ground-truth class.",	9.0,	4
17,	"CROSS ENTROPY LOSS",	"VALVERDE ET AL.",	"Valverde et al. are referenced in the context of discussing the challenges associated with the selection of weighting factors in Cross Entropy Loss for multi-class classification tasks.",	6.0,	4
18,	"TVERSKY INDEX (TI)",	"TVERSKY LOSS (TL)",	"The Tversky loss is derived from the Tversky index, which is a more general case of the dice coefficient.",	8.0,	3
    ```
    -----Sources-----
    ```csv
    "id",	"content"
0,	"$ denote a set of training instances and each instance $x_{i}\in X$ is associated with a golden binary label $y_{i}=\left[y_{i0},y_{i1}\right]$ denoting the ground-truth class $x_{i}$ belongs to, and $p_{i}\,=\,[p_{i0},p_{i1}]$ is the predicted probabilities of the two classes respectively, where $y_{i0},y_{i1}\in$ $\{0,1\},p_{i0},p_{i1}\in[0,1]$ and $p_{i1}+p_{i0}=1$ .  

# 3.2 Cross Entropy Loss  

The vanilla cross entropy (CE) loss is given by:  

$$
\mathrm{CE}=-\frac{1}{N}\sum_{i}\sum_{j\in\{0,1\}}y_{i j}\log p_{i j}
$$  

As can be seen from Eq.1, each $x_{i}$ contributes equally to the final objective. Two strategies are normally used to address the the case where we wish that not all $x_{i}$ are treated equally: associating different classes with different weighting factor $\alpha$ or resampling the datasets. For the former, Eq.1 is adjusted as follows:  

$$
\mathrm{CE}=-\frac{1}{N}\sum_{i}\alpha_{i}\sum_{j\in\{0,1\}}y_{i j}\log p_{i j}
$$  

where $\alpha_{i}\in[0,1]$ may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation. In this work, we use $\textstyle\log({\frac{n-n_{t}}{n_{t}}}+K)$ to calculate the coefficient $\alpha$ , where $n_{t}$ is the number of samples with class $t$ and $n$ is the total number of samples in the training set. $K$ is a hyperparameter to tune. Intuitively, this equation assigns less weight to the majority class and more weight to the minority class. The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human-designed criteria, e.g. extracting equal training samples from each class. Both strategies are equivalent to changing the data distribution during training and thus are of the same nature. Empirically, these two methods are not widely used due to the trickiness of selecting $\alpha$ especially for multi-class classification tasks and that inappropriate selection can easily bias towards rare classes (Valverde et al., 2017).  

# 3.3 Dice Coefficient and Tversky Index  

Sørensen–Dice coefficient (Sorensen, 1948; Dice, 1945), dice coefficient (DSC) for short, is an F1- oriented statistic used to gauge the similarity of two sets. Given two sets $A$ and $B$ , the vanilla dice coefficient between them is given as follows:  

$$
\operatorname{DSC}(A,B)={\frac{2|A\cap B|}{|A|+|B|}}
$$  

In our case, $A$ is the set that contains all positive examples predicted by a specific model, and $B$ is the set of all golden positive examples in the dataset. When applied to boolean data with the definition of true positive (TP), false positive (FP), and false negative (FN), it can be then written as follows:  

$$
{\begin{array}{r l}&{{\mathrm{DSC}}={\frac{\mathrm{2TP}}{\mathrm{2TP}+{\mathrm{FN}}+{\mathrm{FP}}}}={\frac{\mathrm{2}{\frac{\mathrm{TP}}{{\mathrm{TP}}+{\mathrm{FN}}}}{\frac{\mathrm{TP}}{{\mathrm{TP}}+{\mathrm{FP}}}}}{{\frac{\mathrm{TP}}{{\mathrm{TP}}+{\mathrm{FN}}}}+{\frac{\mathrm{TP}}{{\mathrm{TP}}+{\mathrm{FP}}}}}}}\\ &{\qquad={\frac{\mathrm{2Pre}\times{\mathrm{Rec}}}{{\mathrm{Pre}}+{\mathrm{Rec}}}}=F1}\end{array}}
$$  

For an individual example $x_{i}$ , its corresponding dice coefficient is given as follows:  

$$
\mathrm{DSC}(x_{i})=\frac{2p_{i1}y_{i1}}{p_{i1}+y_{i1}}
$$  

As can be seen, a negative example $(y_{i1}=0)$ ) does not contribute to the objective. For smoothing purposes, it is common to add a $\gamma$ factor to both the nominator and the denominator, making the form to be as follows (we simply set $\gamma=1$ in the rest of Table 2: Different losses and their formulas. We add $+1$ to DL, TL and DSC so that they are positive.  

this paper):  

$$
\mathrm{DSC}(x_{i})=\frac{2p_{i1}y_{i1}+\gamma}{p_{i1}+y_{i1}+\gamma}
$$  

As can be seen, negative examples whose DSC is $\frac{\gamma}{p_{i1}\!+\!\gamma}$ , also contribute to the training. Additionally, Milletari et al. (2016) proposed to change the denominator to the square form for faster convergence, which leads to the following dice loss (DL):"
1,	"0.05\,\mathrm{F}1\right)$ ) over DL. In contrast, it significantly outperforms DL for +negative dataset. This is in line with our expectation since DSC helps more on more imbalanced datasets. The performance of FL and DL are not consistent across different datasets, while DSC consistently performs the best on all datasets.  

# 5.2 Dice loss for accuracy-oriented tasks?  

We argue that the cross-entropy objective is actually accuracy-oriented, whereas the proposed losses perform as a soft version of F1 score. To Table 9: The effect of DL and DSC on sentiment classification tasks. BERT $\mathrm{\bf\Sigma+}\mathrm{\bfCE}$ refers to fine-tuning BERT and setting cross-entropy as the training objective.  

explore the effect of the dice loss on accuracyoriented tasks such as text classification, we conduct experiments on the Stanford Sentiment Treebank (SST) datasets including SST-2 and SST-5. We fine-tuned $\mathsf{B E R T_{L a r g e}}$ with different training objectives. Experimental results for SST are shown in Table 9. For SST-5, BERT with CE achieves 55.57 in terms of accuracy, while DL and DSC perform slightly worse (54.63 and 55.19, respectively). Similar phenomenon is observed for SST-2. These results verify that the proposed dice loss is not accuracy-oriented, and should not be used for accuracy-oriented tasks.  

# 5.3 Hyper-parameters in Tversky Index  

As mentioned in Section 3.3, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., $\alpha$ and $\beta$ ) in TI to test how they manipulate the tradeoff. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset. Experimental results are shown in Table 10. The highest F1 on Chinese OntoNotes4.0 is 84.67 when $\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\alpha$ is set to 0.4. In addition, we can observe that the performance varies a lot as $\alpha$ changes in distinct datasets, which shows that the hyperparameters $\alpha,\beta$ acturally play an important role in TI.  

# 6 Conclusion  

In this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (F1 score). Experimental results show that the proposed loss function help Table 10: The effect of hyperparameters in Tversky Index. We set $\beta=1-\alpha$ and thus we only list $\alpha$ here.  

  

to achieve significant performance boost without changing model architectures.  

# Acknowledgement  

We thank all anonymous reviewers, as well as Qinghong Han, Wei Wu and Jiawei Wu for their comments and suggestions. The work is supported by the National Natural Science Foundation of China (NSFC No. 61625107 and 61751209).  

# References  

Bernd Bohnet, Ryan T. McDonald, Gon¸calo Simo˜es, Daniel Andor, Emily Pitler, and Joshua Maynez. 2018. Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2642–2652.  

Haw-Shiuan Chang, Erik G. Learned-Miller, and Andrew McCallum. 2017. Active bias: Training more accurate neural networks by emphasizing high variance samples. In NIPS.  

N. V. Chawla, K. W. Bowyer, Lawrence O. Hall, and W. P. Kegelmeyer. 2002. Smote: Synthetic minority over-sampling technique. J. Artif. Intell. Res., 16:321– 357.  

Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051.  

Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang, Lingyu Duan, Zhibo Chen, Changwei He, and Junni Zou. 2019. Towards accurate one-stage object detection with ap-loss. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 5119–5127.  

Shijuan Chen, Haibo He, and Edwardo A. Garcia. 2010. Ramoboost: Ranked minority oversampling in boosting. IEEE Transactions on Neural Networks, 21:1624– 1642.  

Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc V. Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Procfessing, Brussels, Belgium, October 31 - November 4, 2018, pages 1914–1925.  

Pradeep Dasigi, Nelson F Liu, Ana Marasovic, Noah A Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. arXiv preprint arXiv"
2,	"false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the $F_{\beta}$ score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.  

Only using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy-negative examples. This is intrinsically because dice loss is actually a soft version of the F1 score. Taking the binary classification task as an example, at test time, an example will be classified as negative as long as its probability is smaller than 0.5, but training will push the value to 0 as much as possible. This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easynegative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hard-negative examples and positive ones. Inspired by the idea of focal loss (Lin et al., 2017) in computer vision, we propose a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$ , and this weight dynamically changes as training proceeds. This strategy helps deemphasize confident examples during training as their probability $p$ approaches 1, making the model attentive to hard-negative examples, and thus alleviates the dominating effect of easy-negative examples. Combing both strategies, we observe significant performance boosts on a wide range of data imbalanced NLP tasks.  

The rest of this paper is organized as follows: related work is presented in Section 2. We describe different proposed losses in Section 3. Experimental results are presented in Section 4. We perform ablation studies in Section 5, followed by a brief conclusion in Section 6.  

# 2 Related Work  

# 2.1 Data Resampling  

The idea of weighting training examples has a long history. Importance sampling (Kahn and Marshall, 1953) assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost (Kanduri et al., 2018) select harder examples to train subsequent classifiers. Similarly, hard example mining (Malisiewicz et al., 2011) downsamples the majority class and exploits the most difficult examples. Oversampling (Chen et al., 2010; Chawla et al., 2002) is used to balance the data distribution. Another line of data resampling is to dynamically control the weights of examples as training proceeds. For example, focal loss (Lin et al., 2017) used a soft weighting scheme that emphasizes harder examples during training. In self-paced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss which encourages learning easier examples first. At each training step, selfpaced learning algorithm optimizes model parameters and example weights jointly. Other works (Chang et al., 2017; Katharopoulos and Fleuret, 2018) adjusted the weights of different training examples based on training loss. Besides, recent work (Jiang et al., 2017; Fan et al., 2018) proposed to learn a separate network to predict sample weights.  

# 2.2 Data Imbalance Issue in Computer Vision  

The background-object label imbalance issue is severe and thus well studied in the field of object detection (Li et al., 2015; Girshick, 2015; He et al., 2015; Girshick et al., 2013; Ren et al., 2015). The idea of hard negative mining (HNM) (Girshick et al., 2013) has gained much attention recently. Pang et al. (2019) proposed a novel method called IoU-balanced sampling and Chen et al. (2019) designed a ranking model to replace the conventional classification task with an average-precision loss to alleviate the class imbalance issue. The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP.  

Sudre et al. (2017) addressed the severe class imbalance issue for the image segmentation task. They proposed to use the class re-balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks. Shen et al. (2018) investigated the influence of Dice-based loss for multi-class organ segmentation using a dataset of abdominal CT volumes. Kodym et al. (2018) proposed to use the batch soft Dice loss function to train the CNN network for the task of segmentation of organs at risk (OAR) of medical images. Shamir et al. (2019) extended the definition of the classical Dice coefficient to facilitate the direct comparison of a ground truth binary image with a probabilistic map. In this paper, we introduce dice loss into NLP tasks as the training objective and propose a dynamic weight adjusting strategy to address the dominating influence of easy-negative examples.  

# 3 Losses  

# 3.1 Notation  

For illustration purposes, we use the binary classification task to demonstrate how different losses work. The mechanism can be easily extended to multi-class classification. Let $X$ denote a set of training instances and each instance $x_{i}\in X$ is associated with a golden binary label $y_{i}=\left[y_{i0},y_{i1}\right]$ denoting the ground-truth class $x_{i}$ belongs to, and $p_{i}\,=\,[p_{i0},p_{i1}]$ is the predicted probabilities of the two classes respectively, where $y_{i0},y_{i"
    ```
    
The paper proposes to calculate the coefficient α for the Weighted Cross Entropy Loss using the following formula:

$$\alpha = \log\left(\frac{n - n_t}{n_t} + K\right)$$

Where:
- \( n \) is the total number of samples in the training set.
- \( n_t \) is the number of samples with class \( t \).
- \( K \) is a hyperparameter to tune.

This formula assigns less weight to the majority class and more weight to the minority class, aiming to address class imbalance in the training data.
