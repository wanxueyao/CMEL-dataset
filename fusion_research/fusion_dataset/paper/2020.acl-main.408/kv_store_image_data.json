{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_1.jpg",
        "caption": [
            "Figure 1: Examples of instances, labels, and rationales illustrative of four (out of seven) datasets included in ERASER. The ‘erased’ snippets are rationales. "
        ],
        "footnote": [],
        "context": "Interpretability is a broad In curating and releasing ERASER we take inspiration from the stickiness of the GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) benchmarks for evaluating progress in natural language understanding tasks, which have driven rapid progress on models for general language representation learning. We believe the still somewhat nascent subfield of interpretable NLP stands to benefit similarly from an analogous collection of standardized datasets and tasks; we hope these will aid the design of standardized metrics to measure different properties of ‘interpretability’, and we propose a set of such metrics as a starting point. https://www.eraserbenchmark.com/ 1 Introduction Interest has recently grown in designing NLP systems that can reveal why models make specific predictions. But work in this direction has been conducted on different datasets and using different metrics to quantify performance; this has made it difficult to compare methods and track progress. We aim to address this issue by releasing a standardized benchmark of datasets — repurposed and augmented from pre-existing corpora, spanning a range of NLP tasks — and associated metrics for measuring different properties of rationales. We refer to this as the Evaluating Rationales And Simple English Reasoning (ERASER $\\circledcirc$ ) benchmark. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-8d6c836f0365aa5d8c1bc2d7a0625a8e",
        "description": "The image is a composite of four sections, each illustrating different datasets included in the ERASER benchmark. The first section is labeled 'Movie Reviews' and contains a positive review snippet: 'In this movie, ... Plots to take over the world. The acting is great! The soundtrack is run-of-the-mill, but the action more than makes up for it.' Below this snippet are two options: (a) Positive and (b) Negative. The second section is labeled 'e-SNLI' and shows a hypothesis (H): 'A man in an orange vest leans over a pickup truck,' and a premise (P): 'A man is touching a truck.' Below these statements are three options: (a) Entailment, (b) Contradiction, and (c) Neutral. The third section is labeled 'Commonsense Explanations (CoS-E)' and poses the question: 'Where do you find the most amount of leafs?' with five options: (a) Compost pile, (b) Flowers, (c) Forest, (d) Field, and (e) Ground. The fourth section is labeled 'Evidence Inference' and includes an article snippet about a clinical trial comparing saline with furosemide. A prompt asks about the reported difference between patients receiving placebo and those receiving furosemide, with three options: (a) Sig. decreased, (b) No sig. difference, and (c) Sig. increased.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_2.jpg",
        "caption": [],
        "footnote": [],
        "context": "Evaluating rationales. Work on evaluating rationales has often compared these to human judgments (Strout et al., 2019; Doshi-Velez and Kim, 2017), or elicited other human evaluations of explanations (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018). There has also been work on visual evaluations of saliency maps (Li et al., 2016; Ding et al., 2017; Sundararajan et al., explicit training data is available for the former. Rajani et al. (2019) fine-tuned a Transformerbased language model (Radford et al., 2018) on free-text rationales provided by humans, with an objective of generating open-ended explanations to improve performance on downstream tasks.  work has proposed ‘pipeline’ approaches in which independent models are trained to perform rationale extraction and classification on the basis of these, respectively (Lehman et al., 2019; Chen et al., 2019), assuming Table 1: Overview of datasets in the ERASER benchmark. Tokens is the average number of tokens in each document. Comprehensive rationales mean that all supporting evidence is marked; !denotes cases where this is (more or less) true by default; $\\diamond,\\,\\bullet$ are datasets for which we have collected comprehensive rationales for either a subset or all of the test datasets, respectively. Additional information can be found in Appendix A. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-20e92f18c134263325b7748451674989",
        "description": "The image is a table labeled 'Overview of datasets in the ERASER benchmark.' The table is structured with four main columns: Name, Size (train/dev/test), Tokens, and Comp?. Each row represents a different dataset. The 'Name' column lists the names of the datasets: Evidence Inference, BoolQ, Movie Reviews, FEVER, MultiRC, CoS-E, and e-SNLI. The 'Size (train/dev/test)' column provides the number of instances for each split of the dataset. For example, Evidence Inference has 7958 train, 972 dev, and 959 test instances. The 'Tokens' column shows the average number of tokens in each document. For instance, Evidence Inference has an average of 4761 tokens. The 'Comp?' column indicates whether comprehensive rationales are available for the dataset, marked with symbols such as $\\diamond$, $\\bullet$, or a checkmark. For example, Evidence Inference and BoolQ have $\\diamond$ symbols, indicating that comprehensive rationales are collected for either a subset or all of the test datasets.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_3.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 2: Human agreement with respect to rationales. For Movie Reviews and BoolQ we calculate the mean agreement of individual annotators with the majority vote per token, over the two-three annotators we hired via Upwork and Amazon Turk, respectively. The e-SNLI dataset already comprised three annotators; for this we calculate mean agreement between individuals and the majority. For CoS-E, MultiRC, and FEVER, members of our team annotated a subset to use a comparison to the (majority of, where appropriate) existing rationales. We collected comprehensive rationales for Evidence Inference from Medical Doctors; as they have a high amount of expertise, we a high Cohen $\\kappa$ (Cohen, 1960); with substantial or better agreement. 4 Metrics In ERASER models are evaluated both for their predictive performance and with respect to the rationales that they extract. For the former, we rely on the established metrics for the respective tasks. Here we describe the metrics we propose to evaluate the quality of extracted rationales. We do not claim that these are necessarily the best metrics for evaluating rationales, however. Indeed, we hope the release of ERASER will spur additional research into how best to measure the quality of model explanations in the context of NLP. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-e34d27aa833558c38111606bd47ec2b7",
        "description": "The image is a table labeled 'Table 2: Human agreement with respect to rationales.' The table provides statistical information about the agreement of annotators on various datasets. The columns are as follows: Dataset, Cohen κ (with standard deviation), F1 (with standard deviation), P (Precision with standard deviation), R (Recall with standard deviation), #Annotators/doc, and #Documents. The rows represent different datasets: Evidence Inference, BoolQ, Movie Reviews, FEVER, MultiRC, CoS-E, and e-SNLI. For Evidence Inference, all values are marked with a dash (-) indicating no data or not applicable. For BoolQ, the values are Cohen κ: 0.618 ± 0.194, F1: 0.617 ± 0.227, P: 0.647 ± 0.260, R: 0.726 ± 0.217, with 3 annotators per document and 199 documents. For Movie Reviews, the values are Cohen κ: 0.712 ± 0.135, F1: 0.799 ± 0.138, P: 0.693 ± 0.153, R: 0.989 ± 0.102, with 2 annotators per document and 96 documents. For FEVER, the values are Cohen κ: 0.854 ± 0.196, F1: 0.871 ± 0.197, P: 0.931 ± 0.205, R: 0.855 ± 0.198, with 2 annotators per document and 24 documents. For MultiRC, the values are Cohen κ: 0.728 ± 0.268, F1: 0.749 ± 0.265, P: 0.695 ± 0.284, R: 0.910 ± 0.259, with 2 annotators per document and 99 documents. For CoS-E, the values are Cohen κ: 0.619 ± 0.308, F1: 0.654 ± 0.317, P: 0.626 ± 0.319, R: 0.792 ± 0.371, with 2 annotators per document and 100 documents. For e-SNLI, the values are Cohen κ: 0.743 ± 0.162, F1: 0.799 ± 0.130, P: 0.812 ± 0.154, R: 0.853 ± 0.124, with 3 annotators per document and 9807 documents.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_4.jpg",
        "caption": [
            "Figure 2: Illustration of faithfulness scoring metrics, comprehensiveness and sufficiency, on the Commonsense Explanations (CoS-E) dataset. For the former, erasing the tokens comprising the provided rationale $\\left(\\tilde{x}_{i}\\right)$ ought to decrease model confidence in the output ‘Forest’. For the latter, the model should be able to come to a similar disposition regarding ‘Forest’ using only the rationales $r_{i}$ ."
        ],
        "footnote": [],
        "context": "As defined, the above measures have assumed discrete rationales $r_{i}$ . We would also like to evaluate the faithfulness of continuous importance scores assigned to tokens by models. Here we adopt a simple approach for this. We convert soft scores over features $s_{i}$ provided by a model into discrete rationales $r_{i}$ by taking the top $-k_{d}$ values, where $k_{d}$ is a threshold for dataset $d$ . We set These metrics are illustrated in Figure 2. $$ {\\mathrm{sufficiency}}=m(x_{i})_{j}-m(r_{i})_{j} $$ Sufficiency. This captures the degree to which the snippets within the extracted rationales are adequate for a model to make a prediction.  consider the predicted probability from the model for the same class once the supporting rationales are stripped. Intuitively, the model ought to be less confident in its prediction once rationales are removed from $x_{i}$ . We can measure this as: $$ {\\mathrm{comprehensiveness}}=m(x_{i})_{j}-m(x_{i}\\backslash r_{i})_{j} $$ A high score here implies that the rationales were indeed influential in the prediction, while a low score suggests that they were not. A negative value here means that the model became more confident in its prediction after the rationales were removed; this would seem counter-intuitive if the rationales were indeed the reason for its prediction. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-1a6008f8664dc874a3e62b39184c58af",
        "description": "The image is a figure illustrating faithfulness scoring metrics, specifically comprehensiveness and sufficiency, on the Commonsense Explanations (CoS-E) dataset. The figure consists of three main panels, each depicting a bar chart with five categories: (a) Compost pile, (b) Flowers, (c) Forest, (d) Field, and (e) Ground. Each panel has a corresponding neural network diagram below it, indicating the process of feature extraction and prediction.\\n\\n- **Left Panel**: This panel shows the predicted probability distribution for the class 'Forest' given the input \\( x_i \\). The bars represent the probabilities for each category, with the highest probability (red bar) for 'Forest'. Below this panel is a neural network diagram showing the input features being processed to generate the prediction.\\n\\n- **Middle Panel**: This panel illustrates the concept of comprehensiveness. It shows the predicted probability distribution after erasing the tokens comprising the provided rationale \\( \\\\tilde{x}_i \\). The red bar for 'Forest' is lower compared to the left panel, indicating a decrease in model confidence. The neural network diagram below highlights the removal of certain features, leading to a different prediction.\\n\\n- **Right Panel**: This panel demonstrates the concept of sufficiency. It shows the predicted probability distribution using only the rationales \\( r_i \\). The red bar for 'Forest' remains high, suggesting that the model can still make a similar prediction using only the rationales. The neural network diagram below emphasizes the use of selected features for prediction.\\n\\nEach panel includes a question at the bottom: 'Where do you find the most amount of leafs?', which is used to contextualize the predictions. The overall figure aims to evaluate how well the model's predictions align with the provided rationales and whether the rationales are sufficient for the model to maintain its confidence in the prediction.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 4: Metrics for ‘soft’ scoring models. Perf. is accuracy (CoS-E) or  To soft score features we consider: Simple gradients, attention induced over contextualized representations, and LIME (Ribeiro et al., 2016). BERT; when we exceed this, we effectively start encoding a ‘new’ sequence (setting the positional index to 0) via BERT. The hope is that the LSTM learns to compensate for this. Evidence Inference and BoolQ comprise very long $(>\\!1000$ token) inputs; we were unable to run BERT over these. We instead resorted to swapping GloVe 300d embeddings (Pennington et al., 2014) in place of BERT representations for tokens. spans.  The hidden representations from the LSTM are collapsed into a single vector using additive attention (Bahdanau et al., 2015). The LSTM layer allows us to bypass the 512 word limit imposed by Table 3: Performance of models that perform hard rationale selection. All models are supervised at the rationale level except for those marked with (u), which learn only from instance-level supervision; denotes cases in which rationale training degenerated due to the REINFORCE style training. Perf. is accuracy (CoS-E) or macro-averaged F1 (others). Bert-To-Bert for CoS-E and e-SNLI uses a token classification objective. BertTo-Bert CoS-E uses the highest scoring answer. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-a69e074dfaffbb851a506d0d912d71ad",
        "description": "The image is a table labeled 'Metrics for ‘soft’ scoring models.' The table is structured with rows representing different datasets and columns representing performance metrics. The datasets include Evidence Inference, BoolQ, Movie Reviews, FEVER, MultiRC, CoS-E, and e-SNLI. Each dataset has corresponding values for Perf., IOU F1, and Token F1. For example, Lei et al. (2016) for Evidence Inference has a Perf. of 0.461, IOU F1 of 0.000, and Token F1 of 0.000. Bert-To-Bert for the same dataset has a Perf. of 0.708, IOU F1 of 0.455, and Token F1 of 0.468. The table provides detailed numerical values for each metric across all datasets, highlighting the performance of different models.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "In Table 3 we evaluate Here we present initial results for the baseline models discussed in Section 5, with respect to the metrics proposed in Section 4. We present results in two parts, reflecting the two classes of rationales discussed above: ‘Hard’ approaches that perform discrete selection of snippets, and ‘soft’ methods that assign continuous importance scores to tokens. 6 Evaluation Table 4: Metrics for ‘soft’ scoring models. Perf. is accuracy (CoS-E) or F1 (others). Comprehensiveness and sufficiency are in terms of AOPC (Eq. 3). ‘Random’ assigns random scores to tokens to induce orderings; these are averages over 10 runs. uses a token classification objective. BertTo-Bert CoS-E uses the highest scoring answer.   BERT; when we exceed this, we effectively start encoding a ‘new’ sequence (setting the positional index to 0) via BERT. The hope is that the LSTM learns to compensate for this. Evidence Inference and BoolQ comprise very long $(>\\!1000$ token) inputs; we were unable to run BERT over these. We instead resorted to swapping GloVe 300d embeddings (Pennington et al., 2014) in place of BERT representations for tokens. spans. To soft score features we consider: Simple gradients, attention induced over contextualized representations, and LIME (Ribeiro et al., 2016). ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-a69e074dfaffbb851a506d0d912d71ad",
        "description": "The image is a table labeled 'Table 4: Metrics for ‘soft’ scoring models.' The table is structured with the following columns: Perf., AUPRC, Comp. ↑, and Suff. ↓. Each row represents different models and their respective metrics. The rows are categorized under different datasets: Evidence Inference, BoolQ, Movies, FEVER, MultiRC, CoS-E, and e-SNLI. For each dataset, four models are listed: GloVe + LSTM - Attention, GloVe + LSTM - Gradient, GloVe + LSTM - Lime, and GloVe + LSTM - Random. The values in the Perf. column represent accuracy (CoS-E) or F1 (others). The AUPRC column shows the Area Under the Precision-Recall Curve. The Comp. ↑ and Suff. ↓ columns are in terms of AOPC (Eq. 3). The 'Random' model assigns random scores to tokens to induce orderings; these are averages over 10 runs. The table provides detailed numerical values for each metric across the different models and datasets. For example, for the Evidence Inference dataset, the GloVe + LSTM - Attention model has a Perf. of 0.429, AUPRC of 0.506, Comp. ↑ of -0.002, and Suff. ↓ of -0.023. Similarly, for the BoolQ dataset, the same model has a Perf. of 0.471, AUPRC of 0.525, Comp. ↑ of 0.010, and Suff. ↓ of 0.022. The table highlights the performance and evaluation metrics of various models across different datasets.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "or questions with rationales Table 6: General dataset statistics: number of labels, instances, unique documents, and average numbers of sentences and tokens in documents, across the publicly released train/validation/test splits in ERASER. For CoS-E and e-SNLI, the sentence counts are not meaningful as the partitioning of question/sentence/answer formatting is an arbitrary choice in this framework.  Table 5: Detailed breakdowns for each dataset - the number of documents, instances, evidence statements, and lengths. Additionally we include the percentage of each relevant document that is considered a rationale. For test sets, counts are for all instances including documents with non comprehensive rationales. we download the 12/20/18 Wikipedia archive, and use FuzzyWuzzy https://github. com/seatgeek/fuzzywuzzy to identify the source paragraph span that best matches the original release. If the Levenshtein distance ratio does not reach a score of at least 90, the corresponding instance is removed. For public release, we use the official validation set for testing, and repartition train into a training and validation set. e-SNLI (Camburu et al., 2018) We perform minimal processing. We separate the premise and hypothesis statements into separate documents. Commonsense Explanations (CoS-E) (Rajani et al., 2019) We perform minimal processing, primarily deletion of any questions without a rationale ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-92ae10ec56afd21a8da50f6e9109d55a",
        "description": "The image is a table labeled 'Table 5: Detailed breakdowns for each dataset' that provides statistics for various datasets across train, validation, and test splits. The table is structured with the following columns: Dataset, Documents, Instances, Rationale %, Evidence Statements, and Evidence Lengths. Each row represents a different dataset and its respective statistics. For example, the MultiRC dataset has 400 documents and 24029 instances in the train split, with a rationale percentage of 17.4% and 56298 evidence statements. The Evidence Inference dataset has 1924 documents and 7958 instances in the train split, with a rationale percentage of 1.34% and 10371 evidence statements. The Movie Reviews dataset has 1599 documents and 1600 instances in the train split, with a rationale percentage of 9.35% and 13878 evidence statements. The FEVER dataset has 2915 documents and 97957 instances in the train split, with a rationale percentage of 20.0% and 146856 evidence statements. The BoolQ dataset has 4518 documents and 6363 instances in the train split, with a rationale percentage of 6.64% and 6363.0 evidence statements. The e-SNLI dataset has 911938 documents and 549309 instances in the train split, with a rationale percentage of 27.3% and 1199035.0 evidence statements. The CoS-E dataset has 8733 documents and 8733 instances in the train split, with a rationale percentage of 26.6% and 8733 evidence statements. The table highlights the detailed statistics for each dataset, including the number of documents, instances, rationale percentages, evidence statements, and evidence lengths.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.acl-main.408/images/image_8.jpg",
        "caption": [],
        "footnote": [],
        "context": "or questions with rationales that were not possible to automatically map back to the underlying text. As recommended by the authors of Talmor et al. (2019) we repartition the train and validation sets into a train, validation, and test set for this benchmark. We encode the entire question and Table 6: General dataset statistics: number of labels, instances, unique documents, and average numbers of sentences and tokens in documents, across the publicly released train/validation/test splits in ERASER. For CoS-E and e-SNLI, the sentence counts are not meaningful as the partitioning of question/sentence/answer formatting is an arbitrary choice in this framework. we use the official validation set for testing, and repartition train into a training and validation set. e-SNLI (Camburu et al., 2018) We perform minimal processing. We separate the premise and hypothesis statements into separate documents. Commonsense Explanations (CoS-E) (Rajani et al., 2019) We perform minimal processing, primarily deletion of any questions without a rationale  Table 5: Detailed breakdowns for each dataset - the number of documents, instances, evidence statements, and lengths. Additionally we include the percentage of each relevant document that is considered a rationale. For test sets, counts are for all instances including documents with non comprehensive rationales. ",
        "chunk_order_index": 11,
        "chunk_id": "chunk-92ae10ec56afd21a8da50f6e9109d55a",
        "description": "The image is a table that provides general dataset statistics for various datasets. The table has six columns: Dataset, Labels, Instances, Documents, Sentences, and Tokens. The rows represent different datasets. The 'Evidence Inference' dataset has 3 labels, 9889 instances, 2411 documents, an average of 156.0 sentences per document, and 4760.6 tokens per document. The 'BoolQ' dataset has 2 labels, 10661 instances, 7026 documents, an average of 175.3 sentences per document, and 3582.5 tokens per document. The 'Movie Reviews' dataset has 2 labels, 2000 instances, 1999 documents, an average of 36.8 sentences per document, and 774.1 tokens per document. The 'FEVER' dataset has 2 labels, 110190 instances, 4099 documents, an average of 12.1 sentences per document, and 326.5 tokens per document. The 'MultiRC' dataset has 2 labels, 32091 instances, 539 documents, an average of 14.9 sentences per document, and 302.5 tokens per document. The 'CoS-E' dataset has 5 labels, 10917 instances, 10917 documents, an average of 1.0 sentence per document, and 27.6 tokens per document. The 'e-SNLI' dataset has 3 labels, 568939 instances, 944565 documents, an average of 1.7 sentences per document, and 16.0 tokens per document.",
        "segmentation": false
    }
}