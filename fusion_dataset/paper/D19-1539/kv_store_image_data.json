{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_1.jpg",
        "caption": [
            "Figure 1: Illustration of the model. $\\mathbf{B}\\mathbf{lock}_{i}$ is a standard transformer decoder block. Green blocks operate left to right by masking future time-steps and blue blocks operate right to left. At the top, states are combined with a standard multi-head self-attention module whose output is fed to a classifier that predicts the center token. "
        ],
        "footnote": [],
        "context": "Experiments on the GLUE (Wang et al., 2018) benchmark show strong gains over the state of the art for each task, including a 9.1 point gain on RTE over Radford et al. (2018). These improvements are consistent with, if slightly behind, BERT (Devlin et al., 2018), which we will discuss in more detail in the next section. We also show that it is possible to stack task-specific architectures for NER and constituency parsing on top of our pretrained representations, and achieve new state-ofthe-art performance levels for both tasks. We also present extensive experimental analysis to better understand these results, showing  token in the training data (Figure 1). We achieve this by introducing a cloze-style training objective where the model must predict the center word given left-to-right and right-to-left context representations. Our model separately computes both forward and backward states with a masked self-attention architecture, that closely resembles a language model. At the top of the network, the forward and backward states are combined to jointly predict the center word. This approach allows us to consider both contexts when predicting words and to incur loss for every word in the training set, if the model does not assign it high likelihood. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-67eb3fcd0b8599eeb07673be2599a910",
        "description": "The image is a flowchart illustrating a model architecture. At the top, there is a gray block labeled 'comb' with an arrow pointing upwards labeled 'b'. This block receives inputs from two sets of blocks below it. On the left side, there are green blocks labeled 'Block_N' and 'Block_1', arranged in a hierarchical structure with arrows pointing upwards. The green blocks represent standard transformer decoder blocks that operate left to right by masking future time-steps. On the right side, there are blue blocks also labeled 'Block_N' and 'Block_1', similarly arranged in a hierarchical structure with arrows pointing upwards. The blue blocks represent blocks that operate right to left. At the bottom, there are arrows labeled '<s>' pointing towards the 'Block_1' blocks on both sides. Additionally, there are two arrows pointing towards the 'comb' block from the 'Block_N' blocks on both sides. One arrow is green and points from the left, while the other is blue and points from the right. The overall structure suggests a model that combines forward and backward states for predicting the center token.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_2.jpg",
        "caption": [
            "Figure 2: Illustration of fine-tuning for a downstream task. For classification problems, output of the first and last token is fed to a task-specific classifier. Masking for the final combination layer (comb) is removed which results in representations based on all forward and backward states (cf. Figure 1). The red dot-dashed arrows show connections that are masked during training, but unmasked for fine-tuning. "
        ],
        "footnote": [],
        "context": "Classification and regression tasks. For single sentence classification tasks, we consider the language model outputs for We use the following approach to fine-tune the pretrained two tower model to specific downstream tasks (Figure 2). 4 Fine-tuning While all states that contain information about the current target word are masked in the final selfattention block during training, we found it beneficial to disable this masking when fine tuning the pretrained model for downstream tasks. This is especially true for tasks that label each token, such as NER, as this allows the model to access the full context including the token itself. the base model we sum the two representations and for the larger models they are concatenated. Keys and values are based on the forward and backward states fed to the attention module. In summary, this module has access to information about the entire input surrounding the current target token. During training, we predict every token in this way. The output of this module is fed to an output classifier which predicts the center token. We use an adaptive softmax for the output classifier (Grave et al., 2017) for the word based models and regular softmax for the BPE based models. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-bd217e7a6216f1573ca3b47fe0f48027",
        "description": "The image is a flowchart illustrating the fine-tuning process for a downstream task in a neural network model. The chart is divided into three main columns labeled 'a', 'b', and 'c'. Each column contains a sequence of green and blue rectangular nodes connected by arrows, representing different states or layers in the model. The green nodes are solid, while the blue nodes have a diagonal striped pattern. The nodes are interconnected with gray arrows indicating the flow of information. At the top of each column, there is a 'comb' node, which combines the outputs from the previous layers. The 'comb' nodes are connected to an output layer labeled 'Embedding of b'. Red dot-dashed arrows indicate connections that are masked during training but unmasked during fine-tuning. These red arrows connect the 'comb' nodes to the final combination layer, allowing the model to access all forward and backward states. The overall structure shows how the model processes information through multiple layers and combines them for the final output.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 1: Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time as measured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model. "
        ],
        "context": "Every setup uses model dimensionaltiy $d\\,=$ 1024 with 1024, followed by a 160K band with dimensionality 256. The remaining types have dimensionality 64; there are 480K types for the small model and 780K for the large model. The BPE model uses a vocabulary of 55K types and we share input and output embeddings in a flat softmax with dimension 1024 (Inan et al., 2016; Press and Wolf, 2017). The BPE vocabulary was constructed by applying 30K merge operations over the training data, then applying the BPE code to the training data and retaining all types occurring at least three times. (Ott et al., 2019). For hyper-parameter and optimization choices we mostly follow Baevski and Auli (2018). Our experiments consider three model sizes shown in Table 1: There are two CNN input models in a base and large configuration as well as a Byte-Pair-Encoding based model (BPE; Sennrich et al., 2016). The CNN models have unconstrained input vocabulary, and an output vocabulary limited to 1M most common types for the large model, and 700K most common types for the base model. CNN models use an adaptive softmax in the output: the head band contains the 60K most frequent types with dimensionality ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-9466068e0789cf83c4509d73e1ec583f",
        "description": "The image is a table labeled 'Table 1: Hyper-parameters for our models.' The table provides detailed information about the hyper-parameters of three different models: CNN Base, CNN Large, and BPE Large. The table is structured with the following columns: Model, Parameters, Updates, Blocks, FFN Dim, Attn Heads (final layer), Query formation (final layer), and Train time (days). The rows contain the following values:\\n\\n- **CNN Base**: Parameters = 177M, Updates = 600K, Blocks = 6, FFN Dim = 4096, Attn Heads (final layer) = 12, Query formation (final layer) = Sum, Train time (days) = 6.\\n- **CNN Large**: Parameters = 330M, Updates = 1M, Blocks = 12, FFN Dim = 4096, Attn Heads (final layer) = 32, Query formation (final layer) = Concat, Train time (days) = 10.\\n- **BPE Large**: Parameters = 370M, Updates = 1M, Blocks = 12, FFN Dim = 4096, Attn Heads (final layer) = 32, Query formation (final layer) = Concat, Train time (days) = 4.5.\\n\\nThe table highlights the differences in parameters, updates, blocks, feed-forward network dimension (FFN Dim), attention heads in the final layer, query formation method in the final layer, and training time across the three models.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Test results as per the GLUE evaluation server. The average column does not include the WNLI test set. mcc $=$ Matthews correlation, acc $=$ Accuracy, scc $=$ Spearman correlation. "
        ],
        "context": "Table 2 shows results for three configurations of our approach (cf. Table 1). The BPE model has more parameters than the CNN model but does not perform better in aggregate, however, it is faster to train. All our models outperform the unidirectional transformer (OpenAI GPT) of Radford et al. (2018), however, our model is about $50\\%$ larger than their model. We also show results for STILTs (Phang et al., 2018) and BERT (Devlin et al., 2018). Our CNN base model performs as well as STILTs in aggregate, however, on some tasks involving STS-B Pearson correlation as well as QQP accuracy. 2018), the Stanford Question Answering Dataset (QNLI; Rajpurkar et al., 2016), the Recognizing Textual Entailment (RTE; Dagan et al., 2006, Bar Haim et al., 2006, Ciampiccolo et al., 2007 Bentivogli et al., 2009). We exclude the Winograd NLI task from our results similar to Radford et al. (2018); Devlin et al. (2018) and report accuracy. For MNLI we report both matched (m) and mismatched (mm) accuracy on test. We also report an average over the GLUE metrics. This figure is not comparable to the average on the official GLUE leaderboard since we exclude Winograd and do not report MRPC accuracy ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-4b3f83319bc6f531992c0907c2ddd16e",
        "description": "The image is a table labeled 'Table 2: Test results as per the GLUE evaluation server.' The table presents the performance of various models on different natural language processing tasks. The columns represent different datasets and metrics, including CoLA (Matthews correlation), SST-2 (Accuracy), MRPC (F1 score), STS-B (Spearman correlation), QQP (F1 score), MNLI-m/mm (Accuracy), QNLI (Accuracy), RTE (Accuracy), and an average score. The rows list different models: OpenAI GPT, CNN Base, CNN Large, BPE Large, GPT on STILTs, BERT_BASE, and BERT_LARGE. Each cell contains a numerical value representing the model's performance on the respective task. For example, the OpenAI GPT scores 45.4 on CoLA, 91.3 on SST-2, 82.3 on MRPC, 80.0 on STS-B, 70.3 on QQP, 82.1/81.4 on MNLI-m/mm, 88.1 on QNLI, and 56.0 on RTE, with an average score of 75.2. The BERT_LARGE model performs the best overall, scoring 60.5 on CoLA, 94.9 on SST-2, 89.3 on MRPC, 86.5 on STS-B, 72.1 on QQP, 86.7/85.9 on MNLI-m/mm, 91.1 on QNLI, and 70.1 on RTE, with an average score of 81.9.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "We evaluated span-level F1 performance on the CoNLL 2003 Named Entity Recognition (NER) task, where spans of text must be segmented and labeled as Person, Organization, Location, or Miscellaneous. We adopted the NER architecture in Peters et al. (2018), a biLSTM-CRF, with two minor modifications: (1) instead of two layers of biLSTM, we only used one, and (2) a linear projection layer was added between the token embedding and biLSTM layer. We did grid search on the pairs of learning rate, and found that projection-biLSTMCRF with 1E-03 and pretrained language model with 1E-05 gave us the 6.2.1 Named Entity Recognition   to significantly improve performance. For example, BERT trains on exactly two sentences while as we train on entire paragraphs. 6.2 Structured Prediction We also evaluated performance on two structured predictions tasks, NER and constituency parsing. For both problems, we stacked task-specific architectures from recent work on top of our pretrained two tower models. We evaluate two ways of stacking: (1) ELMo-style, where the pretrained models are not fine-tuned but are linearly combined at different depths, and (2) with fine-tuning, where we set different learning rates for the task-specific layers but otherwise update all of the parameters during the task-specific training. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-4b3f83319bc6f531992c0907c2ddd16e",
        "description": "The image is a table that presents the F1 scores for different models on the CoNLL 2003 Named Entity Recognition (NER) task. The table has two main columns: 'dev F1' and 'test F1', which represent the F1 scores on the development and test datasets, respectively. The rows list various models and their corresponding F1 scores. The first row shows the model 'ELMo_BASE' with dev F1 of 95.7 and test F1 of 92.2. The second row lists 'CNN Large + ELMo' with slightly higher scores at 96.4 for dev F1 and 93.2 for test F1. The third row, 'CNN Large + fine-tune', achieves the highest scores in this set with 96.9 for dev F1 and 93.5 for test F1. The fourth and fifth rows present the BERT models, 'BERT_BASE' and 'BERT_LARGE', with dev F1 scores of 96.4 and 96.6, respectively, and test F1 scores of 92.4 and 92.8, respectively. The table highlights the performance comparison among these models, showing that the CNN Large model with fine-tuning performs the best.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_6.jpg",
        "caption": [
            "Table 3: CoNLL-2003 Named Entity Recognition results. Test result was evaluated on parameter set with the best dev F1. "
        ],
        "footnote": [
            "Table 4: Penn Treebank Constituency Parsing results. Test result was evaluated on parameter set with the best dev F1. "
        ],
        "context": "We evaluated span-level F1 performance on the CoNLL 2003 Named Entity Recognition (NER) task, where spans of text must be segmented and labeled as Person, Organization, Location, or Miscellaneous. We adopted the NER architecture in Peters et al. (2018), a biLSTM-CRF, with two minor modifications: (1) instead of two layers of biLSTM, we only used one, and (2) a linear projection layer was added between the token embedding and biLSTM layer. We did grid search on the pairs of learning rate, and found that projection-biLSTMCRF with 1E-03 and pretrained language model with 1E-05 gave us the 6.2.1 Named Entity Recognition  to significantly improve performance. For example, BERT trains on exactly two sentences while as we train on entire paragraphs. 6.2 Structured Prediction We also evaluated performance on two structured predictions tasks, NER and constituency parsing. For both problems, we stacked task-specific architectures from recent work on top of our pretrained two tower models. We evaluate two ways of stacking: (1) ELMo-style, where the pretrained models are not fine-tuned but are linearly combined at different depths, and (2) with fine-tuning, where we set different learning rates for the task-specific layers but otherwise update all of the parameters during the task-specific training.  ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-4b3f83319bc6f531992c0907c2ddd16e",
        "description": "The image is a table labeled 'Table 3: CoNLL-2003 Named Entity Recognition results.' The table presents the F1 scores for different models on the development (dev) and test sets. It contains four rows and three columns. The first column lists the models: ELMo_BASE, CNN Large + ELMo, and CNN Large + fine-tune. The second column shows the dev F1 scores: 95.2 for ELMo_BASE, 95.1 for CNN Large + ELMo, and 95.5 for CNN Large + fine-tune. The third column shows the test F1 scores: 95.1 for ELMo_BASE, 95.2 for CNN Large + ELMo, and 95.6 for CNN Large + fine-tune. The table highlights that the CNN Large + fine-tune model achieves the highest F1 scores on both the dev and test sets.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_7.jpg",
        "caption": [],
        "footnote": [
            "Table 5: Different loss functions on the development sets of GLUE (cf. Table 2). Results are based on the CNN base model (Table 1) "
        ],
        "context": "Table 4 We also report parseval F1 for Penn Treebank constituency parsing. We adopted the current state-ofthe-art architecture (Kitaev and Klein, 2018). We again used grid search for learning rates and number of layers in parsing encoder, and used 8E-04 for language model finetuning, 8E-03 for the parsing model parameters, and two layers for encoder. 6.2.2 Constituency Parsing Table 3 shows the results, with comparison to previous published $\\mathrm{ELMo}_{B A S E}$ results (Peters et al., 2018) and the BERT models. Both of our stacking methods outperform the previous state of the art, but fine tuning gives the biggest gain.  Entity Recognition We evaluated span-level F1 performance on the CoNLL 2003 Named Entity Recognition (NER) task, where spans of text must be segmented and labeled as Person, Organization, Location, or Miscellaneous. We adopted the NER architecture in Peters et al. (2018), a biLSTM-CRF, with two minor modifications: (1) instead of two layers of biLSTM, we only used one, and (2) a linear projection layer was added between the token embedding and biLSTM layer. We did grid search on the pairs of learning rate, and found that projection-biLSTMCRF with 1E-03 and pretrained language model with 1E-05 gave us the best result. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-4b3f83319bc6f531992c0907c2ddd16e",
        "description": "The image is a table labeled 'Table 5: Different loss functions on the development sets of GLUE (cf. Table 2)'. The table compares the performance of different models on various tasks from the GLUE benchmark. The rows represent different model configurations: 'cloze', 'bilm', and 'cloze + bilm'. The columns represent different tasks from the GLUE benchmark, including CoLA (mcc), SST-2 (acc), MRPC (F1), STS-B (scc), QQP (F1), MNLI-m (acc), QNLI (acc), RTE (acc), and an average score (Avg). Each cell contains a numerical value representing the performance metric for that task and model configuration. For example, the 'cloze' model achieves a score of 55.1 on the CoLA task, 92.9 on the SST-2 task, and so on. The 'bilm' model scores 50.0 on CoLA, 92.4 on SST-2, etc. The 'cloze + bilm' model combines the two approaches and achieves scores such as 52.6 on CoLA, 93.2 on SST-2, and so forth. The average scores across all tasks are 80.9 for 'cloze', 79.3 for 'bilm', and 80.4 for 'cloze + bilm'.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_8.jpg",
        "caption": [
            "Figure 3: Average GLUE score with different amounts of Common Crawl data for pretraining. "
        ],
        "footnote": [],
        "context": "Figure 3 shows that more training data can significantly increase accuracy. We train all models with Next we investigate how much pretraining benefits from larger training corpora and how the domain of the data influences end-task performance. 6.4 Domain and amount of training data Table 5 shows that the cloze loss performs significantly better than the bilm loss and that combining the two loss types does not improve over the cloze loss by itself. We conjecture that individual left and right context prediction tasks are too different from center word prediction and that their learning signals are not complementary enough.  loss to obtain a triplet loss which trains the model to predict the current word given both left and right context, as well as just right or left context. The latter is much harder than the cloze loss since less context is available and therefore gradients for the bilm loss are much larger: the cloze model achieves perplexity of about 4 while as for the bilm it is 27-30, depending on the direction. This results in the bilm loss dominating the triplet loss and we found that scaling the bilm term by a factor of 0.15 results in better performance. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-9cca33bff05f008d1e9e37238d107278",
        "description": "The image is a line graph titled 'Average GLUE score with different amounts of Common Crawl data for pretraining.' The x-axis represents the amount of training data tokens, ranging from 562M to 18B. The y-axis represents the average GLUE score, ranging from 80 to 81.5. The graph shows a blue line with circular markers indicating the average GLUE scores at different data points. The scores are as follows: 562M tokens have an average GLUE score of approximately 80, 1.1B tokens have a score of about 80.5, 2.25B tokens have a score of around 81, 4.5B tokens have a score of about 81, 9B tokens have a score of approximately 81.5, and 18B tokens have a score of about 81.5. The trend indicates that as the amount of training data increases, the average GLUE score generally improves, with a significant increase between 562M and 1.1B tokens, followed by a more gradual increase thereafter.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/D19-1539/images/image_9.jpg",
        "caption": [],
        "footnote": [
            "Table 6: Effect of different domains and amount of data for pretraining on the on the development sets of GLUE (cf. Table 2). Results are based on the CNN base model (Table 1). "
        ],
        "context": "We also experiment with BooksCorpus (Zhu et al., 2015) as well as English Wikipedia, similar to Devlin et al. (2018). Examples in BooksCorpus are a mix of individual sentences and paragraphs; examples are on average 36 tokens. Wikipedia examples are longer paragraphs of 66 words on average. To reduce the effect of training on examples of different lengths, we adopted the following strategy: we concatenate all training examples into a single string and then crop blocks of 512 consecutive tokens from this string. We train on a batch of these blocks (BWiki - blck). It turns out that this strategy  data. The same table also shows results for News Crawl which contains newswire data. This data generally performs less well than Common Crawl, even on MRPC which is newswire. A likely reason is that News Crawl examples are individual sentences of 23 words on average which compares to several sentences or 50 words on average for Common Crawl. Mutli-sentence training examples are more effective for end-tasks based on sentence pairs, e.g., there is a 14 point accuracy gap on RTE between News Crawl and Common Crawl with 4.5B tokens. More News Crawl data is most beneficial for CoLA and STS-B. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-9cca33bff05f008d1e9e37238d107278",
        "description": "The image is a table labeled 'Table 6: Effect of different domains and amount of data for pretraining on the development sets of GLUE (cf. Table 2). Results are based on the CNN base model (Table 1)'. The table is structured with rows representing different datasets and columns representing various metrics. The datasets include 'ccrawl' with varying amounts of training data in millions of tokens (M tok), 'news crawl', 'BWiki - sent', and 'BWiki - blck'. The metrics evaluated are CoLA (mcc), SST-2 (acc), MRPC (F1), STS-B (scc), QQP (F1), MNLI-m (acc), QNLI (acc), RTE (acc), and an average (Avg). For example, with 562 M tok of ccrawl data, the metrics are as follows: CoLA (52.5), SST-2 (92.9), MRPC (88.2), STS-B (88.3), QQP (87.1), MNLI-m (81.7), QNLI (85.7), RTE (63.3), and Avg (79.9). As the amount of ccrawl data increases to 18000 M tok, the metrics improve to CoLA (56.3), SST-2 (93.1), MRPC (88.0), STS-B (88.8), QQP (87.2), MNLI-m (82.3), QNLI (86.3), RTE (68.4), and Avg (81.3). The news crawl dataset shows lower performance across all metrics compared to ccrawl. For instance, with 562 M tok of news crawl data, the metrics are CoLA (50.9), SST-2 (92.8), MRPC (81.4), STS-B (78.2), QQP (84.9), MNLI-m (79.1), QNLI (82.0), RTE (55.7), and Avg (75.6). The BWiki datasets show similar performance to each other but generally lower than ccrawl. For example, with 3300 M tok of BWiki - sent data, the metrics are CoLA (53.5), SST-2 (91.6), MRPC (86.4), STS-B (86.2), QQP (86.9), MNLI-m (82.3), QNLI (86.9), RTE (63.8), and Avg (79.7).",
        "segmentation": false
    }
}