<?xml version='1.0' encoding='UTF-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d6" for="edge" attr.name="order" attr.type="long"/>
<key id="d5" for="edge" attr.name="source_id" attr.type="string"/>
<key id="d4" for="edge" attr.name="description" attr.type="string"/>
<key id="d3" for="edge" attr.name="weight" attr.type="double"/>
<key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;IMAGE_4&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">"The image is a detailed diagram illustrating the decoding mechanism in a pointer-generator network, a type of sequence-to-sequence attentional model used for text summarization. The diagram is divided into several key components: \n\n1. **Encoder Hidden States**: Represented by a series of vertical bars at the bottom left, these states are denoted as {h_e^1, ..., h_e^N}, indicating the hidden states generated by a bidirectional LSTM encoder from the input words. \n2. **Decoder Hidden State**: Shown as a single vertical bar on the right side, labeled as h_d^t, representing the hidden state of the unidirectional LSTM decoder at step t. \n3. **Context Vector**: Depicted as a single vertical bar in the center, labeled as h*(α̂^t), this vector is generated using the updated word attention α̂^t. \n4. **Word Distribution p_vocab**: Illustrated by a series of vertical bars on the far right, this distribution represents the probability of generating words from a fixed vocabulary. \n5. **Final Word Distribution p_final**: Shown by a series of vertical bars at the top, this distribution combines the word distribution from the vocabulary and the context vector to generate the final output. \n6. **Updated Word Attention α̂^t**: Represented by a series of vertical bars on the left, this attention mechanism updates the context vector based on the current decoder state. \n7. **p_gen**: A scalar value that determines the probability of generating a word from the vocabulary or copying a word from the input. \n8. **1 - p_gen**: Another scalar value that complements p_gen, determining the probability of copying a word from the input. \nThe arrows and connections between these components illustrate the flow of information during the decoding process. The diagram highlights how the model dynamically switches between generating words from a vocabulary and copying words from the input text, depending on the values of p_gen and 1 - p_gen."</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
</node>
<node id="&quot;CONTEXT VECTOR&quot;">
  <data key="d0">"PERSON"</data>
  <data key="d1">"A vector summarizing the context information for the current time step in the decoder, denoted as h*(α^t</data>
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
</node>
<node id="&quot;ENCODER HIDDEN STATES&quot;">
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d1">"Encoder Hidden States是从image_4中提取的实体。"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;DECODER HIDDEN STATE&quot;">
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d1">"Decoder Hidden State是从image_4中提取的实体。"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;UPDATED WORD ATTENTION&quot;">
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d1">"Updated Word Attention是从image_4中提取的实体。"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;WORD DISTRIBUTION P_VOCAB&quot;">
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d1">"Word Distribution p_vocab是从image_4中提取的实体。"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<node id="&quot;FINAL WORD DISTRIBUTION P_FINAL&quot;">
  <data key="d2">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d1">"Final Word Distribution p_final是从image_4中提取的实体。"</data>
  <data key="d0">"UNKNOWN"</data>
</node>
<edge source="&quot;IMAGE_4&quot;" target="&quot;ENCODER HIDDEN STATES&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Encoder Hidden States是从image_4中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;CONTEXT VECTOR&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Context Vector是从image_4中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;DECODER HIDDEN STATE&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Decoder Hidden State是从image_4中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;UPDATED WORD ATTENTION&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Updated Word Attention是从image_4中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;WORD DISTRIBUTION P_VOCAB&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Word Distribution p_vocab是从image_4中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;IMAGE_4&quot;" target="&quot;FINAL WORD DISTRIBUTION P_FINAL&quot;">
  <data key="d3">10.0</data>
  <data key="d4">"Final Word Distribution p_final是从image_4中提取的实体。"</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONTEXT VECTOR&quot;" target="&quot;ENCODER HIDDEN STATES&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The context vector is derived from the encoder hidden states using the attention mechanism."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONTEXT VECTOR&quot;" target="&quot;DECODER HIDDEN STATE&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The context vector influences the decoder hidden state to generate the next word."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;CONTEXT VECTOR&quot;" target="&quot;UPDATED WORD ATTENTION&quot;">
  <data key="d3">9.0</data>
  <data key="d4">"The updated word attention weights are used to compute the context vector."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;DECODER HIDDEN STATE&quot;" target="&quot;WORD DISTRIBUTION P_VOCAB&quot;">
  <data key="d3">6.0</data>
  <data key="d4">"The decoder hidden state contributes to generating the word distribution over the vocabulary."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;UPDATED WORD ATTENTION&quot;" target="&quot;FINAL WORD DISTRIBUTION P_FINAL&quot;">
  <data key="d3">7.0</data>
  <data key="d4">"The updated word attention affects the final word distribution through the generation probability p_gen."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
<edge source="&quot;WORD DISTRIBUTION P_VOCAB&quot;" target="&quot;FINAL WORD DISTRIBUTION P_FINAL&quot;">
  <data key="d3">8.0</data>
  <data key="d4">"The word distribution over the vocabulary is combined with the attention-based distribution to form the final word distribution."</data>
  <data key="d5">./fusion_research/fusion_dataset/paper/P18-1013/images/image_4.jpg</data>
  <data key="d6">1</data>
</edge>
</graph></graphml>
