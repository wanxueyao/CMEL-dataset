{
    "image_1": [
        {
            "merged_entity_name": "Neural Machine Translation",
            "entity_type": "EVENT",
            "description": "Neural Machine Translation (NMT) is the event or process being studied in the paper, focusing on the utility of pre-trained word embeddings for improving performance in low-resource scenarios. It involves the use of pre-trained word embeddings in neural network models for NLP tasks and explores their effectiveness in NMT tasks through various experiments.",
            "source_image_entities": [
                "DATASET",
                "TRAIN",
                "DEV",
                "TEST"
            ],
            "source_text_entities": [
                "NEURAL MACHINE TRANSLATION"
            ]
        },
        {
            "merged_entity_name": "Galician to English Translation",
            "entity_type": "GEO",
            "description": "Translation task from Galician to English, which is part of the low-resource language pairs used in the NMT experiments. Galician is a language that showed significant gains in BLEU scores with pre-trained word embeddings, indicating its status as a low-resource language.",
            "source_image_entities": [
                "GL → EN"
            ],
            "source_text_entities": [
                "GALICIAN",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Portuguese to English Translation",
            "entity_type": "GEO",
            "description": "Translation task from Portuguese to English, included in the parallel corpus created from TED talks transcripts for the NMT experiments. Portuguese is paired with Galician for the NMT experiments, and the language pair represents different language families, allowing for comparison across languages with different characteristics.",
            "source_image_entities": [
                "PT → EN"
            ],
            "source_text_entities": [
                "PORTUGUESE",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Azerbaijani to English Translation",
            "entity_type": "GEO",
            "description": "Translation task from Azerbaijani to English, which is part of the low-resource language pairs used in the NMT experiments. Azerbaijani is paired with Turkish, and the languages in the pair are similar in vocabulary, grammar, and sentence structure, controlling for language characteristics and improving the possibility of transfer learning in multi-lingual models.",
            "source_image_entities": [
                "AZ → EN"
            ],
            "source_text_entities": [
                "AZERBAIJANI",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Turkish to English Translation",
            "entity_type": "GEO",
            "description": "Translation task from Turkish to English, included in the parallel corpus created from TED talks transcripts for the NMT experiments. Turkish is paired with Azerbaijani for the NMT experiments, and the language pair represents different language families, allowing for comparison across languages with different characteristics.",
            "source_image_entities": [
                "TR → EN"
            ],
            "source_text_entities": [
                "TURKISH",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Belarusian to English Translation",
            "entity_type": "GEO",
            "description": "Translation task from Belarusian to English, which is part of the low-resource language pairs used in the NMT experiments. Belarusian is paired with Russian, and the languages in the pair are similar in vocabulary, grammar, and sentence structure, controlling for language characteristics and improving the possibility of transfer learning in multi-lingual models.",
            "source_image_entities": [
                "BE → EN"
            ],
            "source_text_entities": [
                "BELARUSIAN",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Russian to English Translation",
            "entity_type": "GEO",
            "description": "Translation task from Russian to English, included in the parallel corpus created from TED talks transcripts for the NMT experiments. Russian is paired with Belarusian for the NMT experiments, and the language pair represents different language families, allowing for comparison across languages with different characteristics.",
            "source_image_entities": [
                "RU → EN"
            ],
            "source_text_entities": [
                "RUSSIAN",
                "ENGLISH"
            ]
        }
    ],
    "image_2": [
        {
            "merged_entity_name": "Galician to English Translation",
            "entity_type": "EVENT",
            "description": "Translation event from Galician to English, indicated by a significant gain of up to 11 BLEU points with pre-trained word embeddings, showing its utility in low-resource language pairs.",
            "source_image_entities": [
                "GL → EN"
            ],
            "source_text_entities": [
                "GALICIAN",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Portuguese to English Translation",
            "entity_type": "EVENT",
            "description": "Translation event from Portuguese to English, part of the parallel corpus used in the NMT experiments, and showing consistent gains in BLEU scores with pre-trained embeddings.",
            "source_image_entities": [
                "PT → EN"
            ],
            "source_text_entities": [
                "PORTUGUESE",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Azerbaijani to English Translation",
            "entity_type": "EVENT",
            "description": "Translation event from Azerbaijani to English, included in the low-resource language pair used in the NMT experiments, with gains in BLEU scores being either small or large depending on the baseline system's performance.",
            "source_image_entities": [
                "AZ → EN"
            ],
            "source_text_entities": [
                "AZERBAIJANI",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Turkish to English Translation",
            "entity_type": "EVENT",
            "description": "Translation event from Turkish to English, included in the parallel corpus, paired with Azerbaijani for the NMT experiments.",
            "source_image_entities": [
                "TR → EN"
            ],
            "source_text_entities": [
                "TURKISH",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Belarusian to English Translation",
            "entity_type": "EVENT",
            "description": "Translation event from Belarusian to English, included in the low-resource language pair used in the NMT experiments, with gains in BLEU scores being either small or large depending on the baseline system's performance.",
            "source_image_entities": [
                "BE → EN"
            ],
            "source_text_entities": [
                "BELARUSIAN",
                "ENGLISH"
            ]
        },
        {
            "merged_entity_name": "Russian to English Translation",
            "entity_type": "EVENT",
            "description": "Translation event from Russian to English, included in the parallel corpus, paired with Belarusian for the NMT experiments, and showing larger accuracy gains from pre-training due to low baseline BLEU scores.",
            "source_image_entities": [
                "RU → EN"
            ],
            "source_text_entities": [
                "RUSSIAN",
                "ENGLISH"
            ]
        }
    ],
    "image_3": [
        {
            "merged_entity_name": "BLEU Score Analysis",
            "entity_type": "EVENT",
            "description": "A line graph showing the BLEU scores for different language translations as a function of training set size, including Portuguese to English (Pt→En), Turkish to English (Tr→En), and Russian to English (Ru→En), with each language having two lines representing standard (std) and pre-trained (pre) models. The results demonstrate the efficacy of pre-trained word embeddings in Neural Machine Translation (NMT), particularly in source languages, and the gains in BLEU scores are more significant with pre-trained source language embeddings.",
            "source_image_entities": [
                "GRAPH 1",
                "GRAPH 2"
            ],
            "source_text_entities": [
                "BLEU SCORE"
            ]
        },
        {
            "merged_entity_name": "Neural Machine Translation",
            "entity_type": "CONCEPT",
            "description": "Neural Machine Translation (NMT) is the focus of the research discussed in the text, which involves the use of pre-trained word embeddings to improve translation quality. The paper examines the efficacy of pre-trained word embeddings across various languages and their impact on BLEU scores, which measure the quality of machine translation.",
            "source_image_entities": [],
            "source_text_entities": [
                "NEURAL MACHINE TRANSLATION",
                "NMT"
            ]
        },
        {
            "merged_entity_name": "Pre-trained Word Embeddings",
            "entity_type": "CONCEPT",
            "description": "Pre-trained word embeddings refer to the use of word embeddings that have been trained prior to being used in a specific task, such as Neural Machine Translation. The paper discusses the utility of these embeddings, showing that pre-training the word embeddings in the source and/or target languages helps to increase the BLEU scores to some degree.",
            "source_image_entities": [],
            "source_text_entities": [
                "PRE-TRAINED EMBEDDINGS"
            ]
        }
    ],
    "image_4": [
        {
            "entity_name": "DATASET",
            "entity_type": "EVENT",
            "description": "A table comparing different language translation datasets and their performance metrics, specifically focusing on translations to Portuguese (PT), including the effect of pre-training on BLEU score over six languages and the impact of linguistic similarity and pre-training on BLEU scores.",
            "source_image_entities": [
                "DATASET"
            ],
            "source_text_entities": [
                "NEURAL MACHINE TRANSLATION",
                "BLEU SCORE",
                "PORTUGUESE"
            ]
        },
        {
            "entity_name": "LANG. FAMILY",
            "entity_type": "ORGANIZATION",
            "description": "The linguistic family of the languages being compared in the dataset, including West-Iberian, Western Romance, Romance, Indo-European, and a pair with no common linguistic family, with a specific focus on the Portuguese language and its linguistic relatives.",
            "source_image_entities": [
                "LANG. FAMILY"
            ],
            "source_text_entities": [
                "INDO-EUROPEAN",
                "ROMANCE",
                "WESTERN ROMANCE",
                "WEST-IBERIAN",
                "PORTUGUESE"
            ]
        },
        {
            "entity_name": "STD",
            "entity_type": "PERSON",
            "description": "Standard deviation metric used to evaluate the performance of the language translation models, specifically in the context of Neural Machine Translation and the efficacy of pre-trained word embeddings.",
            "source_image_entities": [
                "STD"
            ],
            "source_text_entities": [
                "BLEU SCORE"
            ]
        },
        {
            "entity_name": "PRE",
            "entity_type": "PERSON",
            "description": "Precision metric used to evaluate the performance of the language translation models, showing improvements over the standard deviation for each language pair, and particularly relevant to the discussion on pre-trained word embeddings in Neural Machine Translation.",
            "source_image_entities": [
                "PRE"
            ],
            "source_text_entities": [
                "BLEU SCORE",
                "NMT"
            ]
        }
    ],
    "image_5": [
        {
            "entity_name": "DATASET",
            "entity_type": "ORGANIZATION",
            "description": "A structured table displaying translation performance metrics between different language pairs, specifically from various languages to English (EN), with columns for 'unaligned' and 'aligned' conditions, as referenced in the context of pre-trained word embeddings' efficacy.",
            "source_image_entities": [
                "DATASET"
            ],
            "source_text_entities": [
                "TABLE 4",
                "TABLE 5",
                "TABLE 6",
                "TABLE 7"
            ]
        },
        {
            "entity_name": "GL → EN",
            "entity_type": "EVENT",
            "description": "Translation event from Galician (GL) to English (EN), showing a score of 12.8 in the unaligned condition and 11.5 in the aligned condition, with a decrease of 1.3, as highlighted in the analysis of the BLEU score and its correlation with word embedding alignment.",
            "source_image_entities": [
                "GL → EN"
            ],
            "source_text_entities": [
                "GALICIAN",
                "ENGLISH",
                "BLEU SCORE",
                "TABLE 4"
            ]
        },
        {
            "entity_name": "PT → EN",
            "entity_type": "EVENT",
            "description": "Translation event from Portuguese (PT) to English (EN), showing a score of 30.8 in the unaligned condition and 30.6 in the aligned condition, with a decrease of 0.2, as noted in the discussion on the effect of pre-training on multilingual translation into English.",
            "source_image_entities": [
                "PT → EN"
            ],
            "source_text_entities": [
                "PORTUGUESE",
                "ENGLISH",
                "TABLE 5",
                "FIGURE 2"
            ]
        },
        {
            "entity_name": "AZ → EN",
            "entity_type": "EVENT",
            "description": "Translation event from Azerbaijani (AZ) to English (EN), showing a score of 2.0 in the unaligned condition and 2.1 in the aligned condition, with an increase of 0.1, as part of the study on low-resource languages and their translation performance.",
            "source_image_entities": [
                "AZ → EN"
            ],
            "source_text_entities": [
                "AZ",
                "ENGLISH",
                "TABLE 4",
                "LOW-RESOURCE LANGUAGES"
            ]
        },
        {
            "entity_name": "TR → EN",
            "entity_type": "EVENT",
            "description": "Translation event from Turkish (TR) to English (EN), showing a score of 17.9 in the unaligned condition and 17.7 in the aligned condition, with a decrease of 0.2, as discussed in the context of the BLEU score and its implications for translation quality.",
            "source_image_entities": [
                "TR → EN"
            ],
            "source_text_entities": [
                "TABLE 4",
                "BLEU SCORE",
                "ENGLISH"
            ]
        },
        {
            "entity_name": "BE → EN",
            "entity_type": "EVENT",
            "description": "Translation event from Belarusian (BE) to English (EN), showing a score of 3.0 in both unaligned and aligned conditions, with no change, as analyzed in the context of the effect of pre-training on multilingual translation systems.",
            "source_image_entities": [
                "BE → EN"
            ],
            "source_text_entities": [
                "BELARUSIAN",
                "ENGLISH",
                "TABLE 4",
                "TABLE 5"
            ]
        },
        {
            "entity_name": "RU → EN",
            "entity_type": "EVENT",
            "description": "Translation event from Russian (RU) to English (EN), showing a score of 21.1 in the unaligned condition and 21.4 in the aligned condition, with an increase of 0.3, as discussed in the context of the BLEU score and its implications for translation quality.",
            "source_image_entities": [
                "RU → EN"
            ],
            "source_text_entities": [
                "RUSSIAN",
                "ENGLISH",
                "TABLE 4",
                "BLEU SCORE"
            ]
        }
    ],
    "image_6": [
        {
            "entity_name": "GL/PT",
            "entity_type": "GEO",
            "description": "GL/PT represents the language pair of Galician (GL) to Portuguese (PT), noted for their high similarity and used in the study to investigate the effect of linguistic similarity on pre-trained embeddings. The similarity of GL/PT is the highest among the language pairs considered, and they showed the largest gains in BLEU scores when applying pre-trained embeddings.",
            "source_image_entities": [
                "GL + PT"
            ],
            "source_text_entities": [
                "GL/PT"
            ]
        },
        {
            "entity_name": "BE/RU",
            "entity_type": "GEO",
            "description": "BE/RU represents the language pair of Belarusian (BE) to Russian (RU), noted for their low similarity. The gains in BLEU scores when applying pre-trained embeddings are smaller for this language pair compared to others, likely due to the partial mutual intelligibility and rich morphology of Slavic languages.",
            "source_image_entities": [
                "BE + RU"
            ],
            "source_text_entities": [
                "BE/RU"
            ]
        },
        {
            "entity_name": "BLEU Score",
            "entity_type": "EVENT",
            "description": "BLEU score is an evaluation metric used to assess the quality of machine translation, referenced in the context of pre-trained word embeddings' efficacy. It is used to measure the correlation between word embedding alignment and the quality of bilingual translation tasks, as presented in Table 4, and to evaluate the quality of machine translation in the context of pre-trained embeddings.",
            "source_image_entities": [
                "BI",
                "STD",
                "PRE",
                "ALIGN"
            ],
            "source_text_entities": [
                "BLEU SCORE"
            ]
        },
        {
            "entity_name": "Pre-trained Embeddings",
            "entity_type": "CONCEPT",
            "description": "Pre-trained embeddings refer to the use of word embeddings that have been trained prior to being used in a specific task, such as Neural Machine Translation (NMT). They are shown to help capture rarer vocabulary and generate sentences that are more grammatically well-formed in low-resource languages, as demonstrated in Table 6.",
            "source_image_entities": [
                "PRE"
            ],
            "source_text_entities": [
                "PRE-TRAINED EMBEDDINGS"
            ]
        },
        {
            "entity_name": "Smith et al. (2017)",
            "entity_type": "ORGANIZATION",
            "description": "Smith et al. (2017) proposed an approach to learn orthogonal transformations for aligning word embeddings of multiple languages into a single space, which is a method used in the context of pre-training word embeddings. Their approach is mentioned as a way to convert the word embeddings of multiple languages to a single space, used in the study to learn reasonable projection of word embeddings during the normal training process.",
            "source_image_entities": [],
            "source_text_entities": [
                "SMITH ET AL., 2017",
                "SMITH ET AL. (2017)"
            ]
        }
    ],
    "image_7": [
        {
            "entity_name": "Chris",
            "entity_type": "PERSON",
            "description": "A lawyer who is described as brilliant but lacks knowledge in patent law and genetics, successfully translates a person’s name in the context of pre-trained embeddings in low-resource languages.",
            "source_image_entities": [
                "CHRIS",
                "LAWYER"
            ],
            "source_text_entities": [
                "\"TABLE 6\""
            ]
        },
        {
            "entity_name": "Patent Law and Genetics",
            "entity_type": "EVENT",
            "description": "The legal framework concerning patents and the scientific study of genes and heredity, areas where Chris has little knowledge, are also less frequent concepts that pre-trained embeddings help represent better in low-resource languages.",
            "source_image_entities": [
                "PATENT LAW",
                "GENETICS"
            ],
            "source_text_entities": [
                "\"TABLE 6\""
            ]
        }
    ],
    "image_8": [
        {
            "entity_name": "BLEU Score",
            "entity_type": "EVENT",
            "description": "BLEU score is an evaluation metric used to assess the quality of machine translation, referenced in the context of pre-trained word embeddings' efficacy and used in bilingual and multilingual translation tasks.",
            "source_image_entities": [
                "BI:STD",
                "BI:PRE",
                "MULTI:STD",
                "MULTI:PRE+ALIGN"
            ],
            "source_text_entities": [
                "BLEU SCORE"
            ]
        },
        {
            "entity_name": "Smith et al. (2017)",
            "entity_type": "ORGANIZATION",
            "description": "Smith et al. (2017) proposed an approach to learn orthogonal transformations for aligning word embeddings of multiple languages into a single space, as referenced in the context of pre-trained embeddings and their impact on translation quality.",
            "source_image_entities": [],
            "source_text_entities": [
                "SMITH ET AL., 2017",
                "SMITH ET AL. (2017)"
            ]
        },
        {
            "entity_name": "NMT",
            "entity_type": "CONCEPT",
            "description": "NMT stands for Neural Machine Translation, a concept central to the discussion of pre-trained embeddings and their impact on translation quality, including the examination of pre-training word embeddings in NMT from multiple angles.",
            "source_image_entities": [],
            "source_text_entities": [
                "NMT",
                "NMT"
            ]
        },
        {
            "entity_name": "Pre-trained Embeddings",
            "entity_type": "CONCEPT",
            "description": "Pre-trained embeddings refer to the use of word embeddings that have been trained prior to being used in a specific task, such as in Neural Machine Translation systems, and their impact on the quality of machine translation.",
            "source_image_entities": [],
            "source_text_entities": [
                "PRE-TRAINED EMBEDDINGS"
            ]
        }
    ],
    "image_9": [
        {
            "entity_name": "F-measure in NMT",
            "entity_type": "CONCEPT",
            "description": "F-measure is a measure of a test's accuracy, specifically in the context of machine learning and information retrieval, used to evaluate the performance of different methods in Neural Machine Translation (NMT) as shown in a bar chart comparing 'std' and 'pre' methods across different frequency ranges in a training corpus.",
            "source_image_entities": [
                "F-MEASURE",
                "GRAPH",
                "FREQUENCY IN TRAINING CORPUS"
            ],
            "source_text_entities": [
                "F-MEASURE"
            ]
        },
        {
            "entity_name": "Pre-trained Embeddings in NMT",
            "entity_type": "CONCEPT",
            "description": "Pre-trained embeddings refer to the use of word embeddings that have been trained prior to being used in a specific task, such as in Neural Machine Translation (NMT), where they show benefits in training and accuracy, particularly for low-frequency words and function words.",
            "source_image_entities": [],
            "source_text_entities": [
                "PRE-TRAINED EMBEDDINGS",
                "NMT"
            ]
        },
        {
            "entity_name": "BLEU Score in NMT",
            "entity_type": "CONCEPT",
            "description": "BLEU score is a metric used to evaluate the quality of machine translation, referenced in the context of pre-trained word embeddings' efficacy in Neural Machine Translation (NMT), and is also presented in Table 4 to show the correlation between word embedding alignment and BLEU score in bilingual translation tasks.",
            "source_image_entities": [],
            "source_text_entities": [
                "BLEU SCORE"
            ]
        }
    ]
}