{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_1.jpg",
        "caption": [
            "Figure 1: Linked WikiText-2 Example. A localized knowledge graph containing facts that are (possibly) conveyed in the sentence above. The graph is built by iteratively linking each detected entity to Wikidata, then adding any relations to previously mentioned entities. Note that not all entities are connected, potentially due to missing relations in Wikidata. "
        ],
        "footnote": [],
        "context": "training. For instance, when conditioned on the text at the top of Figure 1, an AWD-LSTM language model (Merity et al., 2018) trained on Wikitext-2 assigns higher probability to the word “PlayStation” than “Game Boy”, even though this sentence appears verbatim in the training data. This is not surprising—existing models represent the distribution over the entire vocabulary directly, whether they are common words, references to real world entities, or factual information like dates and numbers. As a result, language models are unable to generate factually correct sentences, do not generalize to rare/unseen entities, and often omit rare tokens from the consistent with the world they describe. Although language models are quite skilled at generating grammatical sentences, and previous work has shown that language models also possess some degree of common-sense reasoning and basic knowledge (Vinyals and Le, 2015; Serban et al., 2016; Trinh and Le, 2019), their ability to generate factually correct text is quite limited. The clearest limitation of existing language models is that they, at best, can only memorize facts observed during [Super Mario Land] is a [1989] [side-scrolling] [platform video game] developed and published by [Nintendo] as a [launch title] for their [Game Boy] [handheld game console].  ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-04cdbdac133e6bf24d195a81fabc28a2",
        "description": "The image is a flowchart representing the relationships and attributes of the video game 'Super Mario Land'. The chart includes several nodes connected by arrows, each node representing a different attribute or entity related to the game. The main nodes are as follows: 'Super Mario Land' (Q647249) is connected to 'Nintendo' (Q8093) as the publisher, 'platform game' (Q828322) as the genre, 'Game Boy' (Q186437) as the platform, and 'launch game' (Q1425505) as the launch title. The publication date is indicated as '21 April 1989'. Additionally, 'Super Mario Land' is categorized as a 'side-scrolling video game' (Q2281714). The chart uses different colors for each node: green for 'Super Mario Land', orange for 'Nintendo', pink for 'platform game', brown for 'Game Boy', blue for 'launch game', and purple for 'side-scrolling video game'. The arrows indicate the relationships between these entities, such as 'PUBLISHER', 'GENRE', 'PLATFORM', 'MANUFACTURER', and 'INSTANCE OF'.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_2.jpg",
        "caption": [
            "Figure 2: KGLM Illustration. When trying to generate the token following “published by”, the model first decides the type of the mention $(t_{t})$ to be a related entity (darker indicates higher probability), followed by identifying the parent $(p_{t})$ , relation $\\left({{r}_{t}}\\right)$ , and entity to render $(e_{t})$ from the local knowledge graph as (Super Mario Land, Publisher, Nintendo). The final distribution over the words includes the standard vocabulary along with aliases of Nintendo, and the model selects “Nintendo” as the token $x_{t}$ . Facts related to Nintendo will be added to the local graph. "
        ],
        "footnote": [],
        "context": "• Decide the type of $x_{t}$ , which we denote by $t_{t}$ : whether it is a reference to an entity in $\\kappa g_{<t}$ (related), a reference to an entity not in $\\kappa\\mathcal{G}_{<t}$ (new), or not an entity mention $(\\varnothing)$ . • If $t_{t}=$ new then choose the upcoming entity $e_{t}$ from the set of all entities $\\mathcal{E}$ . • If $t_{t}=$ related then: – Choose a parent entity $p_{t}$ from $\\mathcal{E}_{<t}$ .– Choose a factual relation $r_{t}$ to render, $r_{t}\\in\\{(p,r,e)\\in\\mathcal{K}\\mathcal{G}_{<t}|p=p_{t}\\}.$ .– Choose $e_{t}$ as one of the tail entities, $e_{t}\\in\\{e|(p_{t},r_{t},e)\\in\\mathcal{K G}_{<t}\\}.$ . • If $t_{t}=\\emptyset$ then $e_{t}=\\emptyset$ . • that have appeared in the context already, KGLM will maintain a local knowledge graph containing all facts involving entities that have appeared in the context. As the model decides to refer to entities that have not been referred to yet, it will grow the local knowledge graph with additional entities and facts to reflect the new entity. Formally, we will compute $p(x_{t},\\mathcal{E}_{t}|x_{<t},\\mathcal{E}_{<t})$ where $x_{<t}$ is the sequence of observed tokens, $\\mathcal{E}_{<t}$ is the set of entities mentioned in $\\scriptstyle x_{<t}$ , and $\\kappa g_{<t}$ is the local knowledge graph determined by $\\mathcal{E}_{<t}$ , as described above. The generative process is: ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-3ded7fc44093a7d42cdcd18d1e6016b3",
        "description": "The image is a flowchart illustrating the process of generating tokens in the context of a knowledge graph language model (KGLM). The flowchart is divided into several sections, each representing a step in the token generation process. The first section, labeled 'Relation to Existing Entity', shows a decision point where the model decides if the token is related to an existing entity. This is indicated by a darker shade, suggesting a higher probability. The next section, 'parent from local entities', lists potential parent entities such as 'platform game' and 'side-scrolling game', with 'Super Mario Land' highlighted. The third section, 'pick from all entities', shows a list of entities including 'AAA Inc.', 'Sony Inc.', and 'Zzyzx, CA'. The fourth section, 'Distribution over standard vocabulary and aliases of e_t', includes a list of words like 'a', 'the', 'company', 'dog', etc., along with aliases for 'Nintendo' such as 'Kabushiki', 'Koppai', and 'Nintendo'. The final section, 'Not an Entity Mention', indicates that if the token is not an entity mention, the distribution is over the standard vocabulary. The flowchart also includes a graphical representation of the relationships between entities, showing 'Super Mario Land' as the parent entity, 'Publisher' as the relation, and 'Nintendo' as the entity to render. The final distribution over words includes the standard vocabulary along with aliases of 'Nintendo', and the model selects 'Nintendo' as the token x_t. Facts related to Nintendo will be added to the local graph.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 1: Example Annotation of the sentence from Figure 1, including corresponding variables from Figure 2. Note that Game Boy has multiple parent and relation annotations, as the platform for Super Mario Land and as manufactured by Nintendo. Wikidata identifiers are made human-readable (e.g., SML is Q647249) for clarity. "
        ],
        "context": "Expanding the annotations Since there may be entities that were missed in the initial set, as well as non-entity tokens of interest such as dates and quantities we further expand the entity annotations using string matching. For entities, we match the set of aliases provided in Wikidata. For dates, we create an exhaustive list of all of the possible ways of expressing the date (e.g. \"December 7, 1941\", \"7-12-1941\", \"1941\", ...). We perform a similar approach for quantities, using the pint library in Python to handle the different ways of expressing units (e.g. \"g\", \"gram\", ...). Since there are many  missed by the linker. Local knowledge graph The next step iteratively creates a generative story for the entities using relations in the knowledge graph as well as identifies new entities. To do this, we process the text token by token. Each time an entity is encountered, we add all of the related entities in Wikidata as candidates for matching. If one of these related entities is seen later in the document, we identify the entity as a parent for the later entity. Since multiple relations may appear as explanations for each token, we allow a token to have multiple facts. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-2f06c69af9c4d4b92c28cab88c6fe605",
        "description": "The image is a table that provides an example annotation of a sentence from Figure 1, including corresponding variables from Figure 2. The table is structured with rows and columns representing different aspects of the annotation process. The columns include 'Tokens', 'Mention type', 'Entity Mentioned', 'Relation', and 'Parent Entity'. Each row corresponds to a token in the sentence. For example, the token 'Super' is marked as 'new' in the 'Mention type' column and has 'SML' (Q647249) as its 'Entity Mentioned'. The date '1989' is linked to '04-21-1989' and has a 'pub date' relation. The term 'side-scrolling' is marked as 'new' and has 'SIDE_SCROLL' as its 'Entity Mentioned'. The entity 'Game Boy' has multiple parent and relation annotations, indicating it is both the platform for Super Mario Land and manufactured by Nintendo. The table uses human-readable identifiers for clarity, such as 'SML' for Q647249. The context explains the expansion of annotations using string matching for entities, dates, and quantities, and the iterative creation of a generative story for entities using relations in the knowledge graph.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_4.jpg",
        "caption": [
            "Table 2: Linked WikiText-2 Corpus Statistics. "
        ],
        "footnote": [],
        "context": "Dataset Statistics Statistics for Linked WikiText-2 are provided in Table 2. In this corpus, more than $10\\%$ of the tokens are considered entity tokens, i.e. they are generated as factual references to information in the knowledge graph. Each entity is only mentioned a few times (less than 5 on average, with a long tail), and with more than thousand different relations. Thus it is clear that regular language models Even with these omissions and mistakes, it is clear that the annotations are rich and detailed, with a high coverage, and thus should prove beneficial for training knowledge graph language models. of the knowledge graph language model (KGLM). The entity mentioned for most tokens here are human-provided links, apart from “1989” that is linked to 04-21-1989 by the string matching process. The annotations indicate which of the entities are new and related based on whether they are reachable by entities linked so far, clearly making a mistake for side-scrolling game and platform video game due to missing links in Wikidata. Finally, multiple plausible reasons for Game Boy are included: it’s the platform for Super Mario Land and it is manufactured by Nintendo, even though only the former is more relevant here. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-2f06c69af9c4d4b92c28cab88c6fe605",
        "description": "The image is a table labeled 'Table 2: Linked WikiText-2 Corpus Statistics' that provides statistical information about the Linked WikiText-2 dataset. The table is structured with three main columns: Train, Dev, and Test. Each column represents a partition of the dataset and contains the following rows: 'Documents' (Train: 600, Dev: 60, Test: 60), 'Tokens' (Train: 2,019,195, Dev: 207,982, Test: 236,062), 'Vocab. Size' (Train: 33,558, Dev and Test: not provided), 'Mention Tokens' (Train: 207,803, Dev: 21,226, Test: 24,441), 'Mention Spans' (Train: 122,983, Dev: 12,214, Test: 15,007), 'Unique Entities' (Train: 41,058, Dev: 5,415, Test: 5,625), and 'Unique Relations' (Train: 1,291, Dev: 484, Test: 504). The table highlights the dataset's significant size and the detailed annotations across its splits. The caption indicates the table summarizes corpus statistics, while the context discusses the importance of entity tokens, relations, and knowledge graph annotations in training language models.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_5.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Perplexity Results on Linked WikiText-2. Results for models marked with \\* are obtained using importance sampling. "
        ],
        "context": "Fact Completion Since factual text generation is our primary objective, we evaluate the ability of language models to complete sentences with factual We present the model perplexities in Table 3. To marginalize over annotations, perplexities for the ENTITYNLM, EntityCopyNet, and KGLM are estimated using the importance sampling approach described in Section 4. We observe that the KGLM attains substantially lower perplexity than the other entity-based language models (44.1 vs. 76.1/85.4), providing strong evidence that leveraging knowledge graphs is crucial for accurate language modeling. Furthermore, KGLM significantly outperforms all models in unknown penalized perplexity, demonstrating its ability to generate rare tokens.  correct entity and copies the correct alias token with high probability, other models can attain better perplexity by assigning a higher probability to UNK. Accordingly, we also measure unknown penalized perplexity (UPP) (a.k.a adjusted perplexity) introduced by Ueberla (1994), and used recently by Ahn et al. (2016) and Spithourakis and Riedel (2018). This metric penalizes the probability of UNK tokens by evenly dividing their probability mass over $\\mathcal{U}$ , the set of tokens that get mapped to UNK . We can be compute UPP by replacing $p(\\mathrm{UNK})$ in the perplexity above by $\\textstyle{\\frac{1}{|\\mathcal{U}|}}p(\\operatorname{UNK})$ where $|\\mathcal{U}|$ is estimated from the data. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-b7ebeb4e41946712146a875868639d42",
        "description": "The image is a table labeled 'Table 3: Perplexity Results on Linked WikiText-2.' The table presents the perplexity (PPL) and unknown penalized perplexity (UPP) results for different language models. The rows of the table are as follows: ENTITYNLM* (Ji et al., 2017) with PPL of 85.4 and UPP of 189.2, EntityCopyNet* with PPL of 76.1 and UPP of 144.0, AWD-LSTM (Merity et al., 2018) with PPL of 74.8 and UPP of 165.8, and KGLM* with PPL of 44.1 and UPP of 88.5. The footnote indicates that results for models marked with * are obtained using importance sampling. The context explains that the KGLM model achieves substantially lower perplexity than other entity-based language models, providing strong evidence that leveraging knowledge graphs is crucial for accurate language modeling.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_6.jpg",
        "caption": [],
        "footnote": [
            "Table 4: Fact Completion. Top- $k$ accuracy $(@1/@5,\\%)$ for predicting the next token for an incomplete factual sentence. See examples in Table 5. "
        ],
        "context": "Amongst models trained on the same data, both KGLM variants significantly outperform AWDLSTM; they produce accurate facts, while AWDLSTM produced generic, common words. KGLMs are also competitive with models trained on orders of magnitude more data, producing factual completions that require specific knowledge, such as birthplaces, dates, and authors. However, they Table 4 presents performance of each language model on the relations. The oracle KGLM is given the correct entity annotation for $X$ , while the NEL KGLM uses the discriminative model used for importance sampling combined with the NEL entity linker to produce an entity annotation for $X$ .  is our primary objective, we evaluate the ability of language models to complete sentences with factual information. We additionally compare with the small GPT-2 (Radford et al., 2019), a language model trained on a much larger corpus of text. We select 6 popular relations from Freebase, and write a simple completion template for each, such as “ $X$ was born in ” for the birthplace relation. We generate sentences for these templates for a number of $(X,Y)$ pairs for which the relation holds, and manually examine the first token generated by each language model to determine whether it is correct. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-b7ebeb4e41946712146a875868639d42",
        "description": "The image is a table labeled 'Table 4: Fact Completion' that provides the top-k accuracy (@1/@5, %) for predicting the next token for an incomplete factual sentence. The table is structured with four main columns: AWD-LSTM, GPT-2, KGLM Oracle, and KGLM NEL. Each column represents a different language model and contains the following rows: 'nation-capital' (AWD-LSTM: 0/0, GPT-2: 6/7, KGLM Oracle: 0/0, KGLM NEL: 0/4), 'birthloc' (AWD-LSTM: 0/9, GPT-2: 14/14, KGLM Oracle: 94/95, KGLM NEL: 85/92), 'birthdate' (AWD-LSTM: 0/25, GPT-2: 8/9, KGLM Oracle: 65/68, KGLM NEL: 61/67), 'spouse' (AWD-LSTM: 0/0, GPT-2: 2/3, KGLM Oracle: 2/2, KGLM NEL: 1/19), 'city-state' (AWD-LSTM: 0/13, GPT-2: 62/62, KGLM Oracle: 9/59, KGLM NEL: 4/59), 'book-author' (AWD-LSTM: 0/2, GPT-2: 0/0, KGLM Oracle: 61/62, KGLM NEL: 25/28). The table highlights the performance of each language model across various relations, with KGLM variants significantly outperforming AWD-LSTM in most cases. The average accuracy is also provided at the bottom of the table (AWD-LSTM: 0.0/8.2, GPT-2: 15.3/15.8, KGLM Oracle: 38.5/47.7, KGLM NEL: 29.3/44.8).",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1598/images/image_7.jpg",
        "caption": [],
        "footnote": [
            "Table 5: Completion Examples. Examples of fact completion by KGLM and GPT-2, which has been trained on a much larger corpus. GPT-2 tends to produce very common and general tokens, such as one of a few popular cities to follow “born in”. KGLM sometimes makes mistakes in linking to the appropriate fact in the KG, however, the generated facts are more specific and contain rare tokens. We omit AWD-LSTM from this figure as it rarely produced tokens apart from the generic “the” or $\\acute{a}$ , or “ $\\mathbf{\\dot{\\rho}}\\langle U N K\\rangle^{\\circ}$ ."
        ],
        "context": "Effect of changing the KG For most language models, it is difficult to control their generation since factual knowledge is entangled with generation capabilities of the model. For KGLM, an additional benefit of its use of an external source of knowledge is that KGLM is directly controllable via modifications to the KG. To illustrate this capability with a simple example, we create completion of “Barack Obama was born on ” with the original fact (Barack Obama, birthDate, 1961- 08-04), resulting in the top three decoded tokens as “August”, “4”, “1961”. After changing the birth date to 2013-03-21, the top three trained on 600 documents, and the recent state-of-the-art language model, GPT-2, trained on the WebText corpus with over 8 million documents (Radford et al., 2019). For examples that both models get factually correct or incorrect, the generated tokens by KGLM are often much more specific, as opposed to selection of more popular/generic tokens (GPT-2 often predicts “New York” as the birthplace, even for popular entities). KGLM, in particular, gets factual statements correct when the head or tail entities are rare, while GPT-2 can only complete facts for more-popular entities while using more-generic tokens (such as “January” instead of $^{\\bullet}2O^{\\bullet}$ ). ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-b7ebeb4e41946712146a875868639d42",
        "description": "The image is a table labeled 'Table 5: Completion Examples' that provides examples of fact completion by KGLM and GPT-2. The table is structured with four main columns: Input Sentence, Gold (the correct answer), GPT-2 (the generated answer by GPT-2), and KGLM (the generated answer by KGLM). Each row represents a different input sentence and its corresponding answers. The rows are categorized into three groups: Both correct, KGLM correct, GPTv2 correct, and Both incorrect. Here are the detailed entries:\\n\\n1. **Both correct**\\n   - Input Sentence: Paris Hilton was born in __\\n     - Gold: New York City\\n     - GPT-2: New\\n     - KGLM: 1981\\n   - Input Sentence: Arnold Schwarzenegger was born on __\\n     - Gold: 1947-07-30\\n     - GPT-2: July\\n     - KGLM: 30\\n   - Input Sentence: Bob Dylan was born in __\\n     - Gold: Duluth\\n     - GPT-2: New\\n     - KGLM: Duluth\\n\\n2. **KGLM correct**\\n   - Input Sentence: Barack Obama was born on __\\n     - Gold: 1961-08-04\\n     - GPT-2: January\\n     - KGLM: August\\n   - Input Sentence: Ulysses is a book that was written by __\\n     - Gold: James Joyce\\n     - GPT-2: a\\n     - KGLM: James\\n   - Input Sentence: St. Louis is a city in the state of __\\n     - Gold: Missouri\\n     - GPT-2: Missouri\\n     - KGLM: Oldham\\n\\n3. **GPTv2 correct**\\n   - Input Sentence: Richard Nixon was born on __\\n     - Gold: 1913-01-09\\n     - GPT-2: January\\n     - KGLM: 20\\n   - Input Sentence: Kanye West is married to __\\n     - Gold: Kim Kardashian\\n     - GPT-2: Kim\\n     - KGLM: the\\n\\n4. **Both incorrect**\\n   - Input Sentence: The capital of India is __\\n     - Gold: New Delhi\\n     - GPT-2: the\\n     - KGLM: a\\n   - Input Sentence: Madonna is married to __\\n     - Gold: Carlos Leon\\n     - GPT-2: a\\n     - KGLM: Alex\\n\\nThe table highlights the differences in the factual accuracy and specificity of the generated tokens between KGLM and GPT-2.",
        "segmentation": false
    }
}