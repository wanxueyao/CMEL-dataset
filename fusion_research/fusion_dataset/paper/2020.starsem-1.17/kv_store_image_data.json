{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_1.jpg",
        "caption": [],
        "footnote": [],
        "context": "Virtual assistants play important roles in facilitating our daily life, such as booking hotels, reserving restaurants and making travel plans. Dialog State Tracking (DST), which estimates users’ goal and intention based on conversation history, is a core component in task-oriented Figure 1: An example of dialog state tracking for booking a hotel and reserving a restaurant. Each turn contains a user utterance (grey) and a system utterance (orange). The dialog state tracker (green) tracks all the $<$ domain, slot, value $>$ triplets until the current turn. Blue color denotes the new state appearing at that turn. Best viewed in color. on the MultiWOZ datasets show that our method significantly outperforms the BERT-based counterpart, finding that the key is a deep interaction between the domain-slot and context information. When evaluated on noisy (MultiWOZ 2.0) and cleaner (MultiWOZ 2.1) settings, our method performs competitively and robustly across the two different settings. Our method sets the new state of the art in the noisy setting, while performing more robustly than the best model in the cleaner setting. We also conduct a comprehensive error analysis on the dataset, including the effects of the dual strategy for each slot, to facilitate future research. 1 Introduction ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-d342986fead6b9e93e3b5e50b8194cac",
        "description": "The image is a screenshot of a dialog state tracking example for booking a hotel and reserving a restaurant. The dialog is structured into turns, with each turn containing a user utterance (in grey) and a system utterance (in orange). The dialog state tracker (in green) tracks all the <domain, slot, value> triplets until the current turn. The blue color denotes the new state appearing at that turn. The conversation begins with the user stating they are looking for a cheap hotel. The system responds by asking if the user has a specific area in mind. The user then specifies they need parking. The system finds a cheap hotel with parking and asks if the user wants to book it. The user confirms and provides details for 6 people staying for 3 nights starting on Tuesday. The system confirms the booking and provides a reference number. The user then requests to book an expensive restaurant with Japanese food. The dialog state tracker updates accordingly with each turn.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_2.jpg",
        "caption": [
            "Figure 2: The architecture of our proposed DS-DST model. The left part is a fixed BERT model which acts as a feature extractor and outputs the representations of values in the candidate-value list for each categorical slot (marked in purple). The right part is the other fine-tuned BERT model which outputs representations for the concatenation of each domain-slot pair and the recent dialog context. "
        ],
        "footnote": [],
        "context": "For each non-categorical slot, its value can be mapped to a span with start and end position in the dialog context, e.g., slot leave at in the taxi domain has spans $4:30\\mathrm{pm}$ in the context. 3.3 Non-Categorical Slot-Value Prediction where tj ygate is the one-hot gate label for the jth domain-slot pair at the $t_{t h}$ turn. $$ \\mathcal{L}_{g a t e}=\\sum_{t=1}^{T}\\sum_{j=1}^{N}-\\log(P_{t j}^{g a t e}\\cdot(y_{t j}^{g a t e})^{\\top}), $$ We adopt the cross-entropy loss function for the slot gate classification as follows: where $W_{g a t e}$ and $b_{g a t e}$ are learnable parameters and bias, respectively. t c a r e,p r e d i c t i o n\\}$ , where none denotes that a domain-slot pair is not mentioned or the value is ‘none’ at this turn, dontcare implies that the user can accept any values for this slot, and prediction represents that the slot should be processed by the model with a real value. We utilize $r_{t j}^{\\mathrm{CLS}}$ for the slot-gate classification, and the probability for the $j_{t h}$ domain-slot pair at the $t_{t h}$ turn is calculated as: $$ \\begin{array}{r}{P_{t j}^{g a t e}=\\mathrm{softmax}(W_{g a t e}\\cdot\\left(r_{t j}^{\\mathrm{{CLS}}}\\right)^{\\top}+b_{g a t e}),}\\end{array} $$ ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-66776e940011a1a6551ba1d5b182197d",
        "description": "The image is a detailed diagram illustrating the architecture of a proposed DS-DST model. The left part of the diagram represents a fixed BERT model, which acts as a feature extractor and outputs representations of values in the candidate-value list for each categorical slot (marked in purple). This part includes nodes labeled 'CLS', 'v1', ..., 'vc', and 'SEP', with arrows indicating the flow of information through the network. The right part of the diagram shows another fine-tuned BERT model that outputs representations for the concatenation of each domain-slot pair and the recent dialog context. This section includes nodes labeled 'r^CLS', 'r1', ..., 'rn', 'r^SEP', 'rn+2', ..., 'rK', with arrows indicating the flow of information. Above this section, there are two bar charts labeled 'Start Vector Distribution' and 'End Vector Distribution', showing the distribution of start and end positions for non-categorical slots in the dialog context. Below the BERT models, there are boxes labeled 'Candidate-Value List', 'Domain-Slot Pair', and 'Recent Dialog Context', providing examples of the types of data these models process. The Candidate-Value List includes examples such as 'cheap', 'expensive', 'moderate'. The Domain-Slot Pair includes examples like 'hotel, price range', 'taxi, arrive by'. The Recent Dialog Context includes an example dialogue snippet.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_3.jpg",
        "caption": [
            "Domain Hotel Train Restaurant Attraction Taxi "
        ],
        "footnote": [],
        "context": "$$ \\begin{array}{r l r}{\\lefteqn{\\mathcal{L}_{p i c k l i s t}=\\sum_{t=1}^{T}\\sum_{j=1}^{N-M}\\operatorname*{max}(0,\\lambda-c During the training process, we employ a hinge loss to enlarge the difference between the similarity of $r_{t j}^{\\mathrm{CLS}}$ to the target value and that to the most similar value in the candidate-value list: where $r_{t j}^{\\mathrm{CLS}}$ and $y_{l}^{\\mathrm{CLS}}$ are the aggregated representations from the slot-context encoder and the reference candidate value, respectively. Table 1: The dataset information of MultiWOZ 2.0 and MultiWOZ 2.1. The top two rows list 5 selected domains, consisting of 30 domain-slot pairs. The last three rows show the number of dialogues for each domain. e n s i v e,m o d e r a t e\\}$ . At the $t_{t h}$ turn, for the $j_{t h}$ domain-slot pair, we first use another pre-trained BERT to get the aggregated representation of each value in the candidate list: $$ y_{l}^{\\mathrm{CLS}}=\\mathrm{BERT}([\\mathrm{CLS}]\\oplus V_{l}\\oplus[\\mathrm{SEP}]), $$ where $l\\in\\{1,\\ldots,L\\}$ . Note that during the training process, this separate BERT model acts as a feature extractor and its model parameters are fixed. We calculate the relevance score between the aggregated representation and a reference candidate by the cosine similarity (Lin et al., 2017): $$ c o s(r_{t j}^{\\mathrm{CLS}},y_{l}^{\\mathrm{CLS}})=\\frac{r_{t j}^{\\mathrm{CLS}}\\cdot(y_{l}^{\\mathrm{CLS}})^{\\top}}{\\left\\|r_{t j}^{\\mathrm{CLS}}\\right\\|\\|y_{l}^{\\mathrm{CLS}}\\|}, $$ ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-66776e940011a1a6551ba1d5b182197d",
        "description": "The image is a table that provides dataset information for MultiWOZ 2.0 and MultiWOZ 2.1. The table is divided into two main sections: the top section lists five selected domains (Hotel, Train, Restaurant, Attraction, Taxi) along with their respective slots, and the bottom section shows the number of dialogues for each domain across three datasets (Train, Validation, Test). Each domain has multiple slots associated with it. For example, the Hotel domain includes slots such as price range, type, parking, book stay, book day, book people, area, stars, internet, and name. The Train domain includes destination, day, departure, arrive by, book people, leave at, and so on. The Restaurant domain includes food, price range, area, name, book time, book day, book people, and more. The Attraction domain includes area, name, and type. The Taxi domain includes leave at, destination, departure, and arrive by. The bottom section shows the number of dialogues for each domain in the Train, Validation, and Test datasets. For instance, the Hotel domain has 3381 dialogues in the Train set, 416 in the Validation set, and 394 in the Test set. Similarly, the Train domain has 3103 dialogues in the Train set, 484 in the Validation set, and 494 in the Test set. The Restaurant domain has 3813 dialogues in the Train set, 438 in the Validation set, and 437 in the Test set. The Attraction domain has 2717 dialogues in the Train set, 401 in the Validation set, and 395 in the Test set. The Taxi domain has 1654 dialogues in the Train set, 207 in the Validation set, and 195 in the Test set.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_4.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Joint accuracy on the test sets of MultiWOZ 2.0 and 2.1. +: the models require a full ontology, and $\\star$ : the results are reported by Eric et al. (2019) "
        ],
        "context": "SST-2 (Chen et al., 2020). BERT-DST (Rastogi et al., 2020). It uses BERT to obtain schema element embeddings and encode system as well as user utterances for dialogue state tracking. Different from the original model, it incorporates a pointer-generator copying mechanism for non-categorical slots of the MultiWOZ datasets. COMER (Ren et al., 2019). It applies BERT as contextualized word embeddings and first generates the slot sequences in the belief state, then generates the value sequences for each slot. TRADE (Wu et al., 2019). It contains a slot gate module for slots classification and a pointer generator for states generation. pair. with several existing models and introduce some of them as below: SpanPtr (Xu and Hu, 2018). It applies a RNNbased pointer network to find text spans with start and end pointers for each domain-slot pair. Ptr-DST. It is a variant based on SpanPtr with the exception that some slots are categorical slots, following DS-DST. DSTreader (Gao et al., 2019b). It models the DST from the perspective of machine reading comprehensions and applies a pre-trained BERT as initial word embeddings. DSTQA (Zhou and Small, 2019). It applies a dynamically-evolving knowledge graph and generates question asking for the values of a domain-slot ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-4a88ef391418e2f5c8b39687aa4cd0cc",
        "description": "The image is a table labeled 'Table 2: Joint accuracy on the test sets of MultiWOZ 2.0 and 2.1.' The table lists various dialogue state tracking (DST) models and their performance metrics on two datasets, MultiWOZ 2.0 and MultiWOZ 2.1. The table has three columns: 'Models,' 'MultiWOZ 2.0,' and 'MultiWOZ 2.1.' Each row represents a different model and its corresponding joint accuracy percentage on the two datasets. The models listed are SpanPtr, Ptr-DST, DSTreader, TRADE, COMER, DSTQA w/span, DSTQA w/o span+, BERT-DST, MA-DST, SST-2+, and NA-DST. The joint accuracy percentages range from 30.28% to 55.23%. Notable performances include DSTQA w/span with 51.36% on MultiWOZ 2.0 and 49.67% on MultiWOZ 2.1, and SST-2+ with 51.17% on MultiWOZ 2.0 and 55.23% on MultiWOZ 2.1. The footnote indicates that some models require a full ontology and that some results are reported by Eric et al. (2019).",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Detailed comparisons with BERT related methods Compared with those methods as shown in Table 2, we can observe that DS-Span, which employs the strength of BERT, outperforms SpanPtr by $10.91\\%$ , and it outperforms COMMER and DSTreader, which also use a pre-trained BERT model as dialog context embeddings and word embeddings, respectively. DS-DST outperforms BERT-DST, which separately encodes dialog context and domain-slot pairs based on BERT, by $7.81\\%$ on ules in Section 3.5, we also conduct experiments for separately training the non-categorical slots and categorical slots. DS-DST drops by $1.90\\%$ on MultiWOZ 2.1, which shows the benefits of jointly training.  further improve the DST performance. Although DS-Picklist is higher than DS-DST, in real scenarios, it may be nontrivial to have access to the full ontology. In the paper, we jointly train the three modTable 3: Joint accuracy on the test sets of MultiWOZ 2.1. BERT-DST is the model used in MultiWOZ 2.1. BERT-DST-Picklist is the original model described in (Rastogi et al., 2020), where a full ontology is required and all the slots are treated as categorical slots,. ‘single turn’ and ‘whole dialog history’ represent the Bert utterance inputs are the current dialog turn and the whole dialog history, respectively. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-f62a8c110929832aa89c04f4579fcabf",
        "description": "The image is a table labeled 'Table 3: Joint accuracy on the test sets of MultiWOZ 2.1.' The table compares the joint accuracy of different dialogue state tracking (DST) models. It contains two columns: 'Models' and 'Joint Accuracy'. The rows under 'Models' include BERT-DST (Rastogi et al., 2020), DS-DST, BERT-DST-Picklist (single turn), BERT-DST-Picklist (whole dialog history), ToD-BERT (Wu et al., 2020), and DS-Picklist. The corresponding 'Joint Accuracy' values are as follows: BERT-DST has 43.40%, DS-DST has 51.21%, BERT-DST-Picklist (single turn) has 39.86%, BERT-DST-Picklist (whole dialog history) has 46.42%, ToD-BERT has 48.00%, and DS-Picklist has 53.30%. The table highlights the performance differences among these models, with DS-Picklist achieving the highest joint accuracy.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "Now that we have observed that DS-DST and DSPicklist perform much better than DS-Span, we focus on where the accuracy improvement comes from. Table 4 shows the accuracy for each slot type on the MultiWOZ 2.1 test set, and we can observe significant improvement over the DS-Span baseline for some slots, including hotel-type, attractiontype, attraction-name, hotel-internet and hotelparking. This is because their values usually have different expressions and cannot be extracted from the dialog context, which decreases the performance of the span-based methods. In contrast, their values can be 5.2 Per Slot Accuracy employ strong interactions to multi-domain DST tasks.  Moreover, our models based on BERT surpass the strong ToD-BERT. We conclude that our improvements come from the strong interactions between slots and dialog context. Therefore, it is important to Table 4: The slot-level accuracy on the test set of MultiWOZ 2.1. $\\cdot_{+/-},$ indicates absolute performance improvement/degradation compared with DS-Span. The numbers highlighted in bold indicate that the difference is significant $(p\\,<\\,0.05)$ , tested by bootstrap resampling (Noreen, 1989). The slots above the first dashed line are categorical slots and the slots below the first dashed line are non-categorical slots for DS-DST. The last row shows the average slot accuracy. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-0674e6b156ddc1542ca4a1c858314e2e",
        "description": "The image is a table labeled 'Table 4: The slot-level accuracy on the test set of MultiWOZ 2.1.' It provides a detailed comparison of slot-level accuracy for three different models: DS-Span, DS-DST, and DS-Picklist. The table is structured with rows representing various slot types and columns representing the models and their respective accuracies. Each row includes the slot name and the accuracy values for each model, along with the improvement or degradation compared to DS-Span in parentheses. Significant improvements are highlighted in bold. The slots are categorized into categorical and non-categorical slots, separated by a dashed line. The categorical slots include hotel-type, attraction-name, restaurant-name, hotel-internet, hotel-parking, attraction-type, hotel-name, hotel-area, restaurant-area, attraction-area, hotel-price range, train-departure, restaurant-food, restaurant-price range, taxi-departure, taxi-destination, hotel-stars, train-destination, train-day, hotel-book day, restaurant-book day, train-leave at, train-arrive by, train-book people, restaurant-book time, taxi-leave at, hotel-book people, taxi-arrive by, hotel-book stay, and restaurant-book people. The average accuracy for each model is provided at the bottom of the table: DS-Span at 96.38%, DS-DST at 97.35%, and DS-Picklist at 97.40%. The table highlights significant improvements for some slots, particularly hotel-type, attraction-type, attraction-name, hotel-internet, and hotel-parking.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_7.jpg",
        "caption": [],
        "footnote": [
            "Table 5: Statistics of Top-10 slots on the MultiWOZ 2.1 validation set based on (#Unfound / #Relative_Turns). DS-DST and DS-Picklist show percentages based on (#Recover / #Unfound). #Unfound is the number of slots whose values cannot be found through span matching in the dialog context, #Relative_Turns is the number of dialogue turns where the slot type is mentioned, and #Recover indicates the number of values correctly predicted by DS-DST or DS-Picklist. "
        ],
        "context": "Error analysis To better understand the improvement, we conducted an error analysis and inspected actual examples on the MultiWOZ 2.1 validation set. Table 5 shows the top-10 slots, according to the ratio of ground-truth slot values which cannot be found through span matching. That is, for such examples, DS-Span cannot extract the ground-truth strings, resulting in the low joint accuracy. Here, we show how well our DS-DST and DS-Picklist can correctly predict the missing values in DS-Span. As we can see in this table, the two methods dramatically reduce the errors for some slots such as 5.3 Analysis and Discussions DS-DST and DSPicklist perform much better than DS-Span, we focus on where the accuracy improvement comes from. Table 4 shows the accuracy for each slot type on the MultiWOZ 2.1 test set, and we can observe significant improvement over the DS-Span baseline for some slots, including hotel-type, attractiontype, attraction-name, hotel-internet and hotelparking. This is because their values usually have different expressions and cannot be extracted from the dialog context, which decreases the performance of the span-based methods. In contrast, their values can be predicted directly from the candidatevalue lists. Compared with other slots, these slots still have space for improvements. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-0674e6b156ddc1542ca4a1c858314e2e",
        "description": "The image is a table labeled 'Table 5: Statistics of Top-10 slots on the MultiWOZ 2.1 validation set based on (#Unfound / #Relative_Turns)'. The table is structured with four main columns: Slot Name, DS-Span (#Unfound / #Relative_Turns), DS-DST, and DS-Picklist. Each row represents a specific slot name and its corresponding statistics. The rows are as follows: hotel-type (667/1395, 86.36%, 85.91%), hotel-parking (419/1048, 89.50%, 86.63%), hotel-internet (421/1124, 95.72%, 94.54%), taxi-leave at (73/364, 0.00%, 43.84%), attraction-name (215/1261, 70.23%, 74.42%), attraction-type (270/1658, 84.81%, 84.07%), train-leave at (181/1164, 2.21%, 41.44%), hotel-area (168/1452, 51.19%, 58.93%), train-arrive by (125/1428, 9.60%, 79.20%), and attraction-area (177/1620, 67.23%, 71.75%). The table highlights the performance of three different methods (DS-Span, DS-DST, and DS-Picklist) in predicting slot values that cannot be found through span matching in the dialog context. The percentages indicate the accuracy of each method in recovering the missing values.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_8.jpg",
        "caption": [],
        "footnote": [
            "Table 6: Predicted dialog states on the MultiWOZ 2.1 validation set, bold face means incorrect prediction. The first two examples show comparisons between DS-Span and DS-DST. The last example shows comparisons between DS-Span, DS-DST and DS-Picklist. "
        ],
        "context": "Open discussions Multi-domain dialog state tracking is enjoying popularity in enhancing research on task-oriented dialog systems, to handle tasks across different domains and support a large number of services. However, it should be noted that there is much room for improvement with the popular MultiWOZ 2.0 (Budzianowski et al., 2018) and MultiWOZ 2.1 (Eric et al., 2019) datasets, due to their annotation errors, ambiguity, and inconsistency. Moreover, a potential problem is that no standard ways have been established for the evaluation of the MultiWOZ dataset. Some papers are following the pre-processing ways provided by $\\mathrm{Wu}$ et al. (2019), while others  is not sufficient. In the third example, all the predictions are semantically correct; however, in terms of the string match, only DS-Picklist can correctly predict the value. The two other methods rely on span extraction. This is caused by formatting issues; that is, it is not always guaranteed that strings in the context satisfy desired formats, such as time expressions. Based on our analysis, future work needs to consider more relevant evaluation metrics than the widely-used string matching metric. For example, in the QA research community, it is investigated how to more robustly evaluate QA models (Chen et al., 2019). ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-0674e6b156ddc1542ca4a1c858314e2e",
        "description": "The image is a table labeled 'Table 6: Predicted dialog states on the MultiWOZ 2.1 validation set'. The table is divided into three main sections, each representing a different dialogue scenario between a user and a system. Each section contains a conversation log and the corresponding ground truths and predictions from three different models: DS-Span, DS-DST, and DS-Picklist. The conversations are structured as follows:\\n\\n1. **Hotel Booking Scenario**: \\n   - User: Looking for an expensive place to stay on the north side of Cambridge.\\n   - System: No matches found, offers to look for something else.\\n   - User: Specifies need for a 4-star hotel with free internet and parking.\\n   - Ground Truths: Hotel, internet (yes), stars (4), parking (yes), type (hotel), area (north), price range (expensive).\\n   - DS-Span: Correctly predicts all attributes except for internet (free internet) and parking (internet).\\n   - DS-DST: Correctly predicts all attributes except for type (none).\\n   - DS-Picklist: Correctly predicts all attributes.\\n\\n2. **Swimming Pool Recommendation Scenario**: \\n   - User: Asking for a good pool to visit on the north side of the city.\\n   - System: Offers two options: Jesus Green Outdoor Pool and Kings Hedges Learner Pool.\\n   - User: Chooses Kings Hedges Learner Pool and asks for the address.\\n   - Ground Truths: Attraction, area (north), type (swimming pool), name (Kings Hedges Learner Pool).\\n   - DS-Span: Incorrectly predicts type (pool).\\n   - DS-DST: Correctly predicts all attributes.\\n   - DS-Picklist: Correctly predicts all attributes.\\n\\n3. **Train Booking Scenario**: \\n   - User: Inquiring about trains leaving for Cambridge on Wednesday.\\n   - System: Provides details of a train (TR4203) leaving from Norwich at 05:16.\\n   - User: Requests booking for two people and provides reference number.\\n   - Ground Truths: Train, arrive by (08:15), departure (Norwich), day (Wednesday), book people (2), destination (Cambridge).\\n   - DS-Span: Correctly predicts all attributes.\\n   - DS-DST: Correctly predicts all attributes.\\n   - DS-Picklist: Correctly predicts all attributes.\\n\\nThe table highlights the performance of different models in predicting dialog states, with DS-Picklist generally performing better in terms of string matching.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": " and DS-DST in all the turns for two dialogues (i.e., MUL0729, PMUL2428) on the validation set of the MultiWOZ 2.1. Table 7 and Table 8 show the predicted dialog states for MUL0729 and PMUL2428, respectively. In Table 7, hotel type and hotel internet are predicted incorrectly by DS-Span, where the value yes of hotel internet has a different description free wifi in the dialog context. For this type of values, DSSpan cannot find the spans directly in the dialog context. In Table 8, DS-Span does not correctly predict the state <taxi, departure, funky fun house> at the $6_{t h}$ turn. ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-b7e67152e09814a094103db45b53d4c0",
        "description": "The image is a table that documents a dialogue interaction between a user and a system, focusing on the booking of a train from Ely to Cambridge and subsequently finding a place to stay. The table is structured into multiple turns, each representing a step in the dialogue. Each turn includes the user's query, the ground truths, and the predictions made by two models: DS-Span and DS-DST. The ground truths and predictions are represented as triplets indicating the type of information (e.g., train, hotel), the specific attribute (e.g., destination, stars), and the value (e.g., cambridge, 4). For example, in Turn 1, the user asks about trains from Ely to Cambridge, and both models correctly predict the departure and destination. In subsequent turns, the dialogue progresses with the user specifying more details such as the desired arrival time, day of travel, and the need for a 4-star hotel. Notably, in Turn 6, DS-Span incorrectly predicts the hotel internet as 'no' instead of 'yes', which is indicated by the ground truth and DS-DST. The table highlights the accuracy and discrepancies in the predictions made by the models compared to the ground truths.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/2020.starsem-1.17/images/image_10.jpg",
        "caption": [],
        "footnote": [],
        "context": "and DS-DST in all the turns for two dialogues (i.e., MUL0729, PMUL2428) on the validation set of the MultiWOZ 2.1. Table 7 and Table 8 show the predicted dialog states for MUL0729 and PMUL2428, respectively. In Table 7, hotel type and hotel internet are predicted incorrectly by DS-Span, where the value yes of hotel internet has a different description free wifi in the dialog context. For this type of values, DSSpan cannot find the spans directly in the dialog context. In Table 8, DS-Span does not correctly predict the state <taxi, departure, funky fun house> at the $6_{t h}$ turn.  ",
        "chunk_order_index": 9,
        "chunk_id": "chunk-b7e67152e09814a094103db45b53d4c0",
        "description": "The image is a table that documents a dialogue interaction between a user and a system, detailing the booking process for a restaurant named Nandos on Monday at 15:00 for 6 people. The table is structured into multiple turns of conversation, each turn containing the user's request, the system's response, and the ground truths along with predictions from two models, DS-Span and DS-DST. The ground truths include various attributes such as the restaurant name, booking day, time, number of people, attraction area, and taxi details. The predictions from DS-Span and DS-DST are compared against these ground truths. For instance, in Turn 2, the ground truth includes <restaurant, book day, monday>, <restaurant, name, nandos>, <restaurant, book time, 15:00>, <restaurant, book people, 6>. Both DS-Span and DS-DST correctly predict these values. In Turn 3, the ground truth includes an additional attribute <attraction, area, east>, which both models also predict correctly. However, in Turn 6, DS-Span fails to predict the state <taxi, departure, funky fun house> correctly, while DS-DST does. The table highlights the accuracy of the models in predicting the dialog states across different turns.",
        "segmentation": false
    }
}