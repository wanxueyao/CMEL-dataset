{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_1.jpg",
        "caption": [
            "Figure 1: Example annotation: phrases that refer to the same scientific concept are annotated into the same coreference cluster, such as MORphological PAser MORPA, it and MORPA (marked as red). "
        ],
        "footnote": [],
        "context": "Organizing scientific information into structured knowledge bases requires information extraction (IE) about scientific entities and their relationships. However, the challenges associated with As scientific communities grow and evolve, new tasks, methods, and datasets are introduced and different methods are compared with each other. Despite advances in search engines, it is still hard to identify new technologies and their relationships with what existed before. To help researchers more quickly identify opportunities for new combinations of tasks, methods and data, it is important to design intelligent algorithms that can extract and organize scientific information from a large collection of documents. 1 Introduction Abstract We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. We create SCIERC, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor (SCIIE) for with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-cc0ebc6806895b46dab167f0a64a568d",
        "description": "The image consists of two main components: a textual description and a graphical representation. The textual description is a sentence that reads: 'To reduce [ambiguity]OtherST, the [MORphological PARser MORPA]Method Used-for is provided with a [PCFG]Method... [It]Generic combines [context-free grammar]Method with... [MORPA]Method is a fully implemented [parser]Method developed for a [text-to-speech system]Task.' The graphical representation below the text is a flowchart or diagram with various nodes connected by arrows. The nodes are labeled with terms such as 'ambiguity,' 'MORphological PARser MORPA,' 'PCFG,' 'context-free grammar,' 'text-to-speech system,' and 'parser.' The node labeled 'MORPA' is highlighted in red. Arrows indicate relationships or connections between these terms, with labels such as 'Used-for' and 'Hyponym-of.' The overall layout suggests a relationship between different methods and concepts used in natural language processing, particularly in the context of parsing and text-to-speech systems.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_2.jpg",
        "caption": [],
        "footnote": [
            "Table 1: Dataset statistics for our dataset SCIERC and two previous datasets on scientific information extraction. All datasets annotate 500 documents. "
        ],
        "context": "Comparison with previous datasets SCIERC is focused on annotating cross-sentence relations and has more relation coverage than SemEval 17 and SemEval 18, as shown in Table 1. SemEval 17 is mostly designed for entity recognition and only covers two relation types. The task in SemEval 18 is to classify a relation between a pair of entities given entity boundaries, but only intra-sentence relations are annotated and each entity only appears in one relation, resulting in sparser relation coverage than our dataset (3.2 vs. 9.4 relations per abstract). SCIERC extends these datasets by adding more relation types and coreference clusters, which annotated example. Following annotation guidelines from QasemiZadeh and Schumann (2016) and using the BRAT interface (Stenetorp et al., 2012), our annotators perform a greedy annotation for spans and always prefer the longer span whenever ambiguity occurs. Nested spans are allowed when a subspan has a relation/coreference link with another term outside the span. Human Agreements One domain expert annotated all the documents in the dataset; $12\\%$ of the data is dually annotated by 4 other domain experts to evaluate the user agreements. The kappa score for annotating entities is $76.9\\%$ , relation extraction is $67.8\\%$ and coreference is $63.8\\%$ . ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-f1250fe597dc463227de39d654a649d8",
        "description": "The image is a table labeled 'Table 1: Dataset statistics for our dataset SCIERC and two previous datasets on scientific information extraction.' The table is structured with four main columns: Statistics, SCIERC, SemEval 17, and SemEval 18. Each column represents a different dataset and contains the following rows: '#Entities' (SCIERC: 8089, SemEval 17: 9946, SemEval 18: 7483), '#Relations' (SCIERC: 4716, SemEval 17: 672, SemEval 18: 1595), '#Relations/Doc' (SCIERC: 9.4, SemEval 17: 1.3, SemEval 18: 3.2), '#Coref links' (SCIERC: 2752, SemEval 17: -, SemEval 18: -), and '#Coref clusters' (SCIERC: 1023, SemEval 17: -, SemEval 18: -). The table highlights the significant differences in the number of entities, relations, and coreference annotations across the three datasets. The context indicates that SCIERC focuses on annotating cross-sentence relations and has more relation coverage than SemEval 17 and SemEval 18.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_3.jpg",
        "caption": [
            "Figure 2: Overview of the multitask setup, where all three tasks are treated as classification problems on top of shared span representations. Dotted arcs indicate the normalization space for each task. "
        ],
        "footnote": [],
        "context": "Relation extraction is to predict the best relation type given an ordered pair of spans $(s_{i},s_{j})$ . Let $L_{\\mathrm{R}}$ be the set of all possible relation types including the null-type $\\epsilon$ . The output structure $R$ is a set of random variables indexed over pairs of spans $(i,j)$ that belong to the same sentence: $r_{i Entity recognition is to predict the best entity type for every candidate span. Let $L_{\\mathrm{E}}$ represent the set of all possible entity types including the null-type $\\epsilon$ The output structure $E$ is a set of random variables indexed by spans: $e_{i}\\in L_{\\mathrm{E}}$ for $i=1,\\ldots,N$ .  Problem Definition The input is a document represented as a sequence of words $D=\\{w_{1},\\ldots,w_{n}\\}$ , from which we derive $S~=~\\{s_{1},\\ldots,s_{N}\\}$ , the set of all possible within-sentence word sequence spans (up to a reasonable length) in the document. The output contains three structures: the entity types $E$ for all spans $S$ , the relations $R$ for all pair of spans $S\\times S$ ,and the coreference links $C$ for all spans in $S$ . The output structures are represented with a set of discrete random variables indexed by spans or pairs of spans. Specifically, the output structures are defined as follows. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-6ced64911e56248a89a8b530563c72dd",
        "description": "The image is a flowchart illustrating the multitask setup for entity recognition, relation extraction, and coreference resolution. The chart is divided into two main sections: Entity Recognition and Relation Extraction. In the Entity Recognition section, there are three boxes labeled 'Task', 'Method', and 'NULL'. Arrows connect these boxes to a larger box labeled 'MORphological Parser MORPA'. Below this, there is another box labeled 'MORphological' with an arrow pointing to 'NULL'. In the Relation Extraction section, there are two boxes labeled 'Hyponym-of' and 'Used-for', both connected to 'NULL'. These boxes are also connected to the 'MORPA' box. There are dotted arcs indicating the normalization space for each task. Below these sections, there are green boxes listing span representations and their corresponding features. At the bottom, there are blue bars representing BiLSTM outputs, with arrows pointing to sentences. The sentences are partially visible, with one reading '...the MORphological Parser MORPA is provided with a...' and the other 'MORPA is a fully implemented parser developed for...'.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_4.jpg",
        "caption": [
            "Figure 3: Knowledge graph construction process. "
        ],
        "footnote": [],
        "context": "We evaluate our unified framework SCIIE on SCIERC and SemEval 17. The knowledge graph for Figure 5: Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase detection while significantly increasing the frequency of specific phrases. 6 Experimental Setup Assigning edges (relations) A pair of entities may appear in different contexts, resulting in different relation types between those entities (Figure 6). For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type.   all entities that belong to the same coreference cluster to replace generic terms with any other nongeneric term in the cluster. Moreover, we replace all the entities in the cluster with the entity that has the longest string. Our qualitative analysis shows that there are fewer ambiguous phrases using coreference links (Figure 5). We calculate the frequency counts of all entities that appear in the whole corpus. We assign nodes in the knowledge graph by selecting the most frequent entities (with counts $>k)$ in the corpus, and merge in any remaining entities for which a frequent entity is a substring. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-309d0af48135e3f30b2c88b741530b50",
        "description": "The image is a flowchart illustrating the process of constructing a scientific knowledge graph (KG). The process begins with multiple abstracts, labeled as Abstract(1), Abstract(2), ..., Abstract(m). Each abstract is processed by a system named SciIE, which generates document-level KGs. These document-level KGs are represented as small graphs with nodes and edges, where nodes are depicted in different colors (blue, green, orange) and shapes (circles), and edges are shown as lines connecting these nodes. The document-level KGs are then merged into a larger, more comprehensive scientific KG. The merging process is indicated by an arrow pointing from the document-level KGs to the final scientific KG. The final scientific KG is depicted as a larger graph with nodes in various colors (gray, blue, orange, green) and shapes (circles), interconnected by edges.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_5.jpg",
        "caption": [
            "Figure 4: A part of an automatically constructed scientific knowledge graph with the most frequent neighbors of the scientific term statistical machine translation (SMT) on the graph. For simplicity we denote Used-for (Reverse) as Uses, Evaluated-for (Reverse) as Evaluated-by, and replace common terms with their acronyms. The original graph and more examples are given Figure 10 in Appendix B. "
        ],
        "footnote": [],
        "context": "We evaluate our unified framework SCIIE on SCIERC and SemEval 17. The knowledge graph for Figure 5: Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase detection while significantly increasing the frequency of specific phrases. 6 Experimental Setup Assigning edges (relations) A pair of entities may appear in different contexts, resulting in different relation types between those entities (Figure 6). For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type.  all entities that belong to the same coreference cluster to replace generic terms with any other nongeneric term in the cluster. Moreover, we replace all the entities in the cluster with the entity that has the longest string. Our qualitative analysis shows that there are fewer ambiguous phrases using coreference links (Figure 5). We calculate the frequency counts of all entities that appear in the whole corpus. We assign nodes in the knowledge graph by selecting the most frequent entities (with counts $>k)$ in the corpus, and merge in any remaining entities for which a frequent entity is a substring.  ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-309d0af48135e3f30b2c88b741530b50",
        "description": "The image is a radial diagram representing a part of an automatically constructed scientific knowledge graph centered around the term 'statistical machine translation' (SMT). The central node labeled 'SMT' is connected to various other nodes through directed edges, indicating relationships such as 'Used-for', 'Evaluated-by', and others. The nodes are arranged in a circular pattern around the central node. Each node represents a related concept or term, and the edges represent the relationships between these terms and SMT. The terms include 'translation', 'search', 'paraphrasing', 'semantic parsing', 'alignment', 'parser', 'adaptation', 'decoding', 'RNN', 'NN', 'topic model', 'WSD', 'word alignment', 'log-linear model', 'domain adaptation', 'stochastic local search', 'word segmentation', 'maximum entropy', 'segmentation', 'perplexity', 'BLEU', 'METEOR', 'ROUGE', 'WER', 'Compare', 'NMT', 'information retrieval', 'ASR', 'Conjunction', 'classification', 'retrieval', 'grammatical error correction', and 'topic model'. The relationships are denoted by arrows pointing from one node to another, indicating the direction of the relationship. For example, 'SMT' is used for 'translation', 'search', 'paraphrasing', etc., and it is evaluated by 'WER', 'ROUGE', 'METEOR', etc.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "$\\mathbf{LSTM+CRF}$ The state-of-the-art NER system (Lample et al., 2016), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientific term extraction (Luan et We compare our model with the following baselines on SCIERCdataset: 6.1 Baselines scientific community analysis is built using the Semantic Scholar Corpus (110k abstracts in total). Figure 6: Frequency of relation types between pairs of entities: (left) automatic speech recognition (ASR) and machine translation (MT), (right) conditional random field (CRF) and graphical model (GM). We use the most frequent relation between pairs of entities in the knowledge graph.   resulting in different relation types between those entities (Figure 6). For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type. 6 Experimental Setup We evaluate our unified framework SCIIE on SCIERC and SemEval 17. The knowledge graph for Figure 5: Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase detection while significantly increasing the frequency of specific phrases. Linking entities through coreference helps disambiguate phrases when generating the knowledge graph. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-050ba62143bccd8f76afdad969812b02",
        "description": "The image is a bar chart comparing the frequency of detected entities with and without coreference resolution across various detection tasks. The chart has six categories: action detection, pedestrian detection, human detection, face detection, object detection, and a generic 'detection' category. Each category has two bars representing 'With Coref.' (blue) and 'Without Coref.' (red). The values for each category are as follows: action detection (With Coref.: 63, Without Coref.: 87), pedestrian detection (With Coref.: 57, Without Coref.: 90), human detection (With Coref.: 124, Without Coref.: 177), face detection (With Coref.: 258, Without Coref.: 510), object detection (With Coref.: 585, Without Coref.: 1297), and detection (With Coref.: 1237, Without Coref.: 1297). The chart shows that using coreference generally reduces the frequency of generic phrase detection while significantly increasing the frequency of specific phrases.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_7.jpg",
        "caption": [],
        "footnote": [],
        "context": "$\\mathbf{LSTM+CRF}$ The state-of-the-art NER system (Lample et al., 2016), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientific term extraction (Luan et We compare our model with the following baselines on SCIERCdataset: 6.1 Baselines scientific community analysis is built using the Semantic Scholar Corpus (110k abstracts in total). Figure 6: Frequency of relation types between pairs of entities: (left) automatic speech recognition (ASR) and machine translation (MT), (right) conditional random field (CRF) and graphical model (GM). We use the most frequent relation between pairs of entities in the knowledge graph. resulting in different relation types between those entities (Figure 6). For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.We assign edges between entities by selecting the most frequent relation type. 6 Experimental Setup We evaluate our unified framework SCIIE on SCIERC and SemEval 17. The knowledge graph for Figure 5: Frequency of detected entities with and without coreferece resolution: using coreference reduces the frequency of the generic phrase detection while significantly increasing the frequency of specific phrases. Linking entities through coreference helps disambiguate phrases when generating the knowledge graph.   ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-050ba62143bccd8f76afdad969812b02",
        "description": "The image consists of two bar charts side by side. The left chart is labeled 'MT-ASR' and the right chart is labeled 'CRF-GM'. Both charts have the y-axis labeled as '# Relation Triples' and the x-axis labeled with different relation types. In the left chart, the relation types are 'Conjunction', 'Used for', and 'Used for (Reverse)'. The number of relation triples for 'Conjunction' is 80, for 'Used for' is 10, and for 'Used for (Reverse)' is 4. In the right chart, the relation types are 'Hyponym of', 'Conjunction', 'Used for', and 'Used for (Reverse)'. The number of relation triples for 'Hyponym of' is 25, for 'Conjunction' is 4, for 'Used for' is 2, and for 'Used for (Reverse)' is 2.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_8.jpg",
        "caption": [
            "(c) Coreference resolution. "
        ],
        "footnote": [],
        "context": "Ablations We evaluate the SCIIE outperforms all the baselines. For entity recognition, our model achieves $1.3\\%$ and $2.4\\%$ relative improvement over $\\scriptstyle\\mathrm{LSTM+CRF}$ with and without ELMO, respectively. Moreover, it achieves $1.8\\%$ and $2.7\\%$ relative improvement over E2E Rel with and without ELMO, respectively. For relation extraction, we observe more significant improvement with $13.1\\%$ relative improvement over E2E Rel and $7.4\\%$ improvement over E2E Rel with ELMO. For coreference resolution, SCIIE outperforms E2E Coref with $4.5\\%$ relative improvement. We still observe a large gap between human-level performance and a machine learning system. We invite the community to address this challenging task.  their singular counterparts. 7 Experimental Results We evaluate SCIIE on SCIERC and SemEval 17 datasets. We provide qualitative results and human evaluation of the constructed knowledge graph. 7.1 IE Results Results on SciERC Table 2 compares the result of our model with baselines on the three tasks: entity recognition (Table 2a), relation extraction (Table 2b), and coreference resolution (Table 2c). As evidenced by the table, our unified multi-task setup Table 2: Comparison with previous systems on the development and test set for our three tasks. For coreference resolution, we report the average P/R/F1 of MUC, $\\mathbf{B}^{3}$ , and $\\mathrm{CEAF}_{\\phi_{4}}$ scores. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-050ba62143bccd8f76afdad969812b02",
        "description": "The image is a table labeled 'Table 2: Comparison with previous systems on the development and test set for our three tasks.' The table is divided into three sections: (a) Entity recognition, (b) Relation extraction, and (c) Coreference resolution. Each section contains rows representing different models and columns for Precision (P), Recall (R), and F1 score for both Development (Dev) and Test sets.\\n\\nFor entity recognition (Table 2a):\\n- LSTM+CRF has Dev P/R/F1 of 67.2/65.8/66.5 and Test P/R/F1 of 62.9/61.1/62.0.\\n- LSTM+CRF+ELMo has Dev P/R/F1 of 68.1/66.3/67.2 and Test P/R/F1 of 63.8/63.2/63.5.\\n- E2E Rel(Pipeline) has Dev P/R/F1 of 66.7/65.9/66.3 and Test P/R/F1 of 60.8/61.2/61.0.\\n- E2E Rel has Dev P/R/F1 of 64.3/68.6/66.4 and Test P/R/F1 of 60.6/61.9/61.2.\\n- E2E Rel+ELMo has Dev P/R/F1 of 67.5/66.3/66.9 and Test P/R/F1 of 63.5/63.9/63.7.\\n- SCIIE has Dev P/R/F1 of 70.0/66.3/68.1 and Test P/R/F1 of 67.2/61.5/64.2.\\n\\nFor relation extraction (Table 2b):\\n- E2E Rel(Pipeline) has Dev P/R/F1 of 34.2/33.7/33.9 and Test P/R/F1 of 37.8/34.2/35.9.\\n- E2E Rel has Dev P/R/F1 of 37.3/33.5/35.3 and Test P/R/F1 of 37.1/32.2/34.1.\\n- E2E Rel+ELMo has Dev P/R/F1 of 38.5/36.4/37.4 and Test P/R/F1 of 38.4/34.9/36.6.\\n- SCIIE has Dev P/R/F1 of 45.4/34.9/39.5 and Test P/R/F1 of 47.6/33.5/39.3.\\n\\nFor coreference resolution (Table 2c):\\n- E2E Coref has Dev P/R/F1 of 59.4/52.0/55.4 and Test P/R/F1 of 60.9/37.3/46.2.\\n- SCIIE has Dev P/R/F1 of 61.5/54.8/58.0 and Test P/R/F1 of 52.0/44.9/48.2.\\n\\nThe table highlights the performance of SCIIE compared to other models across all three tasks, showing significant improvements in most cases.",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": "Results on SemEval 17 Table 4 compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identification, keyphrase extraction and relation extraction as well as the overall score. Span identification aims at identifying spans of entities. Keyphrase classifi- cation and relation extraction has the same setting with the entity and relation extraction in SCIERC. Our model outperforms all the previous models that use hand-designed tion (37.9) significantly benefits when multi-tasked with coreference resolution ( $7.1\\%$ relative improvement). Coreference resolution benefits when multitasked with relation extraction, with $4.9\\%$ relative improvement.  between human-level performance and a machine learning system. We invite the community to address this challenging task. Ablations We evaluate the effect of multi-task learning in each of the three tasks defined in our dataset. Table 3 reports the results for individual tasks when additional tasks are included in the learning objective function. We observe that performance improves with each added task in the objective. For example, Entity recognition (65.7) benefits from both coreference resolution (67.5) and relation extraction (66.8). Relation extracTable 3: Ablation study for multitask learning on SCIERC development set. Each column shows results for the target task. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-2a11044e6b23f189d1c5255331a0663e",
        "description": "The image is a table labeled 'Table 3: Ablation study for multitask learning on SCIERC development set.' The table compares the performance of different tasks in a multi-task learning setup. It has four rows and three columns. The first row lists the tasks: 'Multi Task (SCiIE),' 'Single Task,' '+Entity Rec. ,' '+Relation,' and '+Coreference.' The subsequent rows provide the performance metrics for each task. The columns are labeled 'Entity Rec.', 'Relation,' and 'Coref.'. The values in the 'Entity Rec.' column are 68.1 for Multi Task, 65.7 for Single Task, - for +Entity Rec., 66.8 for +Relation, and 67.5 for +Coreference. The 'Relation' column shows 39.5 for Multi Task, 37.9 for Single Task, 38.9 for +Entity Rec., - for +Relation, and 39.5 for +Coreference. The 'Coref.' column displays 58.0 for Multi Task, 55.3 for Single Task, 57.1 for +Entity Rec., 57.6 for +Relation, and - for +Coreference. The table highlights the performance improvements when tasks are combined in a multi-task learning framework.",
        "segmentation": false
    },
    "image_10": {
        "image_id": 10,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_10.jpg",
        "caption": [
            "Figure 7: Historical trend for top applications of the keyphrase neural network in NLP, speech, and CV conference papers we collected. y-axis indicates the ratio of papers that use neural network in the task to the number of papers that is about the task. "
        ],
        "footnote": [],
        "context": "Knowledge Graph Evaluation Figure 8 shows the human evaluation of the constructed knowledge graph, comparing the quality of automatically generated knowledge graphs with and without the coreference links. We randomly select 10 frequent scientific entities and extract all the relation triples that include one of the selected entities leading to $1.5\\mathrm{k}$ relation triples from both systems. We ask four domain experts to annotate each of these extracted relations to define ground truth labels. Each domain expert is assigned 2 or 3 entities and all of the corresponding relations. Figure 8 shows precision/recall curves for both systems. Since it is not   of the extracted relation triples with the ‘Used-for’ relation type from speech, computer vision, and NLP conference papers. We observe that, before 2000, neural network has been applied to a greater percentage of speech applications compared to the NLP and computer vision papers. In NLP, neural networks first gain popularity in language modeling and then extend to other tasks such as POS Tagging and Machine Translation. In computer vision, the application of neural networks gains popularity in object recognition earlier (around 2010) than the other two more complex tasks of object detection and image segmentation (hardest and also the latest). ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-2a11044e6b23f189d1c5255331a0663e",
        "description": "The image consists of three line graphs, each representing the historical trend for top applications of neural networks in NLP, speech, and computer vision (CV) conference papers. The x-axis represents the years from 1995 to 2015, while the y-axis indicates the ratio of papers that use neural networks in a specific task to the number of papers about that task. The first graph shows trends for NLP tasks: Language Modeling (blue square), Machine Translation (red circle), and POS Tagging (green diamond). The second graph displays trends for speech tasks: Speech Recognition (blue square), Speech Synthesis (red circle), and Speaker Recognition (green diamond). The third graph illustrates trends for CV tasks: Object Recognition (blue square), Object Detection (red circle), and Image Segmentation (green diamond). Each graph shows an increasing trend over time, with significant rises starting around 2010. For example, in NLP, the ratio for Language Modeling starts near zero in 1995 and reaches approximately 0.6 by 2015. Similarly, in CV, the ratio for Object Recognition starts near zero in 1995 and also reaches approximately 0.6 by 2015.",
        "segmentation": false
    },
    "image_11": {
        "image_id": 11,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_11.jpg",
        "caption": [
            "Figure 8: Precision/pseudo-recall curves for human evaluation by varying cut-off thresholds. The AUC is 0.751 with coreference, and 0.695 without. "
        ],
        "footnote": [],
        "context": "Knowledge Graph Evaluation Figure 8 shows the human evaluation of the constructed knowledge graph, comparing the quality of automatically generated knowledge graphs with and without the coreference links. We randomly select 10 frequent scientific entities and extract all the relation triples that include one of the selected entities leading to $1.5\\mathrm{k}$ relation triples from both systems. We ask four domain experts to annotate each of these extracted relations to define ground truth labels. Each domain expert is assigned 2 or 3 entities and all of the corresponding relations. Figure 8 shows precision/recall curves for both systems. Since it is not  of the extracted relation triples with the ‘Used-for’ relation type from speech, computer vision, and NLP conference papers. We observe that, before 2000, neural network has been applied to a greater percentage of speech applications compared to the NLP and computer vision papers. In NLP, neural networks first gain popularity in language modeling and then extend to other tasks such as POS Tagging and Machine Translation. In computer vision, the application of neural networks gains popularity in object recognition earlier (around 2010) than the other two more complex tasks of object detection and image segmentation (hardest and also the latest).  ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-2a11044e6b23f189d1c5255331a0663e",
        "description": "The image is a graph depicting precision/pseudo-recall curves for human evaluation by varying cut-off thresholds. The x-axis represents the pseudo-recall percentage, ranging from 0% to 100%, while the y-axis represents the precision percentage, ranging from 84% to 92%. Two curves are plotted: one in blue with circular markers labeled 'With Coref.' and another in red with square markers labeled 'Without Coref.'. The blue curve starts at approximately 90% precision and 0% pseudo-recall, peaks slightly above 90% precision around 10% pseudo-recall, and then gradually declines to about 86% precision at 100% pseudo-recall. The red curve starts at approximately 89% precision and 0% pseudo-recall, declines steadily to about 85% precision at 100% pseudo-recall. The area under the curve (AUC) is 0.751 for the 'With Coref.' curve and 0.695 for the 'Without Coref.' curve.",
        "segmentation": false
    },
    "image_12": {
        "image_id": 12,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_12.jpg",
        "caption": [],
        "footnote": [
            "Table 4: Results for scientific keyphrase extraction and extraction on SemEval 2017 Task 10, comparing with previous best systems. "
        ],
        "context": "In this paper, we create a new dataset and develop a multi-task model for identifying entities, relations, and coreference clusters in scientific articles. By sharing span representations and leveraging crosssentence information, our multi-task setup effectively improves performance across all tasks. Moreover, we show that our multi-task model is better at predicting span boundaries and outperforms previous state-of-the-art scientific IE systems on entity and relation extraction, without using any handengineered features or pipeline processing. Using our model, we are able to automatically organize the extracted information from a large collection of scientific articles into a knowledge graph. Our analysis 8 Conclusion  of these extracted relations to define ground truth labels. Each domain expert is assigned 2 or 3 entities and all of the corresponding relations. Figure 8 shows precision/recall curves for both systems. Since it is not feasible to compute the actual recall of the systems, we compute the pseudo-recall (Zhang et al., 2015) based on the output of both systems. We observe that the knowledge graph curve with coreference linking is mostly above the curve without coreference linking. The precision of both systems is high (above $84\\%$ for both systems), but the system with coreference links has significantly higher recall. ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-2a11044e6b23f189d1c5255331a0663e",
        "description": "The image is a table labeled 'Table 4: Results for scientific keyphrase extraction and extraction on SemEval 2017 Task 10, comparing with previous best systems.' The table is structured with four main columns: Span Indentification, Keyphrase Extraction, Relation Extraction, and Overall. Each column represents a different task and contains the following rows: Model, Precision (P), Recall (R), and F1 score. The models compared are (Luan 2017), Best SemEval, and SciIE. For Span Indentification, the values are as follows: (Luan 2017) has an F1 score of 56.9; Best SemEval has P=55, R=54, and F1=55; SciIE has P=62.2, R=55.4, and F1=58.6. For Keyphrase Extraction, the values are: (Luan 2017) has an F1 score of 45.3; Best SemEval has P=44, R=43, and F1=44; SciIE has P=48.5, R=43.8, and F1=46.0. For Relation Extraction, the values are: (Luan 2017) has no data; Best SemEval has P=36, R=23, and F1=28; SciIE has P=40.4, R=21.2, and F1=27.8. For Overall, the values are: (Luan 2017) has no data; Best SemEval has P=44, R=41, and F1=43; SciIE has P=48.1, R=41.8, and F1=44.7. The table highlights the performance of each model across different tasks, with SciIE generally outperforming the other models.",
        "segmentation": false
    },
    "image_13": {
        "image_id": 13,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_13.jpg",
        "caption": [
            "Figure 9: Annotation example 1 from ACL "
        ],
        "footnote": [],
        "context": "Figure 10: An example of our automatically generated knowledge graph centered on statistical machine translation. This is the original figure of Figure 4.  ...advantage gained from local smoothness which... We present algorithms exploiting local smoothness in more aggressive ways... Same scientific term but refer to different examples: We use a data structure, we also use another data structure... 5. Do not label negative relations: $\\Chi$ is not used in $\\mathrm{Y}$ or $\\Chi$ is hard to be applied in Y BAnnotation and Knowledge Graph Examples Here we take a screen shot of the BRAT interface for an ACL paper in Figure 9. We also attach the original figure of Figure 3 in Figure 10. More examples can be found in the project website . ",
        "chunk_order_index": 10,
        "chunk_id": "chunk-e2c104027cba0af02f4bebfa9c994044",
        "description": "The image is a screenshot of a text annotation tool interface, specifically the BRAT (Bioinformatics Research Annotation Tool) interface, used for annotating scientific texts. The text is divided into numbered paragraphs and contains annotations highlighting various terms and their relationships. The annotations are categorized as 'Generic', 'Task', 'Other-ScientificTerm', and 'COREF'. The text discusses methods developed for spelling correction in different languages, focusing on agglutinative languages. Key points include: 1. Methods developed for spelling correction for languages like English are not readily applicable to agglutinative languages. 2. This poster presents an approach to spelling correction in agglutinative languages based on two-level morphology and a dynamic-programming-based search algorithm. 3. After an overview of the approach, results from experiments with spelling correction in Turkish are presented. The annotations indicate the relationships between terms, such as 'USED-FOR' and 'HYponym-OF', and highlight the coreference relationships between terms.",
        "segmentation": false
    },
    "image_14": {
        "image_id": 14,
        "image_path": "./fusion_research/fusion_dataset/paper/D18-1360/images/image_14.jpg",
        "caption": [],
        "footnote": [],
        "context": "Figure 10: An example of our automatically generated knowledge graph centered on statistical machine translation. This is the original figure of Figure 4. ...advantage gained from local smoothness which... We present algorithms exploiting local smoothness in more aggressive ways... Same scientific term but refer to different examples: We use a data structure, we also use another data structure... 5. Do not label negative relations: $\\Chi$ is not used in $\\mathrm{Y}$ or $\\Chi$ is hard to be applied in Y BAnnotation and Knowledge Graph Examples Here we take a screen shot of the BRAT interface for an ACL paper in Figure 9. We also attach the original figure of Figure 3 in Figure 10. More examples can be found in the project website .  ",
        "chunk_order_index": 10,
        "chunk_id": "chunk-e2c104027cba0af02f4bebfa9c994044",
        "description": "The image is a radial diagram centered on the term 'statistical machine translation.' The central node, labeled 'Uses,' branches out into multiple related terms and concepts. These branches are categorized under different headings such as 'Compared to,' 'Evaluated by,' 'Conjunction,' and 'Used for.' Under 'Compared to,' terms include 'machine translation' and 'neural machine translation.' Under 'Evaluated by,' terms like 'ROUGE,' 'METEOR,' and 'BLEU' are listed. The 'Conjunction' category includes 'speech synthesis,' 'classification,' 'speech recognition,' and 'automatic speech recognition.' The 'Used for' category lists 'semantic parsing,' 'retrieval,' 'speech translation,' 'hybrid system,' 'search,' 'paraphrasing,' and 'translation.' Each term is connected to the central node via curved lines, forming a radial pattern. The diagram uses a simple color scheme with blue circles at the end of each branch and light gray lines connecting them to the central node.",
        "segmentation": false
    }
}