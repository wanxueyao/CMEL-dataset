{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1125/images/image_1.jpg",
        "caption": [],
        "footnote": [
            "Figure 1: Two examples of initial exchanges from conversations concerning disagreements between editors working on the Wikipedia article about the Dyatlov Pass Incident. Only one of the conversations will eventually turn awry, with an interlocutor launching into a personal attack. "
        ],
        "context": "We make a first approach to this problem by analyzing the role of politeness (or lack thereof) in keeping conversations on track. Prior work has shown that politeness can help shape the course of offline (Clark, 1979; Clark and Schunk, 1980), as well as online interactions (Burke and Kraut, 2008), through mechanisms such as softening the perceived force of a message (Fraser, 1980), acting as a buffer between conflicting interlocutor goals (Brown and Levinson, 1987), and enabling all parties to save face (Goffman, 1955). This suggests the potential of politeness to serve as an indicator of whether a conversation will  of stonewalling. Could we endow artificial systems with such intuitions about the future trajectory of conversations? In this work we aim to computationally capture linguistic cues that predict a conversation’s future health. Most existing conversation modeling approaches aim to detect characteristics of an observed discussion or predict the outcome after the discussion concludes—e.g., whether it involves a present dispute (Allen et al., 2014; Wang and Cardie, 2014) or contributes to the eventual solution of a problem (Niculae and DanescuNiculescu-Mizil, 2016). In contrast, for this new task we need to discover interactional signals of the future trajectory of an ongoing conversation. ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-503b85c2865a6798f816a681f188d145",
        "description": "The image is a screenshot of a conversation thread from a Wikipedia talk page discussing the reliability of sources for an article about the Dyatlov Pass Incident. The conversation is divided into two sections, A and B, each representing different exchanges between editors. In section A, the discussion revolves around the inclusion of a source that mentions an altercation with a foreign intelligence group. Editor A1 questions why this source is not mentioned in the article, acknowledging its weak points but arguing for its existence. Editor A2 responds by challenging the idea of including a bad source just because it exists. In section B, the discussion focuses on the reliability of the St. Petersburg Times as a source. Editor B1 expresses doubt about relying heavily on this single source, especially since it speculates about missile launches and UFOs, and suggests finding corroborating sources. Editor B2 assumes the reliability of the St. Petersburg Times to be similar to other mainstream news sources. The text is presented in a clear, readable format with each editor's comments distinctly separated.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1125/images/image_2.jpg",
        "caption": [
            "Table 1: Descriptions of crowdsourcing jobs, with relevant statistics. More details in Appendix A. "
        ],
        "footnote": [],
        "context": "To select candidate conversations to include in our collection, we use the toxicity classifier Candidate selection. Our goal is to analyze how the start of a civil conversation is tied to its potential future derailment into personal attacks. Thus, we only consider conversations that start out as ostensibly civil, i.e., where at least the first exchange does not exhibit any toxic behavior, and that continue beyond this first exchange. To focus on the especially perplexing cases when the attacks come from within, we seek examples where the attack is initiated by one of the two participants in the initial exchange. of each talk page into structured conversations. This yields roughly $50\\ \\mathrm{mil}.$ -lion conversations across 16 million talk pages. Roughly one percent of Wikipedia comments are estimated to exhibit antisocial behavior (Wulczyn et al., 2017). This illustrates a challenge for studying conversational failure: one has to sift through many conversations in order to find even a small set of examples. To avoid such a prohibitively exhaustive analysis, we first use a machine learning classifier to identify candidate conversations that are likely to contain a toxic contribution, and then use crowdsourcing to vet the resulting labels and construct our controlled dataset. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-ea8690b0974e90a4abb43431e8a7fb3c",
        "description": "The image is a table labeled 'Table 1: Descriptions of crowdsourcing jobs, with relevant statistics.' The table is divided into two main sections. The first section, labeled 'Job 1: Ends in personal attack,' describes a task where three annotators are shown a conversation and asked to determine if its last comment is a personal attack toward someone else in the conversation. The statistics provided are: 367 annotators, 4,022 conversations, and an agreement rate of 67.8%. The second section, labeled 'Job 2: Civil start,' describes another task where conversations are split into snippets of three consecutive comments. Three annotators are asked to determine whether any of the comments in a snippet is toxic. The statistics for this job are: 247 annotators, 1,252 conversations, 2,181 snippets, and an agreement rate of 87.5%. The table highlights the different approaches used in analyzing conversational toxicity and the level of agreement among annotators.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1125/images/image_3.jpg",
        "caption": [],
        "footnote": [
            "Table 2: Prompt types automatically extracted from talk page conversations, with interpretations and examples from the data. Bolded text indicate common prompt phrasings extracted by the framework. Further examples are shown in Appendix B, Table 4. "
        ],
        "context": "To determine the prompt types of comments in our dataset, we first apply the above procedure to derive a set of prompt types from a disjoint (unlabeled) corpus of Wikipedia talk page conversations (Danescu-Niculescu-Mizil et al., 2012). After initial examination of the framework’s output on this external data, we chose to extract $k\\,=\\,6$ prompt types, shown in Table 2 along with our interpretations.8 These prompts represent signatures of conversation-starters spanning a wide range of topics and contexts which reflect core elements of Wikipedia, such as moderation disputes and coordination (Kittur et al., 2007; Kittur and Kraut, 2008). We assign each  . Each row of $\\hat{\\mathcal P}$ is then a promptvector of a phrasing, such that the prompt-vector for phrasing $i$ is close to the reply-vector for phrasing $j$ if comments with phrasing $i$ tend to prompt replies with phrasing $j$ . Clustering the rows of ˆ then yields $k$ conversational prompt types that are unified by their similarity in the space of replies. To infer the prompt type of a new comment, we represent the comment as an average of the representations of its constituent phrasings (i.e., rows of $\\hat{\\mathcal P}$ ) and assign the resultant vector to a cluster.7 ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-e163aba8e64811e0b13fe719c4dd36fd",
        "description": "The image is a table labeled 'Table 2: Prompt types automatically extracted from talk page conversations, with interpretations and examples from the data.' The table is structured with three main columns: Prompt Type, Description, and Examples. Each row represents a different prompt type and contains the following information:\\n\\n1. **Factual check**: Statements about article content, pertaining to or contending issues like factual accuracy. Examples include 'The terms are used interchangeably in the US.' and 'The census is not talking about families here.'\\n\\n2. **Moderation**: Rebukes or disputes concerning moderation decisions such as blocks and reversions. Examples include 'If you continue, you may be blocked from editing.' and 'He’s accused me of being a troll.'\\n\\n3. **Coordination**: Requests, questions, and statements of intent pertaining to collaboratively editing an article. Examples include 'It’s a long list so I could do with your help.' and 'Let me know if you agree with this and I’ll go ahead [...]' \\n\\n4. **Casual remark**: Casual, highly conversational aside-remarks. Examples include 'What’s with this flag image?' and 'I’m surprised there wasn’t an article before.'\\n\\n5. **Action statement**: Requests, statements, and explanations about various editing actions. Examples include 'Please consider improving the article to address the issues [...]' and 'The page was deleted as self-promotion.'\\n\\n6. **Opinion**: Statements seeking or expressing opinions about editing challenges and decisions. Examples include 'I think that it should be the other way around.' and 'This article seems to have a lot of bias.'\\n\\nThe table highlights the variety of prompt types found in Wikipedia talk page conversations, ranging from factual checks and moderation disputes to coordination requests and casual remarks.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1125/images/image_4.jpg",
        "caption": [
            "Figure 2: Log-odds ratios of politeness strategies and prompt types exhibited in the first and second comments of conversations that turn awry, versus those that stay on-track. All: Purple and green markers denote log-odds ratios in the first and second comments, respectively; points are solid if they reflect significant $(p<0.05)$ log-odds ratios with an effect size of at least 0.2. A: $\\diamondsuit\\mathbf{s}$ and $\\sqsubset\\mathbf{s}$ denote first and second comment log-odds ratios, respectively; \\* denotes statistically significant differences at the $p<0.05\\,(^{*})$ ,$p<0.01$ (\\*\\*) and $p<0.001$ (\\*\\*\\*) levels for the first comment (two-tailed binomial test); $^+$ denotes corresponding statistical significance for the second comment. B and C: $\\nabla\\mathbf{s}$ and $\\bigcirc\\mathrm{s}$ correspond to effect sizes in the comments authored by the attacker and non-attacker, respectively, in attacker initiated $({\\bf{B}})$ and non-attacker initiated (C) conversations. "
        ],
        "footnote": [],
        "context": "These effects are echoed in the second comment—i.e., the first reply (represented as $\\boxed{S},$ ). Interestingly, in this case we note that the difference in pronoun use is In contrast, comments which initiate on-track conversations tend to contain gratitude $(p<0.05)$ and greetings $(p<0.001)$ , both positive politeness strategies. Such conversations are also more likely to begin with coordination prompts $(p<0.05)$ , signaling active efforts to foster constructive teamwork. Negative politeness strategies are salient in on-track conversations as well, reflected by the use of hedges $(p<0.01)$ and opinion prompts $(p<0.05)$ ,which may serve to soften impositions or factual contentions (Hu¨bler, 1983).  find a rough correspondence between linguistic directness and the likelihood of future personal attacks. In particular, comments which contain direct questions, or exhibit sentenceinitial you (i.e., $^{\\bullet\\bullet}2^{\\mathrm{nd}}$ person start”), tend to start awry-turning conversations significantly more often than ones that stay on track (both $p<0.001]$ ). This effect coheres with our intuition that directness signals some latent hostility from the conversation’s initiator, and perhaps reinforces the forcefulness of contentious impositions (Brown and Levinson, 1987). This interpretation is also suggested by the relative propensity of the factual check prompt, which tends to cue disputes regarding an article’s factual content $(p<0.05)$ . ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-e163aba8e64811e0b13fe719c4dd36fd",
        "description": "The image is a set of three scatter plots labeled A, B, and C, which display log-odds ratios of politeness strategies and prompt types in the first and second comments of conversations that turn awry versus those that stay on-track. Each plot has a vertical axis listing various linguistic features such as 'Direct question', '2nd person start', 'Prompt: Factual check', etc., and a horizontal axis representing the log-odds ratio ranging from -0.5 to 0.5. The plots are divided into two categories: 'on-track' and 'awry'. Plot A shows the log-odds ratios for the first and second comments in general, with purple and green markers denoting the first and second comments, respectively. Points are solid if they reflect significant (p<0.05) log-odds ratios with an effect size of at least 0.2. Plot B and C focus on attacker-initiated and non-attacker-initiated conversations, respectively, with different markers for comments authored by the attacker and non-attacker. Significant differences are indicated by asterisks (*) and plus signs (+) for the first and second comments, respectively. For example, in Plot A, 'Direct question' and '2nd person start' have significant log-odds ratios, while 'Gratitude' and 'Greetings' do not. In Plot B, 'Direct question' and '2nd person start' also show significant differences, but 'Gratitude' and 'Greetings' do not. In Plot C, 'Direct question' and '2nd person start' again show significant differences, but 'Gratitude' and 'Greetings' do not.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P18-1125/images/image_5.jpg",
        "caption": [],
        "footnote": [
            "Table 3: Accuracies for the balanced futureprediction task. Features based on pragmatic devices are bolded, reference points are italicized. "
        ],
        "context": "Humans: A sample of 100 pairs were labeled by (non-author) volunteer human annotators. They were asked to guess, from the initial exchange, which conversation in a pair will lead to a personal Trained toxicity: We also compare with the toxicity score of the exchange from the Perspective API classifier—a perhaps unfair reference point, since this supervised system was trained on additional human-labeled training examples from the same domain and since it was used to create the very data on which we evaluate. This results in an accuracy of $60.5\\%$ ; combining trained toxicity with our pragmatic features achieves $64.9\\%$ .  per comment) achieve $60.5\\%$ accuracy. The pragmatic features combine to reach $61.6\\%$ accuracy. Reference points. To better contextualize the performance of our features, we compare their predictive accuracy to the following reference points: Interlocutor features: Certain kinds of interlocutors are potentially more likely to be involved in awry-turning conversations. For example, perhaps newcomers or anonymous participants are more likely to derail interactions than more experienced editors. We consider a set of features representing participants’ experience on Wikipedia (i.e., number of edits) and whether the comment authors are anonymous. In our task, these features perform at the level of random chance. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-320068db9e6fc59c389fd8fb7d893367",
        "description": "The image is a table labeled 'Table 3: Accuracies for the balanced futureprediction task.' The table is structured with three main columns: Feature set, # features, and Accuracy. Each row represents a different feature set used in the prediction task and contains the following information: \\n- Bag of words: 5,000 features, 56.7% accuracy. \\n- Sentiment lexicon: 4 features, 55.4% accuracy. \\n- Politeness strategies: 38 features, 60.5% accuracy. \\n- Prompt types: 12 features, 59.2% accuracy. \\n- Pragmatic (all): 50 features, 61.6% accuracy. \\n- Interlocutor features: 5 features, 51.2% accuracy. \\n- Trained toxicity: 2 features, 60.5% accuracy. \\n- Toxicity + Pragmatic: 52 features, 64.9% accuracy. \\n- Humans: 72.0% accuracy. \\nThe table highlights the performance of various feature sets in predicting future interactions, with the highest accuracy achieved by combining trained toxicity with pragmatic features at 64.9%. The reference point of human annotators achieves an accuracy of 72.0%, providing a benchmark for the model's performance.",
        "segmentation": false
    }
}