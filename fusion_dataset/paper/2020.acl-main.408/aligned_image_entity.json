{
    "image_1": {
        "entity_name": "Figure 1 :ERASER Benchmark",
        "entity_type": "EVENT",
        "description": "ERASER Benchmark is a proposed benchmark to advance research on interpretable models in NLP, comprising multiple datasets and tasks with human annotations of 'rationales'.",
        "reason": "The image depicts four sections illustrating different datasets included in the ERASER benchmark. The text information provides context about the ERASER benchmark, which aims to evaluate rationalized NLP models by providing standardized datasets and metrics for measuring interpretability.",
        "matched_chunk_entity_name": "ERASER Benchmark"
    },
    "image_2": {
        "entity_name": "Table 1 :Overview of datasets in the ERASER benchmark",
        "entity_type": "TABLE",
        "description": "A table summarizing various datasets used in the ERASER benchmark, including their names, sizes for train/dev/test splits, average number of tokens, and whether comprehensive rationales are available.",
        "reason": "The image clearly shows a table labeled 'Overview of datasets in the ERASER benchmark,' which matches the description provided in the text. The table includes details such as dataset names, sizes, tokens, and comprehensive rationales, aligning with the content described in the text.",
        "matched_chunk_entity_name": "no match"
    },
    "image_3": {
        "entity_name": "Table 2",
        "entity_type": "CONCEPT",
        "description": "Table 2 is mentioned as a reference for human agreement with respect to rationales across various datasets.",
        "reason": "The image is a table labeled 'Table 2: Human agreement with respect to rationales.' The content of the table matches the description provided in the text, which includes statistical information about the agreement of annotators on various datasets. This aligns with the entity 'Table 2' from the nearby entities list.",
        "matched_chunk_entity_name": "Table 2"
    },
    "image_4": {
        "entity_name": "Figure 2 :Commonsense Explanations (CoS-E)",
        "entity_type": "DATASET",
        "description": "Commonsense Explanations (CoS-E) is a dataset used to illustrate the scoring metrics comprehensiveness and sufficiency.",
        "reason": "The image clearly depicts faithfulness scoring metrics on the Commonsense Explanations (CoS-E) dataset, which aligns with the description provided in the text. The figure illustrates the concepts of comprehensiveness and sufficiency using bar charts and neural network diagrams, which are directly related to the CoS-E dataset.",
        "matched_chunk_entity_name": "Commonsense Explanations (CoS-E)"
    },
    "image_5": {
        "entity_name": "Table 3",
        "entity_type": "CONCEPT",
        "description": "The table provides detailed numerical values for performance metrics across various datasets, highlighting the performance of different models.",
        "reason": "The image is a table labeled 'Metrics for ‘soft’ scoring models,' which aligns with the description provided in the text. The table includes rows representing different datasets and columns representing performance metrics such as Perf., IOU F1, and Token F1. This matches the content and structure described in the text.",
        "matched_chunk_entity_name": "no match"
    },
    "image_6": {
        "entity_name": "Table 4: Metrics for ‘soft’ scoring models.",
        "entity_type": "TABLE",
        "description": "The table provides detailed numerical values for the performance and evaluation metrics of various models across different datasets, including Evidence Inference, BoolQ, Movies, FEVER, MultiRC, CoS-E, and e-SNLI. Each row represents different models and their respective metrics such as Perf., AUPRC, Comp. ↑, and Suff. ↓.",
        "reason": "The image clearly shows a table labeled 'Table 4: Metrics for ‘soft’ scoring models.' which aligns with the description provided in the text information. The table contains detailed numerical values for each metric across the different models and datasets, matching the content described in the text.",
        "matched_chunk_entity_name": "no match"
    },
    "image_7": {
        "entity_name": "Table 5: Detailed breakdowns for each dataset",
        "entity_type": "TABLE",
        "description": "The table provides statistics for various datasets across train, validation, and test splits. It includes the number of documents, instances, rationale percentages, evidence statements, and evidence lengths.",
        "reason": "The image is a table labeled 'Table 5: Detailed breakdowns for each dataset' that matches the description provided in the text. The table contains detailed statistics for each dataset, including the number of documents, instances, rationale percentages, evidence statements, and evidence lengths.",
        "matched_chunk_entity_name": "no match"
    },
    "image_8": {
        "entity_name": "Table 6: General dataset statistics",
        "entity_type": "TABLE",
        "description": "A table that provides general dataset statistics for various datasets, including the number of labels, instances, unique documents, and average numbers of sentences and tokens in documents.",
        "reason": "The image is a table that matches the description provided in the text. It contains general dataset statistics for various datasets, which aligns with the content of Table 6 mentioned in the text.",
        "matched_chunk_entity_name": "no match"
    }
}