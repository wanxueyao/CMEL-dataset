{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_1.jpg",
        "caption": [],
        "footnote": [
            "Figure 2: Our model, single-paragraph BERT, reads and scores each paragraph independently. The answer from the paragraph with the lowest $y_{\\mathrm{empty}}$ score is chosen as the final answer. "
        ],
        "context": "The model receives a question $Q=[q_{1},..,q_{m}]$ and a Our model, single-paragraph BERT, scores and answers each paragraph independently (Figure 2). We then select the answer from the paragraph with the best score, similar to Clark and Gardner (2018).1 3.1 Model Description This section shows the performance of a single-hop model on HOTPOTQA. 3 Single-paragraph QA Parallel research from Chen and Durrett (2019) presents similar findings on HOTPOTQA. Our work differs because we conduct human analysis to understand why questions are solvable using singlehop reasoning. Moreover, we show that selecting distractor paragraphs is difficult using current retrieval methods. graphs from Wikipedia. questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., WIKIHOP (Welbl et al., 2017) and COMPLEXWEBQUESTIONS (Talmor and Berant, 2018), or using crowd workers, e.g., HOTPOTQA (Yang et al., 2018). WIKIHOP questions are posed as triples of a relation and a head entity, and the task is to determine the tail entity of the relationship. COMPLEXWEBQUESTIONS consists of open-domain compositional questions, which are constructed by increasing the complexity of SPARQL queries from WEBQUESTIONS (Berant et al., 2013). We focus on HOTPOTQA, which consists of multi-hop questions written to require reasoning over two para ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-641cd529de353090b26104297132f871",
        "description": "The image is a flowchart illustrating the process of a single-paragraph BERT model in answering questions. The flowchart consists of multiple rows, each representing a question and its corresponding paragraph. Each row contains four main components: 'Question', 'Paragraph', 'BERT', and two output fields labeled 'y_empty' and 'span/yes/no'. The 'Question' column lists the input questions, while the 'Paragraph' column shows the paragraphs that are read by the BERT model. The 'BERT' component represents the model's processing of the paragraph to generate scores. The 'y_empty' field indicates the score for an empty answer, and the 'span/yes/no' field provides the final answer from the paragraph. The paragraph with the lowest 'y_empty' score is chosen as the final answer. The flowchart uses different colors to distinguish between the components: 'Question' is in light blue, 'Paragraph' is in yellow, 'BERT' is in dark blue, and the output fields are in green. The red box highlights the 'y_empty' field with the lowest score, indicating the selected paragraph for the final answer.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_2.jpg",
        "caption": [],
        "footnote": [],
        "context": "Distractor Setting The HOTPOTQA distractor setting pairs the two paragraphs the question was written for (gold paragraphs) with eight spurious paragraphs selected using TF-IDF similarity with the question (distractors). Our single-paragraph BERT HOTPOTQA has two settings: a distractor setting and an open-domain setting. 3.2 Model Results For a particular HOTPOTQA example, we run single-paragraph BERT on each paragraph in parallel and select the answer from the paragraph with the smallest yempty. Table 1: F1 scores on HOTPOTQA. \\* indicates the result is on the validation set; the other results are on the hidden test set shown in the official leaderboard. $[q_{1},...,q_{m},[\\mathtt{S E P}],p_{1},...,p_{n}]$ , where [SEP] is a special token, is fed into BERT: $$ S^{\\prime}=\\operatorname{BERT}(S)\\in\\mathbb{R}^{h\\times(m+n+1)}, $$ where $h$ is the hidden dimension of BERT. Next, a classifier uses max-pooling and learned parameters $W_{1}\\in\\mathbb{R}^{h\\times4}$ to generate four scalars: $$ [y_{\\mathrm{span}};y_{\\mathrm{yes}};y_{\\mathrm{no}};y_{\\mathrm{empty}}]=W_{1}\\mathrm{maxpool}(S^{\\prime}), $$ where $y_{\\mathrm{span}},y_{\\mathrm{yes}},y_{\\mathrm{no}}$ and $y_{\\mathrm{empty}}$ indicate the answer is either a span, $Y\\!\\in\\!S$ , no, or no answer. An extractive paragraph span, span, is obtained separately following Devlin et al. (2018). The final model outputs are a scalar value $y_{\\mathrm{empty}}$ and a text of either span, $Y{\\in}S$ or no, based on which of yspan, yyes, yno has the largest value. ",
        "chunk_order_index": 1,
        "chunk_id": "chunk-641cd529de353090b26104297132f871",
        "description": "The image is a table labeled 'Table 1: F1 scores on HOTPOTQA.' The table presents the performance of various models on two different settings of the HOTPOTQA dataset: Distractor and Open. The table has three columns: Model, Distractor F1, and Open F1. The rows list different models and their corresponding F1 scores. The models listed are Single-paragraph BERT*, BiDAF*, BiDAF, GRN, QFE, DFGN + BERT, MultiQA, DecompRC, BERT Plus, and Cognitive Graph. The Distractor F1 scores range from 58.28 to 69.76, with the highest score achieved by BERT Plus. The Open F1 scores range from 32.89 to 48.87, with the highest score achieved by Cognitive Graph. The asterisk (*) indicates that the result is on the validation set; the other results are on the hidden test set shown in the official leaderboard.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_3.jpg",
        "caption": [],
        "footnote": [],
        "context": "Redundant Evidence $26\\%$ of questions are compositional but are solvable using only part of the question. For instance, in the third example of Table 2 there is only a single founder of “Kaiser Ventures.” Thus, one can ignore the condition on “American industrialist” and “father of modern American shipbuilding.” This To further investigate entity type matching, we reduce the question to the first five tokens starting from the wh-word, following Sugawara et al. (2018). Although most of these reduced questions appear void of critical information, the F1 score of single-paragraph BERT only degrades about 15 F1 from 67.08 to 52.13.  Table 2 requires locating the university where “Ralph Hefferline” was a psychology professor, and multiple universities are provided as distractors. Therefore, the answer cannot be determined in one hop. Weak Distractors $35\\%$ of questions allow single-hop answers in the distractor setting, mostly by entity type matching. Consider the question in the second row of Table 2: in the ten provided paragraphs, only one actress has a government position. Thus, the question is answerable without considering the film “Kiss and Tell.” These examples may become multi-hop in the open-domain setting, e.g., there are numerous actresses with a government position on Wikipedia. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-c96e6bb9567410b6eb2a84b3ea94d4cd",
        "description": "The image is a table with four rows and three columns. The first column, labeled 'Type,' lists different types of questions: Multi-hop, Weak distractors, Redundant evidence, and Non-compositional 1-hop. The second column, labeled 'Question,' provides specific examples for each type. For example, the Multi-hop question asks about the city where Ralph Hefferline was a psychology professor at a university. The Weak distractors question inquires about the government position held by the woman who portrayed Corliss Archer in the film Kiss and Tell. The Redundant evidence question mentions Kaiser Ventures corporation and its founder. The Non-compositional 1-hop question asks about the release date of Poison’s album ‘Shut Up, Make Love’. The third column, labeled '%', shows the percentage associated with each type of question: 27% for Multi-hop, 35% for Weak distractors, 26% for Redundant evidence, and 8% for Non-compositional 1-hop.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_4.jpg",
        "caption": [
            "Table 2: We categorize bridge questions while taking the paragraphs into account. We exclude $4\\%$ of questions that we found to have incorrect or ambiguous answer annotations. See Section 4.1 for details on question types. "
        ],
        "footnote": [],
        "context": "Redundant Evidence $26\\%$ of questions are compositional but are solvable using only part of the question. For instance, in the third example of Table 2 there is only a single founder of “Kaiser Ventures.” Thus, one can ignore the condition on “American industrialist” and “father of modern American shipbuilding.” This To further investigate entity type matching, we reduce the question to the first five tokens starting from the wh-word, following Sugawara et al. (2018). Although most of these reduced questions appear void of critical information, the F1 score of single-paragraph BERT only degrades about 15 F1 from 67.08 to 52.13. Table 2 requires locating the university where “Ralph Hefferline” was a psychology professor, and multiple universities are provided as distractors. Therefore, the answer cannot be determined in one hop. Weak Distractors $35\\%$ of questions allow single-hop answers in the distractor setting, mostly by entity type matching. Consider the question in the second row of Table 2: in the ten provided paragraphs, only one actress has a government position. Thus, the question is answerable without considering the film “Kiss and Tell.” These examples may become multi-hop in the open-domain setting, e.g., there are numerous actresses with a government position on Wikipedia.  ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-c96e6bb9567410b6eb2a84b3ea94d4cd",
        "description": "The image is a table labeled 'Table 2' that categorizes bridge questions while taking paragraphs into account. The table has four rows and four columns. The first column, labeled 'Type', lists three types of questions: Multi-hop, Context-dependent, and Single-hop. The second column, labeled 'Question', provides examples for each type. For the Multi-hop type, the question is 'Who was born first, Arthur Conan Doyle or Penelope Lively?', with a percentage value of 45% and an F1 score of 54.46. For the Context-dependent type, the question is 'Are Hot Rod and the Memory of Our People both magazines?', with a percentage value of 36% and an F1 score of 56.16. For the Single-hop type, the question is 'Which writer was from England, Henry Roth or Robert Erskine Childers?', with a percentage value of 17% and an F1 score of 70.54. The table is used to analyze the performance of a model on different types of questions.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_5.jpg",
        "caption": [
            "Table 3: We automatically categorize comparison questions using rules $2\\%$ cannot be automatically categorized). Single-paragraph BERT achieves near chance accuracy on multi-hop questions but exploits single-hop ones. "
        ],
        "footnote": [],
        "context": "Many comparison questions are multi-hop or context-dependent multi-hop, and single-paragraph Table 4: We train on HOTPOTQA using standard distractors (Original) or using adversarial distractors (Adversarial). The model is then Comparison questions require quantitative or logical comparisons between two quantities or events. We create rules (Appendix C) to group comparison questions into three categories: questions which require multi-hop reasoning (multi-hop), may require multi-hop reasoning (context-dependent), and require single-hop reasoning (single-hop). 4.2 Categorizing Comparison Questions Non-compositional Single-hop $8\\%$ of questions are non-compositional and single-hop. In the last example of Table 2, one sentence contains all of the information needed to answer correctly. from the wh-word, following Sugawara et al. (2018). Although most of these reduced questions appear void of critical information, the F1 score of single-paragraph BERT only degrades about 15 F1 from 67.08 to 52.13. Redundant Evidence $26\\%$ of questions are compositional but are solvable using only part of the question. For instance, in the third example of Table 2 there is only a single founder of “Kaiser Ventures.” Thus, one can ignore the condition on “American industrialist” and “father of modern American shipbuilding.” This category differs from the weak distractors category because its questions are single-hop regardless of the distractors. ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-c96e6bb9567410b6eb2a84b3ea94d4cd",
        "description": "The image is a table labeled 'Table 4: We train on HOTPOTQA using standard distractors (Original) or using adversarial distractors (Adversarial).' The table is structured with two main columns under the header 'Training Data': 'Original' and 'Adversarial.' Each of these columns is further divided into rows corresponding to different types of evaluation data. The rows are labeled as follows: 'Original,' 'Adversarial,' and '+ Type.' The values in the table represent performance metrics, likely accuracy or F1 scores, for different combinations of training and evaluation data. The exact values are as follows: For 'Original' training data, the 'Original' evaluation data yields a score of 67.08, while 'Adversarial' evaluation data yields 59.12. For 'Adversarial' training data, 'Original' evaluation data results in a score of 46.84, and 'Adversarial' evaluation data results in 60.10. The '+ Type' row shows a score of 40.73 for 'Original' evaluation data and 58.42 for 'Adversarial' evaluation data. The table highlights the performance differences when using original versus adversarial distractors during training and evaluation.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_6.jpg",
        "caption": [],
        "footnote": [],
        "context": "This research was supported Acknowledgements tractor paragraphs. For example, we found $35\\%$ of bridge questions are currently single-hop but may become multi-hop when combined with stronger distractors (Section 4.1). However, as we demonstrate in Section 5, selecting strong distractors for RC questions is non-trivial. We suspect this is also due to the insufficiencies of standard TFIDF retrieval for multi-hop questions. In particular, Table 5 shows that single-paragraph BERT achieves 53.12 F1 even when using 500 distractors (rather than eight), indicating that 500 distractors are still insufficient. In this end, future multi-hop RC datasets can develop improved methods for distractor collection.  Figure 1, because the question does not contain terms about “Bonobo apes.” Table 5 shows that the model achieves 39.12 F1 given 500 retrieved paragraphs, but achieves 53.12 F1 when additional two gold paragraphs are given, demonstrating the significant effect of failure to retrieve gold paragraphs. In this context, we suggest that future work can explore better retrieval methods for multi-hop questions. Retrieving Strong Distractors Another way to ensure multi-hop reasoning is to select strong disTable 5: The accuracy of single-paragraph BERT in different open-domain retrieval settings. TF-IDF often fails to retrieve the gold paragraphs even when using 500 candidates. ",
        "chunk_order_index": 3,
        "chunk_id": "chunk-da3785709741127896bd86556478a157",
        "description": "The image is a table labeled 'Table 5: The accuracy of single-paragraph BERT in different open-domain retrieval settings.' The table has two columns: 'Setting' and 'F1'. The rows under the 'Setting' column are as follows: 'Distractor', 'Open-domain 10 Paragraphs', 'Open-domain 500 Paragraphs', and 'Open-domain 500 Paragraphs + Gold Paragraph'. The corresponding F1 scores for these settings are 67.08, 38.40, 39.12, and 53.12, respectively. The table highlights the performance of the model in various retrieval settings, with the highest F1 score achieved when additional gold paragraphs are included.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_7.jpg",
        "caption": [
            "Figure 3: Single-paragraph BERT reads and scores each paragraph independently. The answer from the paragraph with the lowest $y^{\\mathrm{empty}}$ score is chosen as the final answer. "
        ],
        "footnote": [],
        "context": "We now have four scalar where $p_{\\mathrm{start}}^{i}$ and $p_{\\mathrm{end}}^{j}$ indicate the $i$ -th element of $p_{\\mathrm{start}}$ and $j$ -th element of $p_{\\mathrm{end}}$ , respectively. $$ y_{\\mathrm{start}},y_{\\mathrm{end}}=\\underset{i\\leq j}{\\arg\\operatorname*{max}}\\,p_{\\mathrm{start}}^{i}p_{\\mathrm{end}}^{j} $$ where $W_{2},W_{3}\\in\\mathbb{R}^{h}$ are learned parameters. Then, $y_{\\mathrm{start}}$ and $y_{\\mathrm{end}}$ are obtained: $$ \\begin{array}{r}{p_{\\mathrm{start}}=\\mathrm{Softmax}(W_{2}S^{\\prime})}\\\\ {p_{\\mathrm{end}}=\\mathrm{Softmax}(W_{3}S^{\\prime}),}\\end{array} $$ A candidate answer span is then computed separately from the classifier. We define where $y_{\\mathrm{span}},y_{\\mathrm{yes}},y_{\\mathrm{no}}$ and $y_{\\mathrm{empty}}$ indicate the answer is either a span, $Y\\!\\in\\!S$ , no, or no answer. $$ [y_{\\mathrm{span}};y_{\\mathrm{yes}};y_{\\mathrm{no}};y_{\\mathrm{empty}}]=W_{1}{\\mathrm{maxpool}}(S^{\\prime}), $$ where $h$ is the hidden dimension of BERT. Next, a classifier uses max-pooling and learned parameters $W_{1}\\in\\mathbb{R}^{h\\times4}$ to generate four scalars: $$ S^{\\prime}=\\operatorname{BERT}(S)\\in\\mathbb{R}^{h\\times(m+n+1)}, $$  belonging to the Collectivity of Saint Barth´elemy. The island is privately owned. The only inhabitants are some goats. The highest point is 103 meter above sea level. It is situated within R´eserve naturelle nationale de Saint-Barthe´lemy. BFull Model Details Single-paragraph BERT is a pipeline which first retrieves a single paragraph using a classifier and then selects the associated answer. Formally, the model receives a question $Q\\,=\\,[q_{1},..,q_{m}]$ and a single paragraph $P\\,=\\,[p_{1},...,p_{n}]$ as input. The question and paragraph are merged into a single sequence, $S=[q_{1},...,q_{m},[\\mathrm{SEP}],p_{1},...,p_{n}]$ , where [SEP] is a special token indicating the boundary. The sequence is fed into BERT-BASE: ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-5e7ccf1d72e7b038983b2017f5d8ddb6",
        "description": "The image is a detailed flowchart illustrating the process of single-paragraph BERT for reading and scoring each paragraph independently. The chart is divided into two main sections. On the left, there is a flowchart showing the input sequence consisting of a question and a paragraph, which are merged into a single sequence with a special token [SEP] indicating the boundary. This sequence is then fed into BERT-BASE. The output from BERT-BASE is a tensor \\( S' \\in \\mathbb{R}^{h \\times (m+n+1)} \\), where \\( h \\) is the hidden dimension of BERT. This tensor is then processed through max-pooling and learned parameters \\( W_1 \\in \\mathbb{R}^{h \\times 4} \\) to generate four scalars: \\( y_{\\mathrm{span}} \\), \\( y_{\\mathrm{yes}} \\), \\( y_{\\mathrm{no}} \\), and \\( y_{\\mathrm{empty}} \\). These scalars indicate whether the answer is a span, yes, no, or empty. The start and end positions of the span are determined by finding the maximum product of the start and end probabilities \\( p_{\\mathrm{start}}^i \\) and \\( p_{\\mathrm{end}}^j \\) respectively, using learned parameters \\( W_2, W_3 \\in \\mathbb{R}^{h} \\). On the right, the chart shows the outputs from multiple paragraphs, with the final answer chosen from the paragraph with the lowest \\( y_{\\mathrm{empty}} \\) score.",
        "segmentation": false
    },
    "image_8": {
        "image_id": 8,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_8.jpg",
        "caption": [],
        "footnote": [],
        "context": "Based on the identified operation, questions are classified into multi-hop, context-dependent multi-hop, or single-hop. First, numerical questions are always multi-hop (e.g., first example of Table 6). Next, the operations And, Or, Is equal, and Not equal are context-dependent multi-hop. For instance, in the second example of Table 6, if “Hot Rod” is not a magazine, one can immediately answer No. Finally, the operations Which is true and Intersection are single-hop because they can be answered using one paragraph regardless of the context. For entities, we identity the suitable question operation following Algorithm 1.  Algorithm 1 Algorithm for Identifying Question Operations Hugging Face’s implementation.5 We use Adam (Kingma and Ba, 2015) with learning rate $5\\times10^{-5}$ . We lowercase the input and set the maximum sequence length $|S|$ to 300. If a sequence is longer than 300, we split it into multiple sequences and treat them as different examples. C Categorizing Comparison Questions This section describes how we categorize comparison questions. We first identify ten question operations that sufficiently cover comparison questions (Table 6). Next, for each question, we extract the two entities under comparison using the Spacy NER tagger on the question and the two HOTPOTQA supporting facts. Using these extracted ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-76e5e660f564ca4da0077e1cfaaa9966",
        "description": "The image is a table labeled 'Table 6: The question operations used for categorizing comparison questions.' The table is divided into three main categories: Numerical Questions, Logical Questions, and String Questions. Each category lists specific operations and provides examples.\\n\\n- **Numerical Questions**\\n  - Operations: Is greater / Is smaller / Which is greater / Which is smaller\\n  - Example (Which is smaller): Who was born first, Arthur Conan Doyle or Penelope Lively?\\n\\n- **Logical Questions**\\n  - Operations: And / Or / Which is true\\n  - Example (And): Are Hot Rod and the Memory of Our People both magazines?\\n\\n- **String Questions**\\n  - Operations: Is equal / Not equal / Intersection\\n  - Example (Is equal): Are Cardinal Health and Kansas City Southern located in the same state?",
        "segmentation": false
    },
    "image_9": {
        "image_id": 9,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1416/images/image_9.jpg",
        "caption": [],
        "footnote": [],
        "context": "Based on the identified operation, questions are classified into multi-hop, context-dependent multi-hop, or single-hop. First, numerical questions are always multi-hop (e.g., first example of Table 6). Next, the operations And, Or, Is equal, and Not equal are context-dependent multi-hop. For instance, in the second example of Table 6, if “Hot Rod” is not a magazine, one can immediately answer No. Finally, the operations Which is true and Intersection are single-hop because they can be answered using one paragraph regardless of the context. For instance, in the third example of Table entities, we identity the suitable question operation following Algorithm 1. and Ba, 2015) with learning rate $5\\times10^{-5}$ . We lowercase the input and set the maximum sequence length $|S|$ to 300. If a sequence is longer than 300, we split it into multiple sequences and treat them as different examples. C Categorizing Comparison Questions This section describes how we categorize comparison questions. We first identify ten question operations that sufficiently cover comparison questions (Table 6). Next, for each question, we extract the two entities under comparison using the Spacy NER tagger on the question and the two HOTPOTQA supporting facts. Using these extracted  Algorithm 1 Algorithm for Identifying Question Operations ",
        "chunk_order_index": 6,
        "chunk_id": "chunk-76e5e660f564ca4da0077e1cfaaa9966",
        "description": "The image is a pseudocode representation of an algorithm titled 'Algorithm for Identifying Question Operations'. The algorithm is structured as a procedure named CATEGORIZE, which takes three parameters: question, entity1, and entity2. The procedure begins by calling a function f(question, entity1, entity2) to determine coordination and preconjunct values. It then checks if the question is either or both from the coordination and preconjunct values. Next, it calls another function f_head(question, entity1, entity2) to determine the head entity. The algorithm then proceeds with a series of conditional statements based on specific keywords in the question (e.g., more, most, less, earlier, etc.). Depending on these keywords and the existence of the head entity, the algorithm assigns a discrete_operation value, which can be one of the following: 'Which is greater', 'Is greater', 'Which is smaller', 'Is smaller', 'Which is true', 'Intersection', 'Or', 'And', 'Is equal', 'Not equal'. The final step returns the discrete_operation value.",
        "segmentation": false
    }
}