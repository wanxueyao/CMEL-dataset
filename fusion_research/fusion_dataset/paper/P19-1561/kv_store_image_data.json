{
    "image_1": {
        "image_id": 1,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_1.jpg",
        "caption": [
            "Table 1: Adversarial spelling mistakes inducing sentiment misclassification and word-recognition defenses. "
        ],
        "footnote": [],
        "context": "First, in experiments addressing both BiLSTM and fine-tuned BERT models, comprising four different input formats: word-only, char-only, word+char, and word-piece (Wu et al., 2016), we demonstrate that an adversary can degrade a classifier’s performance to that In this paper, we focus on adversarially-chosen spelling mistakes in the context of text classification, addressing the following attack types: dropping, adding, and swapping internal characters within words. These perturbations are inspired by psycholinguistic studies (Rawlinson, 1976; Matt Davis, 2003) which demonstrated that humans can comprehend text altered by jumbling internal characters, provided that the first and last characters of each word remain unperturbed.  could cause image recognition models to misclassify examples (Szegedy et al., 2013), a veritable sub-field has emerged in which authors iteratively propose attacks and countermeasures. For all the interest in adversarial computer vision, these attacks are rarely encountered outside of academic research. However, adversarial misspellings constitute a longstanding real-world problem. Spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails’ intended meaning (Lee and Ng, 2005; Fumera et al., 2006). As another example, programmatic censorship on the Internet has spurred communities to adopt similar methods to communicate surreptitiously (Bitso et al., 2013). ",
        "chunk_order_index": 0,
        "chunk_id": "chunk-a5eb108ca07f16adae9942cf4d2fadf8",
        "description": "The image is a table labeled 'Table 1: Adversarial spelling mistakes inducing sentiment misclassification and word-recognition defenses.' The table is structured with three main columns: Alteration, Movie Review, and Label. Each row represents a different alteration applied to the original movie review and its corresponding label. The first row under 'Original' shows the movie review 'A triumph, relentless and beautiful in its downbeat darkness' with a positive label (+). The second row under 'Swap' shows the altered review 'A triumph, relentless and beuatiful in its downbeat darkness' with a negative label (-). The third row under 'Drop' shows the altered review 'A triumph, relentless and beautiful in its dwnbeat darkness' with a negative label (-). The fourth and fifth rows under '+ Defense' show the reviews 'A triumph, relentless and beautiful in its downbeat darkness' and 'A triumph, relentless and beautiful in its downbeat darkness' with positive labels (+) respectively. The alterations are highlighted in red for swapped or dropped characters and in blue for corrected characters.",
        "segmentation": false
    },
    "image_2": {
        "image_id": 2,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_2.jpg",
        "caption": [
            "tender yet lacerating and darkly funny fable "
        ],
        "footnote": [],
        "context": "In NLP, we often get such invariance for free, e.g., for a word-level model, most of the perturbations produced by our Figure 1: A schematic sketch of our proposed word recognition system, consisting of a foreground and a background model. We train the foreground model on the smaller, domain-specific dataset, and the background model on a larger dataset (e.g., the IMDB movie corpus). We train both models to reconstruct the correct word from the orthography and context of the individual words, using synthetically corrupted inputs during training. Subsequently, we invoke the background model whenever the foreground model predicts UNK . our second consideration for building robust word-recognizers. 3.2 Model Sensitivity In computer vision, an important factor determining the success of an adversary is the norm constraint on the perturbations allowed to an image $(||\\mathbf{x}-\\mathbf{x}^{\\prime}||_{\\infty}\\ <\\ \\epsilon)$ . Higher values of $\\epsilon$ lead to a higher chance of mis-classification for at least one $\\mathbf{x}^{\\prime}$ . Defense methods such as quantization ( $\\mathrm{Xu}$ et al., 2017) and thermometer encoding (Buckman et al., 2018) try to reduce the space of perturbations available to the adversary by making the model invariant to small changes in the input. tender yet lacreating and darkly funny fable ",
        "chunk_order_index": 2,
        "chunk_id": "chunk-a042c5afc27f9a817cbb4c69d058a331",
        "description": "The image is a schematic sketch of a proposed word recognition system, consisting of two models: a foreground model and a background model. The foreground model is trained on a smaller, domain-specific dataset, while the background model is trained on a larger dataset, such as the IMDB movie corpus. Both models are designed to reconstruct the correct word from the orthography and context of individual words, using synthetically corrupted inputs during training. The foreground model is represented by green circles labeled h1, h2, h3, ..., hn, connected by bidirectional arrows, indicating interactions between these states. The background model is depicted similarly but with gray circles. When the foreground model predicts an unknown (UNK) word, the background model is invoked to provide a prediction. Above the models, there are semi-character representations in blue boxes, which are used as inputs for both models. The text above the blue boxes appears scrambled, suggesting synthetic corruption. The overall layout and design indicate a focus on robust word recognition systems that can handle perturbations and unknown words.",
        "segmentation": false
    },
    "image_3": {
        "image_id": 3,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_3.jpg",
        "caption": [
            "Word Recognition "
        ],
        "footnote": [
            "Table 2: Word Error Rates (WER) of ScRNN with each backoff strategy, plus ATD and an ScRNN trained only on the background corpus (78K vocabulary) The error rates include $5.25\\%$ OOV words. "
        ],
        "context": "Results We calculate the word error rates (WER) of each of the models for different attacks and present our findings in Table 2. Note that ATD incorrectly predicts 11.2 words for every 100 words (in the ‘all’ setting), whereas, all of the backoff variations of the ScRNN reconstruct better. The most accurate variant involves backing off to the background model, resulting in a low error rate of $6.9\\%$ , leading to the best performance on word recognition. This is a $32\\%$ relative error reduction compared to the vanilla ScRNN model with a pass-through backoff strategy. We can attribute the improved other anonymized commercial spell checkers. For the ScRNN model, we use a single-layer BiLSTM with a hidden dimension size of 50. The input representation consists of 198 dimensions, which is thrice the number of unique characters (66) in the vocabulary. We cap the vocabulary size to 10K words, whereas we use the entire vocabulary of 78470 words when we backoff to the background model. For training these networks, we corrupt the movie reviews according to all attack types, i.e., applying one of the 4 attack types to each word, and trying to reconstruct the original words via cross entropy loss. ",
        "chunk_order_index": 4,
        "chunk_id": "chunk-667f71c87e99d2706ad4a9d7bf4c4383",
        "description": "The image is a table labeled 'Word Recognition' that provides Word Error Rates (WER) of different spell-corrector models. The table is structured with the following columns: Spell-Corrector, Swap, Drop, Add, Key, and All. Each row represents a different model or strategy. The first row shows the ATD model with error rates of 7.2 for Swap, 12.6 for Drop, 13.3 for Add, 6.9 for Key, and an overall rate of 11.2. The second row shows the ScRNN model trained on a 78K vocabulary with error rates of 6.3 for Swap, 10.2 for Drop, 8.7 for Add, 9.8 for Key, and an overall rate of 8.7. The third section of the table, labeled 'ScRNN (10K) w/ Backoff Variants', includes three rows: Pass-Through with error rates of 8.5 for Swap, 10.5 for Drop, 10.7 for Add, 11.2 for Key, and an overall rate of 10.2; Neutral with error rates of 8.7 for Swap, 10.9 for Drop, 10.8 for Add, 11.4 for Key, and an overall rate of 10.6; and Background with error rates of 5.4 for Swap, 8.1 for Drop, 6.4 for Add, 7.6 for Key, and an overall rate of 6.9. The table highlights the performance of each model across different types of errors and their overall effectiveness.",
        "segmentation": false
    },
    "image_4": {
        "image_id": 4,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_4.jpg",
        "caption": [
            "Sentiment Analysis (1-char attack/2-char attack) "
        ],
        "footnote": [
            "Table 3: Accuracy of various classification models, with and without defenses, under adversarial attacks. Even 1-character attacks significantly degrade classifier performance. Our defenses confer robustness, recovering over $76\\%$ of the original accuracy, under the ‘all’ setting for all four model classes. "
        ],
        "context": "We observe additional gains by using background models as a backoff alternative, because The ScRNN model with pass-through backoff offers better protection, bringing back the adversarial accuracy within $5\\%$ range for the swap attack. It is also effective under other attack classes, and can mitigate the adversarial effect in wordpiece models by $21\\%$ , character-only models by $19\\%$ , and in word, and word $^{+}$ char models by over $4.5\\%$ . This suggests that the direct training signal of word error correction is more effective than the indirect signal of sentiment classification available to DA and Adv for model robustness.  and drop attacks. The robustness of different models can be ordered as word-only $>$ word+char $>$ char-only $\\sim$ word-piece, and the efficacy of different attacks as add $>\\mathrm{key}>\\mathrm{drop}>\\mathrm{swap}.$ Next, we scrutinize the effectiveness of defense methods when faced against adversarially chosen attacks. Clearly from table 3, DA and Adv are not effective in this case. We observed that despite a low training error, these models were not able to generalize to attacks on newer words at test time. ATD spell corrector is the most effective on keyboard attacks, but performs poorly on other attack types, particularly the add attack strategy. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-90970573671f6132f5962121c2c4e4f6",
        "description": "The image is a table labeled 'Sentiment Analysis (1-char attack/2-char attack)' that provides the accuracy of various classification models, with and without defenses, under adversarial attacks. The table is divided into four main sections: Word-Level Models, Char-Level Models, Word+Char Models, and Word-piece Models. Each section contains rows representing different models and their performance under various attack types: No attack, Swap, Drop, Add, Key, and All. The columns represent the model's accuracy under these attack types. For example, in the Word-Level Models section, the BiLSTM model has an accuracy of 79.2% under No attack, and its performance drops to 64.3% and 53.6% under Swap attack. The table highlights the significant degradation in classifier performance even with 1-character attacks and shows the robustness conferred by different defenses. Notably, the ScRNN model with pass-through backoff offers better protection, bringing back the adversarial accuracy within a 5% range for the swap attack. The table also indicates the efficacy of different attacks as add > key > drop > swap and the robustness of different models as word-only > word+char > char-only ~ word-piece.",
        "segmentation": false
    },
    "image_5": {
        "image_id": 5,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_5.jpg",
        "caption": [],
        "footnote": [],
        "context": "Table 4 shows the accuracy of BERT on $200\\,\\mathrm{ex}\\cdot$ -amples from the dev set of the MRPC paraphrase detection task under various attack and defense settings. We re-trained the ScRNN model variants on the MRPC training set for these experiments. Again, we find that simple 1-2 character attacks can bring down the accuracy of BERT significantly $89\\%$ to $31\\%$ ). Word recognition models can provide an effective defense, with both our pass-through and neutral variants recovering most of the accuracy. While the  Table 4: Accuracy of BERT, with and without defenses, on MRPC when attacked under the ‘all’ attack setting. error rate (WER), especially, under the swap and drop attacks. However, these gains do not consistently translate in all other settings, as lower WER is necessary but not sufficient. Besides lower error rate, we find that a solid defense should furnish the attacker the fewest options to attack, i.e. it should have a low sensitivity. As we shall see in section $\\S\\ 4.3$ , the backoff neutral variation has the lowest sensitivity due to mapping UNK predictions to a fixed neutral word. Thus, it results in the highest robustness on most of the attack types for all four model classes.  ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-90970573671f6132f5962121c2c4e4f6",
        "description": "The image is a table labeled 'Table 4: Accuracy of BERT, with and without defenses, on MRPC when attacked under the ‘all’ attack setting.' The table is structured with four rows and five columns. The first column lists different models: BERT, BERT + ATD, BERT + Pass-through, and BERT + Neutral. The subsequent columns represent the accuracy percentages under different attack settings: 'No Attack', 'All attacks - 1-char', and 'All attacks - 2-char'. The values are as follows: for 'No Attack', the accuracies are 89.0% for BERT, 89.9% for BERT + ATD, 89.0% for BERT + Pass-through, and 84.0% for BERT + Neutral. Under 'All attacks - 1-char', the accuracies drop to 60.0% for BERT, 75.8% for BERT + ATD, 84.5% for BERT + Pass-through, and 82.5% for BERT + Neutral. Under 'All attacks - 2-char', the accuracies further decrease to 31.0% for BERT, 61.6% for BERT + ATD, 81.5% for BERT + Pass-through, and 82.5% for BERT + Neutral. The table highlights the significant reduction in accuracy for BERT under attack scenarios and the effectiveness of various defense mechanisms in mitigating these attacks.",
        "segmentation": false
    },
    "image_6": {
        "image_id": 6,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_6.jpg",
        "caption": [
            "Sensitivity Analysis "
        ],
        "footnote": [
            "Table 5: Sensitivity values for word recognizers. Neutral backoff shows lowest sensitivity. "
        ],
        "context": "Table 4 shows the accuracy of BERT on $200\\,\\mathrm{ex}\\cdot$ -amples from the dev set of the MRPC paraphrase detection task under various attack and defense settings. We re-trained the ScRNN model variants on the MRPC training set for these experiments. Again, we find that simple 1-2 character attacks can bring down the accuracy of BERT significantly $89\\%$ to $31\\%$ ). Word recognition models can provide an effective defense, with both our pass-through and neutral variants recovering most of the accuracy. While the neutral backoff model is effective on 2-char attacks, it hurts performance in the no attack setting, since it all other settings, as lower WER is necessary but not sufficient. Besides lower error rate, we find that a solid defense should furnish the attacker the fewest options to attack, i.e. it should have a low sensitivity. As we shall see in section $\\S\\ 4.3$ , the backoff neutral variation has the lowest sensitivity due to mapping UNK predictions to a fixed neutral word. Thus, it results in the highest robustness on most of the attack types for all four model classes.   Table 4: Accuracy of BERT, with and without defenses, on MRPC when attacked under the ‘all’ attack setting. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-90970573671f6132f5962121c2c4e4f6",
        "description": "The image is a table labeled 'Sensitivity Analysis' that provides sensitivity values for word recognizers. The table is divided into two main sections: Closed Vocabulary Models (word-only) and Open Vocab. Models (char/word+char/word-piece). Each section contains rows representing different backoff strategies (Pass-Through, Background, Neutral) and columns representing various types of attacks (Swap, Drop, Add, Key, All). The values in the table are numerical and represent sensitivity percentages. For example, in the Closed Vocabulary Models section, the Pass-Through backoff has sensitivity values of 17.6% for Swap, 19.7% for Drop, 0.8% for Add, 7.3% for Key, and 11.3% for All. In the Open Vocab. Models section, the Pass-Through backoff has significantly higher sensitivity values, such as 39.6% for Swap, 35.3% for Drop, 19.2% for Add, 26.9% for Key, and 30.3% for All. The Neutral backoff shows the lowest sensitivity across all attack types in both model categories.",
        "segmentation": false
    },
    "image_7": {
        "image_id": 7,
        "image_path": "./fusion_research/fusion_dataset/paper/P19-1561/images/image_7.jpg",
        "caption": [
            "Figure 2: Effect of sensitivity and word error rate on robustness (depicted by the bubble sizes) in word-only models (left) and char-only models (right). "
        ],
        "footnote": [],
        "context": "Human Intelligibility We verify if the sentiment (of the reviews) is preserved with char-level attacks. In a human study with 50 attacked Ideally, a preferred defense is one with low sensitivity and word error rate. In practice, however, we see that a low error rate often comes at the cost of sensitivity. We visualize this trade-off in Figure 2, where we plot WER and sensitivity on the two axes, and depict the robustness when using different backoff variants. Generally, sensitivity is the more dominant factor out of the two, as the error rates of the considered variants are reasonably low.  5). This is expected, as it returns a fixed neutral word whenever the ScRNN predicts an UNK , therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only, word+char, word-piece) downstream classifiers consider every unique combination of characters differently, whereas wordonly classifiers internally treat all out of vocabulary (OOV) words alike. Hence, for char-only, word+char, and word-piece models, the passthrough version is more sensitive than the background variant, as it passes words as is (and each combination is considered uniquely). However, for word-only models, pass-through is less sensitive as all the OOV character combinations are rendered identical. ",
        "chunk_order_index": 5,
        "chunk_id": "chunk-90970573671f6132f5962121c2c4e4f6",
        "description": "The image consists of two scatter plots side by side, each depicting the relationship between sensitivity and word error rate (WER) for different models. The left plot is labeled 'word-only models' and the right plot is labeled 'char-only models'. Each plot has sensitivity on the x-axis and WER on the y-axis. The points in the plots are color-coded to represent different backoff variants: blue for 'Pass-through', orange for 'Background', and green for 'Neutral'. The size of the bubbles indicates robustness. In the left plot, there are three data points: a large green bubble at (10.8, 63.2), a medium-sized blue bubble at (10.8, 63.2), and a small orange bubble at (12.4, 59.6). In the right plot, there are four data points: a large green bubble at (10.8, 55.2), a medium-sized blue bubble at (29.6, 51.6), and a small orange bubble at (10.8, 53.6). The scales for both axes range from 6 to 12 for sensitivity and from 7 to 11 for WER. The plots illustrate the trade-off between sensitivity and WER, with larger bubbles indicating higher robustness.",
        "segmentation": false
    }
}